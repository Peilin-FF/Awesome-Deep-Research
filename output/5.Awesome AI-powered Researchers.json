[
  {
    "Date": "2021.02",
    "Title": "Documentation Matters: Human-Centered AI System to Assist Data Science Code Documentation in Computational Notebooks",
    "link": "https://arxiv.org/abs/2102.12592",
    "abstract": "Computational notebooks allow data scientists to express their ideas through\na combination of code and documentation. However, data scientists often pay\nattention only to the code, and neglect creating or updating their\ndocumentation during quick iterations. Inspired by human documentation\npractices learned from 80 highly-voted Kaggle notebooks, we design and\nimplement Themisto, an automated documentation generation system to explore how\nhuman-centered AI systems can support human data scientists in the machine\nlearning code documentation scenario. Themisto facilitates the creation of\ndocumentation via three approaches: a deep-learning-based approach to generate\ndocumentation for source code, a query-based approach to retrieve online API\ndocumentation for source code, and a user prompt approach to nudge users to\nwrite documentation. We evaluated Themisto in a within-subjects experiment with\n24 data science practitioners, and found that automated documentation\ngeneration techniques reduced the time for writing documentation, reminded\nparticipants to document code they would have ignored, and improved\nparticipants' satisfaction with their computational notebook.",
    "keywords": "Computational Notebooks, Data Science, Documentation Generation, Human-Centered AI, Machine Learning Code Documentation"
  },
  {
    "Date": "2021.04",
    "Title": "Accelerating science with human versus alien artificial intelligences",
    "link": "https://arxiv.org/abs/2104.05188",
    "abstract": "Data-driven artificial intelligence models fed with published scientific\nfindings have been used to create powerful prediction engines for scientific\nand technological advance, such as the discovery of novel materials with\ndesired properties and the targeted invention of new therapies and vaccines.\nThese AI approaches typically ignore the distribution of human prediction\nengines -- scientists and inventor -- who continuously alter the landscape of\ndiscovery and invention. As a result, AI hypotheses are designed to substitute\nfor human experts, failing to complement them for punctuated collective\nadvance. Here we show that incorporating the distribution of human expertise\ninto self-supervised models by training on inferences cognitively available to\nexperts dramatically improves AI prediction of future human discoveries and\ninventions. Including expert-awareness into models that propose (a) valuable\nenergy-relevant materials increases the precision of materials predictions by\n~100%, (b) repurposing thousands of drugs to treat new diseases increases\nprecision by 43%, and (c) COVID-19 vaccine candidates examined in clinical\ntrials by 260%. These models succeed by predicting human predictions and the\nscientists who will make them. By tuning AI to avoid the crowd, however, it\ngenerates scientifically promising \"alien\" hypotheses unlikely to be imagined\nor pursued without intervention, not only accelerating but punctuating\nscientific advance. By identifying and correcting for collective human bias,\nthese models also suggest opportunities to improve human prediction by\nreformulating science education for discovery.",
    "keywords": "Artificial Intelligence, Human Expertise, Scientific Discovery, Prediction Engines, Collective Advance"
  },
  {
    "Date": "2022.02",
    "Title": "AI Research Associate for Early-Stage Scientific Discovery",
    "link": "https://arxiv.org/abs/2202.03199",
    "abstract": "Artificial intelligence (AI) has been increasingly applied in scientific\nactivities for decades; however, it is still far from an insightful and\ntrustworthy collaborator in the scientific process. Most existing AI methods\nare either too simplistic to be useful in real problems faced by scientists or\ntoo domain-specialized (even dogmatized), stifling transformative discoveries\nor paradigm shifts. We present an AI research associate for early-stage\nscientific discovery based on (a) a novel minimally-biased ontology for\nphysics-based modeling that is context-aware, interpretable, and generalizable\nacross classical and relativistic physics; (b) automatic search for viable and\nparsimonious hypotheses, represented at a high-level (via domain-agnostic\nconstructs) with built-in invariants, e.g., postulated forms of conservation\nprinciples implied by a presupposed spacetime topology; and (c) automatic\ncompilation of the enumerated hypotheses to domain-specific, interpretable, and\ntrainable/testable tensor-based computation graphs to learn phenomenological\nrelations, e.g., constitutive or material laws, from sparse (and possibly\nnoisy) data sets.",
    "keywords": "Artificial Intelligence, Scientific Discovery, Physics-Based Modeling, Hypothesis Search, Tensor-Based Computation Graphs"
  },
  {
    "Date": "2022.08",
    "Title": "Effidit: Your AI Writing Assistant",
    "link": "https://arxiv.org/abs/2208.01815",
    "abstract": "In this technical report, we introduce Effidit (Efficient and Intelligent\nEditing), a digital writing assistant that facilitates users to write\nhigher-quality text more efficiently by using artificial intelligence (AI)\ntechnologies. Previous writing assistants typically provide the function of\nerror checking (to detect and correct spelling and grammatical errors) and\nlimited text-rewriting functionality. With the emergence of large-scale neural\nlanguage models, some systems support automatically completing a sentence or a\nparagraph. In Effidit, we significantly expand the capacities of a writing\nassistant by providing functions in five categories: text completion, error\nchecking, text polishing, keywords to sentences (K2S), and cloud input methods\n(cloud IME). In the text completion category, Effidit supports generation-based\nsentence completion, retrieval-based sentence completion, and phrase\ncompletion. In contrast, many other writing assistants so far only provide one\nor two of the three functions. For text polishing, we have three functions:\n(context-aware) phrase polishing, sentence paraphrasing, and sentence\nexpansion, whereas many other writing assistants often support one or two\nfunctions in this category. The main contents of this report include major\nmodules of Effidit, methods for implementing these modules, and evaluation\nresults of some key methods.",
    "keywords": "Effidit, AI Writing Assistant, Text Completion, Error Checking, Text Polishing"
  },
  {
    "Date": "2022.10",
    "Title": "Artificial Intelligence for Scientific Research: Authentic Research Education Framework",
    "link": "https://arxiv.org/abs/2210.08966",
    "abstract": "We report a framework that enables the wide adoption of authentic research\neducational methodology at various schools by addressing common barriers. The\nguiding principles we present were applied to implement a program in which\nteams of students with complementary skills develop useful artificial\nintelligence (AI) solutions for researchers in natural sciences. To accomplish\nthis, we work with research laboratories that reveal/specify their needs, and\nthen our student teams work on the discovery, design, and development of an AI\nsolution for unique problems using a consulting-like arrangement. To date, our\ngroup has been operating at New York University (NYU) for seven consecutive\nsemesters, has engaged more than a hundred students, ranging from first-year\ncollege students to master's candidates, and has worked with more than twenty\nprojects and collaborators. While creating education benefits for students, our\napproach also directly benefits scientists, who get an opportunity to evaluate\nthe usefulness of machine learning for their specific needs.",
    "keywords": "Artificial Intelligence, Scientific Research, Authentic Research Education, Framework, Machine Learning"
  },
  {
    "Date": "2022.11",
    "Title": "AI Assistants: A Framework for Semi-Automated Data Wrangling",
    "link": "https://arxiv.org/abs/2211.00192",
    "abstract": "Data wrangling tasks such as obtaining and linking data from various sources,\ntransforming data formats, and correcting erroneous records, can constitute up\nto 80% of typical data engineering work. Despite the rise of machine learning\nand artificial intelligence, data wrangling remains a tedious and manual task.\nWe introduce AI assistants, a class of semi-automatic interactive tools to\nstreamline data wrangling. An AI assistant guides the analyst through a\nspecific data wrangling task by recommending a suitable data transformation\nthat respects the constraints obtained through interaction with the analyst.\n  We formally define the structure of AI assistants and describe how existing\ntools that treat data cleaning as an optimization problem fit the definition.\nWe implement AI assistants for four common data wrangling tasks and make AI\nassistants easily accessible to data analysts in an open-source notebook\nenvironment for data science, by leveraging the common structure they follow.\nWe evaluate our AI assistants both quantitatively and qualitatively through\nthree example scenarios. We show that the unified and interactive design makes\nit easy to perform tasks that would be difficult to do manually or with a fully\nautomatic tool.",
    "keywords": "AI Assistants, Data Wrangling, Semi-Automated, Data Engineering, Interactive Tools"
  },
  {
    "Date": "2023.02",
    "Title": "CARE: Collaborative AI-Assisted Reading Environment",
    "link": "https://arxiv.org/abs/2302.12611",
    "abstract": "Recent years have seen impressive progress in AI-assisted writing, yet the\ndevelopments in AI-assisted reading are lacking. We propose inline commentary\nas a natural vehicle for AI-based reading assistance, and present CARE: the\nfirst open integrated platform for the study of inline commentary and reading.\nCARE facilitates data collection for inline commentaries in a commonplace\ncollaborative reading environment, and provides a framework for enhancing\nreading with NLP-based assistance, such as text classification, generation or\nquestion answering. The extensible behavioral logging allows unique insights\ninto the reading and commenting behavior, and flexible configuration makes the\nplatform easy to deploy in new scenarios. To evaluate CARE in action, we apply\nthe platform in a user study dedicated to scholarly peer review. CARE\nfacilitates the data collection and study of inline commentary in NLP,\nextrinsic evaluation of NLP assistance, and application prototyping. We invite\nthe community to explore and build upon the open source implementation of CARE.",
    "keywords": "AI-assisted reading, inline commentary, CARE platform, NLP assistance, scholarly peer review."
  },
  {
    "Date": "2023.04",
    "Title": "Beyond Summarization: Designing AI Support for Real-World Expository Writing Tasks",
    "link": "https://arxiv.org/abs/2304.02623",
    "abstract": "Large language models have introduced exciting new opportunities and\nchallenges in designing and developing new AI-assisted writing support tools.\nRecent work has shown that leveraging this new technology can transform writing\nin many scenarios such as ideation during creative writing, editing support,\nand summarization. However, AI-supported expository writing--including\nreal-world tasks like scholars writing literature reviews or doctors writing\nprogress notes--is relatively understudied. In this position paper, we argue\nthat developing AI supports for expository writing has unique and exciting\nresearch challenges and can lead to high real-world impacts. We characterize\nexpository writing as evidence-based and knowledge-generating: it contains\nsummaries of external documents as well as new information or knowledge. It can\nbe seen as the product of authors' sensemaking process over a set of source\ndocuments, and the interplay between reading, reflection, and writing opens up\nnew opportunities for designing AI support. We sketch three components for AI\nsupport design and discuss considerations for future research.",
    "keywords": "AI-assisted writing, expository writing, large language models, real-world writing tasks, AI support design."
  },
  {
    "Date": "2023.04",
    "Title": "ChemCrow: Augmenting large-language models with chemistry tools",
    "link": "https://arxiv.org/abs/2304.05376",
    "abstract": "Over the last decades, excellent computational chemistry tools have been\ndeveloped. Integrating them into a single platform with enhanced accessibility\ncould help reaching their full potential by overcoming steep learning curves.\nRecently, large-language models (LLMs) have shown strong performance in tasks\nacross domains, but struggle with chemistry-related problems. Moreover, these\nmodels lack access to external knowledge sources, limiting their usefulness in\nscientific applications. In this study, we introduce ChemCrow, an LLM chemistry\nagent designed to accomplish tasks across organic synthesis, drug discovery,\nand materials design. By integrating 18 expert-designed tools, ChemCrow\naugments the LLM performance in chemistry, and new capabilities emerge. Our\nagent autonomously planned and executed the syntheses of an insect repellent,\nthree organocatalysts, and guided the discovery of a novel chromophore. Our\nevaluation, including both LLM and expert assessments, demonstrates ChemCrow's\neffectiveness in automating a diverse set of chemical tasks. Surprisingly, we\nfind that GPT-4 as an evaluator cannot distinguish between clearly wrong GPT-4\ncompletions and Chemcrow's performance. Our work not only aids expert chemists\nand lowers barriers for non-experts, but also fosters scientific advancement by\nbridging the gap between experimental and computational chemistry.",
    "keywords": "Large-language models, Computational chemistry, Organic synthesis, Drug discovery, Materials design."
  },
  {
    "Date": "2023.06",
    "Title": "Accelerating science with human-aware artificial intelligence",
    "link": "https://arxiv.org/abs/2306.01495",
    "abstract": "Artificial intelligence (AI) models trained on published scientific findings\nhave been used to invent valuable materials and targeted therapies, but they\ntypically ignore the human scientists who continually alter the landscape of\ndiscovery. Here we show that incorporating the distribution of human expertise\nby training unsupervised models on simulated inferences cognitively accessible\nto experts dramatically improves (up to 400%) AI prediction of future\ndiscoveries beyond those focused on research content alone, especially when\nrelevant literature is sparse. These models succeed by predicting human\npredictions and the scientists who will make them. By tuning human-aware AI to\navoid the crowd, we can generate scientifically promising \"alien\" hypotheses\nunlikely to be imagined or pursued without intervention until the distant\nfuture, which hold promise to punctuate scientific advance beyond questions\ncurrently pursued. Accelerating human discovery or probing its blind spots,\nhuman-aware AI enables us to move toward and beyond the contemporary scientific\nfrontier.",
    "keywords": "Artificial Intelligence, Human Expertise, Scientific Discoveries, Prediction Models, Accelerating Science"
  },
  {
    "Date": "2023.06",
    "Title": "Interactive Editing for Text Summarization",
    "link": "https://arxiv.org/abs/2306.03067",
    "abstract": "Summarizing lengthy documents is a common and essential task in our daily\nlives. Although recent advancements in neural summarization models can assist\nin crafting general-purpose summaries, human writers often have specific\nrequirements that call for a more customized approach. To address this need, we\nintroduce REVISE (Refinement and Editing via Iterative Summarization\nEnhancement), an innovative framework designed to facilitate iterative editing\nand refinement of draft summaries by human writers. Within our framework,\nwriters can effortlessly modify unsatisfactory segments at any location or\nlength and provide optional starting phrases -- our system will generate\ncoherent alternatives that seamlessly integrate with the existing summary. At\nits core, REVISE incorporates a modified fill-in-the-middle model with the\nencoder-decoder architecture while developing novel evaluation metrics tailored\nfor the summarization task. In essence, our framework empowers users to create\nhigh-quality, personalized summaries by effectively harnessing both human\nexpertise and AI capabilities, ultimately transforming the summarization\nprocess into a truly collaborative and adaptive experience.",
    "keywords": "Text Summarization, Neural Summarization Models, Interactive Editing, Draft Summaries, Collaborative Experience"
  },
  {
    "Date": "2023.06",
    "Title": "AssistGPT: A General Multi-modal Assistant that can Plan, Execute, Inspect, and Learn",
    "link": "https://arxiv.org/abs/2306.08640",
    "abstract": "Recent research on Large Language Models (LLMs) has led to remarkable\nadvancements in general NLP AI assistants. Some studies have further explored\nthe use of LLMs for planning and invoking models or APIs to address more\ngeneral multi-modal user queries. Despite this progress, complex visual-based\ntasks still remain challenging due to the diverse nature of visual tasks. This\ndiversity is reflected in two aspects: 1) Reasoning paths. For many real-life\napplications, it is hard to accurately decompose a query simply by examining\nthe query itself. Planning based on the specific visual content and the results\nof each step is usually required. 2) Flexible inputs and intermediate results.\nInput forms could be flexible for in-the-wild cases, and involves not only a\nsingle image or video but a mixture of videos and images, e.g., a user-view\nimage with some reference videos. Besides, a complex reasoning process will\nalso generate diverse multimodal intermediate results, e.g., video narrations,\nsegmented video clips, etc. To address such general cases, we propose a\nmulti-modal AI assistant, AssistGPT, with an interleaved code and language\nreasoning approach called Plan, Execute, Inspect, and Learn (PEIL) to integrate\nLLMs with various tools. Specifically, the Planner is capable of using natural\nlanguage to plan which tool in Executor should do next based on the current\nreasoning progress. Inspector is an efficient memory manager to assist the\nPlanner to feed proper visual information into a specific tool. Finally, since\nthe entire reasoning process is complex and flexible, a Learner is designed to\nenable the model to autonomously explore and discover the optimal solution. We\nconducted experiments on A-OKVQA and NExT-QA benchmarks, achieving\nstate-of-the-art results. Moreover, showcases demonstrate the ability of our\nsystem to handle questions far more complex than those found in the benchmarks.",
    "keywords": "Large Language Models, Multi-modal AI Assistant, Plan, Execute, Inspect, Learn, Visual-based tasks, Reasoning paths, Flexible inputs, Intermediate results."
  },
  {
    "Date": "2023.07",
    "Title": "ToolLLM: Facilitating Large Language Models to Master 16000+ Real-world APIs",
    "link": "https://arxiv.org/abs/2307.16789",
    "abstract": "Despite the advancements of open-source large language models (LLMs), e.g.,\nLLaMA, they remain significantly limited in tool-use capabilities, i.e., using\nexternal tools (APIs) to fulfill human instructions. The reason is that current\ninstruction tuning largely focuses on basic language tasks but ignores the\ntool-use domain. This is in contrast to the excellent tool-use capabilities of\nstate-of-the-art (SOTA) closed-source LLMs, e.g., ChatGPT. To bridge this gap,\nwe introduce ToolLLM, a general tool-use framework encompassing data\nconstruction, model training, and evaluation. We first present ToolBench, an\ninstruction-tuning dataset for tool use, which is constructed automatically\nusing ChatGPT. Specifically, the construction can be divided into three stages:\n(i) API collection: we collect 16,464 real-world RESTful APIs spanning 49\ncategories from RapidAPI Hub; (ii) instruction generation: we prompt ChatGPT to\ngenerate diverse instructions involving these APIs, covering both single-tool\nand multi-tool scenarios; (iii) solution path annotation: we use ChatGPT to\nsearch for a valid solution path (chain of API calls) for each instruction. To\nenhance the reasoning capabilities of LLMs, we develop a novel depth-first\nsearch-based decision tree algorithm. It enables LLMs to evaluate multiple\nreasoning traces and expand the search space. Moreover, to evaluate the\ntool-use capabilities of LLMs, we develop an automatic evaluator: ToolEval.\nBased on ToolBench, we fine-tune LLaMA to obtain an LLM ToolLLaMA, and equip it\nwith a neural API retriever to recommend appropriate APIs for each instruction.\nExperiments show that ToolLLaMA demonstrates a remarkable ability to execute\ncomplex instructions and generalize to unseen APIs, and exhibits comparable\nperformance to ChatGPT. Our ToolLLaMA also demonstrates strong zero-shot\ngeneralization ability in an out-of-distribution tool-use dataset: APIBench.",
    "keywords": "Large Language Models, Tool-use, APIs, Instruction Tuning, Generalization Ability"
  },
  {
    "Date": "2023.10",
    "Title": "GeoLLM: Extracting Geospatial Knowledge from Large Language Models",
    "link": "https://arxiv.org/abs/2310.06213",
    "abstract": "The application of machine learning (ML) in a range of geospatial tasks is\nincreasingly common but often relies on globally available covariates such as\nsatellite imagery that can either be expensive or lack predictive power. Here\nwe explore the question of whether the vast amounts of knowledge found in\nInternet language corpora, now compressed within large language models (LLMs),\ncan be leveraged for geospatial prediction tasks. We first demonstrate that\nLLMs embed remarkable spatial information about locations, but naively querying\nLLMs using geographic coordinates alone is ineffective in predicting key\nindicators like population density. We then present GeoLLM, a novel method that\ncan effectively extract geospatial knowledge from LLMs with auxiliary map data\nfrom OpenStreetMap. We demonstrate the utility of our approach across multiple\ntasks of central interest to the international community, including the\nmeasurement of population density and economic livelihoods. Across these tasks,\nour method demonstrates a 70% improvement in performance (measured using\nPearson's $r^2$) relative to baselines that use nearest neighbors or use\ninformation directly from the prompt, and performance equal to or exceeding\nsatellite-based benchmarks in the literature. With GeoLLM, we observe that\nGPT-3.5 outperforms Llama 2 and RoBERTa by 19% and 51% respectively, suggesting\nthat the performance of our method scales well with the size of the model and\nits pretraining dataset. Our experiments reveal that LLMs are remarkably\nsample-efficient, rich in geospatial information, and robust across the globe.\nCrucially, GeoLLM shows promise in mitigating the limitations of existing\ngeospatial covariates and complementing them well. Code is available on the\nproject website: https://rohinmanvi.github.io/GeoLLM",
    "keywords": "GeoLLM, Large Language Models, Geospatial Knowledge, Prediction Tasks, OpenStreetMap"
  },
  {
    "Date": "2023.11",
    "Title": "LLaVA-Plus: Learning to Use Tools for Creating Multimodal Agents",
    "link": "https://arxiv.org/abs/2311.05437",
    "abstract": "LLaVA-Plus is a general-purpose multimodal assistant that expands the\ncapabilities of large multimodal models. It maintains a skill repository of\npre-trained vision and vision-language models and can activate relevant tools\nbased on users' inputs to fulfill real-world tasks. LLaVA-Plus is trained on\nmultimodal instruction-following data to acquire the ability to use tools,\ncovering visual understanding, generation, external knowledge retrieval, and\ncompositions. Empirical results show that LLaVA-Plus outperforms LLaVA in\nexisting capabilities and exhibits new ones. It is distinct in that the image\nquery is directly grounded and actively engaged throughout the entire human-AI\ninteraction sessions, significantly improving tool use performance and enabling\nnew scenarios.",
    "keywords": "LLaVA-Plus, multimodal assistant, skill repository, instruction-following data, tool use performance."
  },
  {
    "Date": "2023.11",
    "Title": "AcademicGPT: Empowering Academic Research",
    "link": "https://arxiv.org/abs/2311.12315",
    "abstract": "Large Language Models (LLMs) have demonstrated exceptional capabilities\nacross various natural language processing tasks. Yet, many of these advanced\nLLMs are tailored for broad, general-purpose applications. In this technical\nreport, we introduce AcademicGPT, designed specifically to empower academic\nresearch. AcademicGPT is a continual training model derived from LLaMA2-70B.\nOur training corpus mainly consists of academic papers, thesis, content from\nsome academic domain, high-quality Chinese data and others. While it may not be\nextensive in data scale, AcademicGPT marks our initial venture into a\ndomain-specific GPT tailored for research area. We evaluate AcademicGPT on\nseveral established public benchmarks such as MMLU and CEval, as well as on\nsome specialized academic benchmarks like PubMedQA, SCIEval, and our\nnewly-created ComputerScienceQA, to demonstrate its ability from general\nknowledge ability, to Chinese ability, and to academic ability. Building upon\nAcademicGPT's foundation model, we also developed several applications catered\nto the academic area, including General Academic Question Answering,\nAI-assisted Paper Reading, Paper Review, and AI-assisted Title and Abstract\nGeneration.",
    "keywords": "AcademicGPT, Large Language Models, academic research, domain-specific GPT, academic benchmarks."
  },
  {
    "Date": "2023.12",
    "Title": "Open Datasheets: Machine-readable Documentation for Open Datasets and Responsible AI Assessments",
    "link": "https://arxiv.org/abs/2312.06153",
    "abstract": "This paper introduces a no-code, machine-readable documentation framework for\nopen datasets, with a focus on responsible AI (RAI) considerations. The\nframework aims to improve comprehensibility, and usability of open datasets,\nfacilitating easier discovery and use, better understanding of content and\ncontext, and evaluation of dataset quality and accuracy. The proposed framework\nis designed to streamline the evaluation of datasets, helping researchers, data\nscientists, and other open data users quickly identify datasets that meet their\nneeds and organizational policies or regulations. The paper also discusses the\nimplementation of the framework and provides recommendations to maximize its\npotential. The framework is expected to enhance the quality and reliability of\ndata used in research and decision-making, fostering the development of more\nresponsible and trustworthy AI systems.",
    "keywords": "Open Datasheets, Machine-readable Documentation, Open Datasets, Responsible AI, RAI Assessments"
  },
  {
    "Date": "2023.12",
    "Title": "Agent-based Learning of Materials Datasets from Scientific Literature",
    "link": "https://arxiv.org/pdf/2312.11690",
    "abstract": "Advancements in machine learning and artificial intelligence are transforming materials discovery.Yet, the availability of structured experimental data remains a bottleneck. The vast corpus of scientificliterature presents a valuable and rich resource of such data. However, manual dataset creationfrom these resources is challenging due to issues in maintaining quality and consistency, scalabilitylimitations, and the risk of human error and bias. Therefore, in this work, we develop a chemist AIagent, powered by large language models (LLMs), to overcome these challenges by autonomouslycreating structured datasets from natural language text, ranging from sentences and paragraphsto extensive scientific research articles. Our chemist AI agent, Eunomia, can plan and executeactions by leveraging the existing knowledge from decades of scientific research articles, scientists,the Internet and other tools altogether. We benchmark the performance of our approach in threedifferent information extraction tasks with various levels of complexity, including solid-state impuritydoping, metal-organic framework (MOF) chemical formula, and property relations. Our resultsdemonstrate that our zero-shot agent, with the appropriate tools, is capable of attaining performancethat is either superior or comparable to the state-of-the-art fine-tuned materials information extractionmethods. This approach simplifies compilation of machine learning-ready datasets for variousmaterials discovery applications, and significantly ease the accessibility of advanced natural languageprocessing tools for novice users in natural language. The methodology in this work is developed asan open-source software on https://github.com/AI4ChemS/Eunomia.",
    "keywords": "\"Agent-based Learning\", \"Materials Datasets\", \"Scientific Literature\", \"Machine Learning\", \"Artificial Intelligence\""
  },
  {
    "Date": "2023.12",
    "Title": "GEAR-Up: Generative AI and External Knowledge-based Retrieval Upgrading Scholarly Article Searches for Systematic Reviews",
    "link": "https://arxiv.org/abs/2312.09948",
    "abstract": "Systematic reviews (SRs) - the librarian-assisted literature survey of\nscholarly articles takes time and requires significant human resources. Given\nthe ever-increasing volume of published studies, applying existing computing\nand informatics technology can decrease this time and resource burden. Due to\nthe revolutionary advances in (1) Generative AI such as ChatGPT, and (2)\nExternal knowledge-augmented information extraction efforts such as\nRetrieval-Augmented Generation, In this work, we explore the use of techniques\nfrom (1) and (2) for SR. We demonstrate a system that takes user queries,\nperforms query expansion to obtain enriched context (includes additional terms\nand definitions by querying language models and knowledge graphs), and uses\nthis context to search for articles on scholarly databases to retrieve\narticles. We perform qualitative evaluations of our system through comparison\nagainst sentinel (ground truth) articles provided by an in-house librarian. The\ndemo can be found at: https://youtu.be/zMdP56GJ9mU.",
    "keywords": "Generative AI, External Knowledge-based Retrieval, Systematic Reviews, Query Expansion, Scholarly Article Searches"
  },
  {
    "Date": "2023.12",
    "Title": "ChatGPT, Llama, can you write my report? An experiment on assisted digital forensics reports written using (Local) Large Language Models",
    "link": "https://arxiv.org/abs/2312.14607",
    "abstract": "Generative AIs, especially Large Language Models (LLMs) such as ChatGPT or\nLlama, have advanced significantly, positioning them as valuable tools for\ndigital forensics. While initial studies have explored the potential of ChatGPT\nin the context of investigations, the question of to what extent LLMs can\nassist the forensic report writing process remains unresolved. To answer the\nquestion, this article first examines forensic reports with the goal of\ngeneralization (e.g., finding the `average structure' of a report). We then\nevaluate the strengths and limitations of LLMs for generating the different\nparts of the forensic report using a case study. This work thus provides\nvaluable insights into the automation of report writing, a critical facet of\ndigital forensics investigations. We conclude that combined with thorough\nproofreading and corrections, LLMs may assist practitioners during the report\nwriting process but at this point cannot replace them.",
    "keywords": "Generative AIs, Large Language Models, digital forensics, forensic report writing, automation."
  },
  {
    "Date": "2024.01",
    "Title": "LLM-Powered Code Vulnerability Repair with Reinforcement Learning and Semantic Reward",
    "link": "https://arxiv.org/abs/2401.03374",
    "abstract": "In software development, the predominant emphasis on functionality often\nsupersedes security concerns, a trend gaining momentum with AI-driven\nautomation tools like GitHub Copilot. These tools significantly improve\ndevelopers' efficiency in functional code development. Nevertheless, it remains\na notable concern that such tools are also responsible for creating insecure\ncode, predominantly because of pre-training on publicly available repositories\nwith vulnerable code. Moreover, developers are called the \"weakest link in the\nchain\" since they have very minimal knowledge of code security. Although\nexisting solutions provide a reasonable solution to vulnerable code, they must\nadequately describe and educate the developers on code security to ensure that\nthe security issues are not repeated. Therefore we introduce a multipurpose\ncode vulnerability analysis system \\texttt{SecRepair}, powered by a large\nlanguage model, CodeGen2 assisting the developer in identifying and generating\nfixed code along with a complete description of the vulnerability with a code\ncomment. Our innovative methodology uses a reinforcement learning paradigm to\ngenerate code comments augmented by a semantic reward mechanism. Inspired by\nhow humans fix code issues, we propose an instruction-based dataset suitable\nfor vulnerability analysis with LLMs. We further identify zero-day and N-day\nvulnerabilities in 6 Open Source IoT Operating Systems on GitHub. Our findings\nunderscore that incorporating reinforcement learning coupled with semantic\nreward augments our model's performance, thereby fortifying its capacity to\naddress code vulnerabilities with improved efficacy.",
    "keywords": "Code Vulnerability, Reinforcement Learning, Semantic Reward, Large Language Model, Code Generation"
  },
  {
    "Date": "2024.01",
    "Title": "Weaver: Foundation Models for Creative Writing",
    "link": "https://arxiv.org/abs/2401.17268",
    "abstract": "This work introduces Weaver, our first family of large language models (LLMs)\ndedicated to content creation. Weaver is pre-trained on a carefully selected\ncorpus that focuses on improving the writing capabilities of large language\nmodels. We then fine-tune Weaver for creative and professional writing purposes\nand align it to the preference of professional writers using a suit of novel\nmethods for instruction data synthesis and LLM alignment, making it able to\nproduce more human-like texts and follow more diverse instructions for content\ncreation. The Weaver family consists of models of Weaver Mini (1.8B), Weaver\nBase (6B), Weaver Pro (14B), and Weaver Ultra (34B) sizes, suitable for\ndifferent applications and can be dynamically dispatched by a routing agent\naccording to query complexity to balance response quality and computation cost.\nEvaluation on a carefully curated benchmark for assessing the writing\ncapabilities of LLMs shows Weaver models of all sizes outperform generalist\nLLMs several times larger than them. Notably, our most-capable Weaver Ultra\nmodel surpasses GPT-4, a state-of-the-art generalist LLM, on various writing\nscenarios, demonstrating the advantage of training specialized LLMs for writing\npurposes. Moreover, Weaver natively supports retrieval-augmented generation\n(RAG) and function calling (tool usage). We present various use cases of these\nabilities for improving AI-assisted writing systems, including integration of\nexternal knowledge bases, tools, or APIs, and providing personalized writing\nassistance. Furthermore, we discuss and summarize a guideline and best\npractices for pre-training and fine-tuning domain-specific LLMs.",
    "keywords": "Weaver, Large Language Models, Creative Writing, Instruction Data Synthesis, Retrieval-Augmented Generation"
  },
  {
    "Date": "2024.02",
    "Title": "Prompt-Time Symbolic Knowledge Capture with Large Language Models",
    "link": "https://arxiv.org/abs/2402.00414",
    "abstract": "Augmenting large language models (LLMs) with user-specific knowledge is\ncrucial for real-world applications, such as personal AI assistants. However,\nLLMs inherently lack mechanisms for prompt-driven knowledge capture. This paper\ninvestigates utilizing the existing LLM capabilities to enable prompt-driven\nknowledge capture, with a particular emphasis on knowledge graphs. We address\nthis challenge by focusing on prompt-to-triple (P2T) generation. We explore\nthree methods: zero-shot prompting, few-shot prompting, and fine-tuning, and\nthen assess their performance via a specialized synthetic dataset. Our code and\ndatasets are publicly available at https://github.com/HaltiaAI/paper-PTSKC.",
    "keywords": "Large Language Models, Prompt-driven Knowledge Capture, Knowledge Graphs, Prompt-to-Triple Generation, Synthetic Dataset."
  },
  {
    "Date": "2024.02",
    "Title": "DS-Agent: Automated Data Science by Empowering Large Language Models with Case-Based Reasoning",
    "link": "https://arxiv.org/abs/2402.17453",
    "abstract": "In this work, we investigate the potential of large language models (LLMs)\nbased agents to automate data science tasks, with the goal of comprehending\ntask requirements, then building and training the best-fit machine learning\nmodels. Despite their widespread success, existing LLM agents are hindered by\ngenerating unreasonable experiment plans within this scenario. To this end, we\npresent DS-Agent, a novel automatic framework that harnesses LLM agent and\ncase-based reasoning (CBR). In the development stage, DS-Agent follows the CBR\nframework to structure an automatic iteration pipeline, which can flexibly\ncapitalize on the expert knowledge from Kaggle, and facilitate consistent\nperformance improvement through the feedback mechanism. Moreover, DS-Agent\nimplements a low-resource deployment stage with a simplified CBR paradigm to\nadapt past successful solutions from the development stage for direct code\ngeneration, significantly reducing the demand on foundational capabilities of\nLLMs. Empirically, DS-Agent with GPT-4 achieves 100\\% success rate in the\ndevelopment stage, while attaining 36\\% improvement on average one pass rate\nacross alternative LLMs in the deployment stage. In both stages, DS-Agent\nachieves the best rank in performance, costing \\$1.60 and \\$0.13 per run with\nGPT-4, respectively. Our data and code are open-sourced at\nhttps://github.com/guosyjlu/DS-Agent.",
    "keywords": "Large Language Models, Automated Data Science, Case-Based Reasoning, Machine Learning Models, Kaggle Expert Knowledge"
  },
  {
    "Date": "2024.02",
    "Title": "An Autonomous Large Language Model Agent for Chemical Literature Data Mining",
    "link": "https://arxiv.org/abs/2402.12993",
    "abstract": "Chemical synthesis, which is crucial for advancing material synthesis and\ndrug discovery, impacts various sectors including environmental science and\nhealthcare. The rise of technology in chemistry has generated extensive\nchemical data, challenging researchers to discern patterns and refine synthesis\nprocesses. Artificial intelligence (AI) helps by analyzing data to optimize\nsynthesis and increase yields. However, AI faces challenges in processing\nliterature data due to the unstructured format and diverse writing style of\nchemical literature. To overcome these difficulties, we introduce an end-to-end\nAI agent framework capable of high-fidelity extraction from extensive chemical\nliterature. This AI agent employs large language models (LLMs) for prompt\ngeneration and iterative optimization. It functions as a chemistry assistant,\nautomating data collection and analysis, thereby saving manpower and enhancing\nperformance. Our framework's efficacy is evaluated using accuracy, recall, and\nF1 score of reaction condition data, and we compared our method with human\nexperts in terms of content correctness and time efficiency. The proposed\napproach marks a significant advancement in automating chemical literature\nextraction and demonstrates the potential for AI to revolutionize data\nmanagement and utilization in chemistry.",
    "keywords": "\"Chemical synthesis\", \"Artificial intelligence\", \"Literature data mining\", \"Large language models\", \"Chemistry assistant\""
  },
  {
    "Date": "2024.02",
    "Title": "Multi-line AI-assisted Code Authoring",
    "link": "https://arxiv.org/abs/2402.04141",
    "abstract": "CodeCompose is an AI-assisted code authoring tool powered by large language\nmodels (LLMs) that provides inline suggestions to 10's of thousands of\ndevelopers at Meta. In this paper, we present how we scaled the product from\ndisplaying single-line suggestions to multi-line suggestions. This evolution\nrequired us to overcome several unique challenges in improving the usability of\nthese suggestions for developers.\n  First, we discuss how multi-line suggestions can have a 'jarring' effect, as\nthe LLM's suggestions constantly move around the developer's existing code,\nwhich would otherwise result in decreased productivity and satisfaction.\n  Second, multi-line suggestions take significantly longer to generate; hence\nwe present several innovative investments we made to reduce the perceived\nlatency for users. These model-hosting optimizations sped up multi-line\nsuggestion latency by 2.5x.\n  Finally, we conduct experiments on 10's of thousands of engineers to\nunderstand how multi-line suggestions impact the user experience and contrast\nthis with single-line suggestions. Our experiments reveal that (i) multi-line\nsuggestions account for 42% of total characters accepted (despite only\naccounting for 16% for displayed suggestions) (ii) multi-line suggestions\nalmost doubled the percentage of keystrokes saved for users from 9% to 17%.\nMulti-line CodeCompose has been rolled out to all engineers at Meta, and less\nthan 1% of engineers have opted out of multi-line suggestions.",
    "keywords": "AI-assisted code authoring, multi-line suggestions, large language models, productivity, user experience."
  },
  {
    "Date": "2024.02",
    "Title": "Streamlining the Selection Phase of Systematic Literature Reviews (SLRs) Using AI-Enabled GPT-4 Assistant API",
    "link": "https://arxiv.org/abs/2402.18582",
    "abstract": "The escalating volume of academic literature presents a formidable challenge\nin staying updated with the newest research developments. Addressing this, this\nstudy introduces a pioneering AI-based tool, configured specifically to\nstreamline the efficiency of the article selection phase in Systematic\nLiterature Reviews (SLRs). Utilizing the robust capabilities of OpenAI's GPT-4\nAssistant API, the tool successfully homogenizes the article selection process\nacross a broad array of academic disciplines. Implemented through a tripartite\napproach consisting of data preparation, AI-mediated article assessment, and\nstructured result presentation, this tool significantly accelerates the\ntime-consuming task of literature reviews. Importantly, this tool could be\nhighly beneficial in fields such as management and economics, where the SLR\nprocess involves substantial human judgment. The adoption of a standard GPT\nmodel can substantially reduce potential biases and enhance the speed and\nprecision of the SLR selection phase. This not only amplifies researcher\nproductivity and accuracy but also denotes a considerable stride forward in the\nway academic research is conducted amidst the surging body of scholarly\npublications.",
    "keywords": "Systematic Literature Reviews, AI-Enabled GPT-4 Assistant API, Article Selection, Academic Research, Bias Reduction"
  },
  {
    "Date": "2024.03",
    "Title": "AutoDev: Automated AI-Driven Development",
    "link": "https://arxiv.org/abs/2403.08299",
    "abstract": "The landscape of software development has witnessed a paradigm shift with the\nadvent of AI-powered assistants, exemplified by GitHub Copilot. However,\nexisting solutions are not leveraging all the potential capabilities available\nin an IDE such as building, testing, executing code, git operations, etc.\nTherefore, they are constrained by their limited capabilities, primarily\nfocusing on suggesting code snippets and file manipulation within a chat-based\ninterface. To fill this gap, we present AutoDev, a fully automated AI-driven\nsoftware development framework, designed for autonomous planning and execution\nof intricate software engineering tasks. AutoDev enables users to define\ncomplex software engineering objectives, which are assigned to AutoDev's\nautonomous AI Agents to achieve. These AI agents can perform diverse operations\non a codebase, including file editing, retrieval, build processes, execution,\ntesting, and git operations. They also have access to files, compiler output,\nbuild and testing logs, static analysis tools, and more. This enables the AI\nAgents to execute tasks in a fully automated manner with a comprehensive\nunderstanding of the contextual information required. Furthermore, AutoDev\nestablishes a secure development environment by confining all operations within\nDocker containers. This framework incorporates guardrails to ensure user\nprivacy and file security, allowing users to define specific permitted or\nrestricted commands and operations within AutoDev. In our evaluation, we tested\nAutoDev on the HumanEval dataset, obtaining promising results with 91.5% and\n87.8% of Pass@1 for code generation and test generation respectively,\ndemonstrating its effectiveness in automating software engineering tasks while\nmaintaining a secure and user-controlled development environment.",
    "keywords": "AI-driven development, software engineering, automated framework, AI Agents, secure environment."
  },
  {
    "Date": "2024.03",
    "Title": "ChatDBG: An AI-Powered Debugging Assistant",
    "link": "https://arxiv.org/abs/2403.16354",
    "abstract": "Debugging is a critical but challenging task for programmers. This paper\nproposes ChatDBG, an AI-powered debugging assistant. ChatDBG integrates large\nlanguage models (LLMs) to significantly enhance the capabilities and\nuser-friendliness of conventional debuggers. ChatDBG lets programmers engage in\na collaborative dialogue with the debugger, allowing them to pose complex\nquestions about program state, perform root cause analysis for crashes or\nassertion failures, and explore open-ended queries like `why is x null?'. To\nhandle these queries, ChatDBG grants the LLM autonomy to \"take the wheel\": it\ncan act as an independent agent capable of querying and controlling the\ndebugger to navigate through stacks and inspect program state. It then reports\nits findings and yields back control to the programmer. By leveraging the\nreal-world knowledge embedded in LLMs, ChatDBG can diagnose issues identifiable\nonly through the use of domain-specific reasoning. Our ChatDBG prototype\nintegrates with standard debuggers including LLDB and GDB for native code and\nPdb for Python. Our evaluation across a diverse set of code, including C/C++\ncode with known bugs and a suite of Python code including standalone scripts\nand Jupyter notebooks, demonstrates that ChatDBG can successfully analyze root\ncauses, explain bugs, and generate accurate fixes for a wide range of\nreal-world errors. For the Python programs, a single query led to an actionable\nbug fix 67% of the time; one additional follow-up query increased the success\nrate to 85%. ChatDBG has seen rapid uptake; it has already been downloaded more\nthan 65,000 times.",
    "keywords": "ChatDBG, AI-powered debugging, large language models, root cause analysis, collaborative dialogue."
  },
  {
    "Date": "2024.03",
    "Title": "LLMs as Academic Reading Companions: Extending HCI Through Synthetic Personae",
    "link": "https://arxiv.org/abs/2403.19506",
    "abstract": "This position paper argues that large language models (LLMs) constitute\npromising yet underutilized academic reading companions capable of enhancing\nlearning. We detail an exploratory study examining Claude from Anthropic, an\nLLM-based interactive assistant that helps students comprehend complex\nqualitative literature content. The study compares quantitative survey data and\nqualitative interviews assessing outcomes between a control group and an\nexperimental group leveraging Claude over a semester across two graduate\ncourses. Initial findings demonstrate tangible improvements in reading\ncomprehension and engagement among participants using the AI agent versus\nunsupported independent study. However, there is potential for overreliance and\nethical considerations that warrant continued investigation. By documenting an\nearly integration of an LLM reading companion into an educational context, this\nwork contributes pragmatic insights to guide development of synthetic personae\nsupporting learning. Broader impacts compel policy and industry actions to\nuphold responsible design in order to maximize benefits of AI integration while\nprioritizing student wellbeing.",
    "keywords": "Large Language Models, Academic Reading Companions, HCI, Synthetic Personae, Learning Enhancement"
  },
  {
    "Date": "2024.04",
    "Title": "DOCMASTER: A Unified Platform for Annotation, Training, & Inference in Document Question-Answering",
    "link": "https://arxiv.org/abs/2404.00439",
    "abstract": "The application of natural language processing models to PDF documents is\npivotal for various business applications yet the challenge of training models\nfor this purpose persists in businesses due to specific hurdles. These include\nthe complexity of working with PDF formats that necessitate parsing text and\nlayout information for curating training data and the lack of\nprivacy-preserving annotation tools. This paper introduces DOCMASTER, a unified\nplatform designed for annotating PDF documents, model training, and inference,\ntailored to document question-answering. The annotation interface enables users\nto input questions and highlight text spans within the PDF file as answers,\nsaving layout information and text spans accordingly. Furthermore, DOCMASTER\nsupports both state-of-the-art layout-aware and text models for comprehensive\ntraining purposes. Importantly, as annotations, training, and inference occur\non-device, it also safeguards privacy. The platform has been instrumental in\ndriving several research prototypes concerning document analysis such as the AI\nassistant utilized by University of California San Diego's (UCSD) International\nServices and Engagement Office (ISEO) for processing a substantial volume of\nPDF documents.",
    "keywords": "Natural Language Processing, PDF Documents, Document Question-Answering, Annotation Platform, Privacy-Preserving."
  },
  {
    "Date": "2024.04",
    "Title": "Empowering Biomedical Discovery with AI Agents",
    "link": "https://arxiv.org/abs/2404.02831",
    "abstract": "We envision \"AI scientists\" as systems capable of skeptical learning and\nreasoning that empower biomedical research through collaborative agents that\nintegrate AI models and biomedical tools with experimental platforms. Rather\nthan taking humans out of the discovery process, biomedical AI agents combine\nhuman creativity and expertise with AI's ability to analyze large datasets,\nnavigate hypothesis spaces, and execute repetitive tasks. AI agents are poised\nto be proficient in various tasks, planning discovery workflows and performing\nself-assessment to identify and mitigate gaps in their knowledge. These agents\nuse large language models and generative models to feature structured memory\nfor continual learning and use machine learning tools to incorporate scientific\nknowledge, biological principles, and theories. AI agents can impact areas\nranging from virtual cell simulation, programmable control of phenotypes, and\nthe design of cellular circuits to developing new therapies.",
    "keywords": "AI Scientists, Biomedical Research, Collaborative Agents, Large Datasets, Hypothesis Spaces"
  },
  {
    "Date": "2024.04",
    "Title": "ResearchAgent: Iterative Research Idea Generation over Scientific Literature with Large Language Models",
    "link": "https://arxiv.org/abs/2404.07738",
    "abstract": "The pace of scientific research, vital for improving human life, is complex,\nslow, and needs specialized expertise. Meanwhile, novel, impactful research\noften stems from both a deep understanding of prior work, and a\ncross-pollination of ideas across domains and fields. To enhance the\nproductivity of researchers, we propose ResearchAgent, which leverages the\nencyclopedic knowledge and linguistic reasoning capabilities of Large Language\nModels (LLMs) to assist them in their work. This system automatically defines\nnovel problems, proposes methods and designs experiments, while iteratively\nrefining them based on the feedback from collaborative LLM-powered reviewing\nagents. Specifically, starting with a core scientific paper, ResearchAgent is\naugmented not only with relevant publications by connecting information over an\nacademic graph but also entities retrieved from a knowledge store derived from\nshared underlying concepts mined across numerous papers. Then, mimicking a\nscientific approach to improving ideas with peer discussions, we leverage\nmultiple LLM-based ReviewingAgents that provide reviews and feedback via\niterative revision processes. These reviewing agents are instantiated with\nhuman preference-aligned LLMs whose criteria for evaluation are elicited from\nactual human judgments via LLM prompting. We experimentally validate our\nResearchAgent on scientific publications across multiple disciplines, showing\nits effectiveness in generating novel, clear, and valid ideas based on both\nhuman and model-based evaluation results. Our initial foray into AI-mediated\nscientific research has important implications for the development of future\nsystems aimed at supporting researchers in their ideation and\noperationalization of novel work.",
    "keywords": "Large Language Models, ResearchAgent, Iterative Research, Scientific Literature, Idea Generation"
  },
  {
    "Date": "2024.04",
    "Title": "Exploring Human-AI Collaboration in Agile: Customised LLM Meeting Assistants",
    "link": "https://arxiv.org/abs/2404.14871",
    "abstract": "This action research study focuses on the integration of \"AI assistants\" in\ntwo Agile software development meetings: the Daily Scrum and a feature\nrefinement, a planning meeting that is part of an in-house Scaled Agile\nframework. We discuss the critical drivers of success, and establish a link\nbetween the use of AI and team collaboration dynamics. We conclude with a list\nof lessons learnt during the interventions in an industrial context, and\nprovide a assessment checklist for companies and teams to reflect on their\nreadiness level. This paper is thus a road-map to facilitate the integration of\nAI tools in Agile setups.",
    "keywords": "Agile software development, AI assistants, Daily Scrum, feature refinement, team collaboration dynamics."
  },
  {
    "Date": "2024.05",
    "Title": "A System for Quantifying Data Science Workflows with Fine-Grained Procedural Logging and a Pilot Study",
    "link": "https://arxiv.org/abs/2405.17845",
    "abstract": "It is important for researchers to understand precisely how data scientists\nturn raw data into insights, including typical programming patterns, workflow,\nand methodology. This paper contributes a novel system, called DataInquirer,\nthat tracks incremental code executions in Jupyter notebooks (a type of\ncomputational notebook). The system allows us to quantitatively measure timing,\nworkflow, and operation frequency in data science tasks without resorting to\nhuman annotation or interview. In a series of pilot studies, we collect 97\ntraces, logging data scientist activities across four studies. While this paper\npresents a general system and data analysis approach, we focus on a\nfoundational sub-question in our pilot studies: How consistent are different\ndata scientists in analyzing the same data? We taxonomize variation between\ndata scientists on the same dataset according to three categories: semantic,\nsyntactic, and methodological. Our results suggest that there are statistically\nsignificant differences in the conclusions reached by different data scientists\non the same task and present quantitative evidence for this phenomenon.\nFurthermore, our results suggest that AI-powered code tools subtly influence\nthese results, allowing student participants to generate workflows that more\nresemble expert data practitioners.",
    "keywords": "Data Science, Workflow, Procedural Logging, Jupyter Notebooks, AI-powered Code Tools"
  },
  {
    "Date": "2024.05",
    "Title": "A FAIR and Free Prompt-based Research Assistant",
    "link": "https://arxiv.org/abs/2405.14601",
    "abstract": "This demo will present the Research Assistant (RA) tool developed to assist\nwith six main types of research tasks defined as standardized instruction\ntemplates, instantiated with user input, applied finally as prompts to\nwell-known--for their sophisticated natural language processing abilities--AI\ntools, such as ChatGPT (https://chat.openai.com/) and Gemini\n(https://gemini.google.com/app). The six research tasks addressed by RA are:\ncreating FAIR research comparisons, ideating research topics, drafting grant\napplications, writing scientific blogs, aiding preliminary peer reviews, and\nformulating enhanced literature search queries. RA's reliance on generative AI\ntools like ChatGPT or Gemini means the same research task assistance can be\noffered in any scientific discipline. We demonstrate its versatility by sharing\nRA outputs in Computer Science, Virology, and Climate Science, where the output\nwith the RA tool assistance mirrored that from a domain expert who performed\nthe same research task.",
    "keywords": "Research Assistant, FAIR, Prompt-based, Research Tasks, Generative AI"
  },
  {
    "Date": "2024.05",
    "Title": "Automated Focused Feedback Generation for Scientific Writing Assistance",
    "link": "https://arxiv.org/abs/2405.20477",
    "abstract": "Scientific writing is a challenging task, particularly for novice researchers\nwho often rely on feedback from experienced peers. Recent work has primarily\nfocused on improving surface form and style rather than manuscript content. In\nthis paper, we propose a novel task: automated focused feedback generation for\nscientific writing assistance. We present SWIF$^{2}$T: a Scientific WrIting\nFocused Feedback Tool. It is designed to generate specific, actionable and\ncoherent comments, which identify weaknesses in a scientific paper and/or\npropose revisions to it. Our approach consists of four components - planner,\ninvestigator, reviewer and controller - leveraging multiple Large Language\nModels (LLMs) to implement them. We compile a dataset of 300 peer reviews\nciting weaknesses in scientific papers and conduct human evaluation. The\nresults demonstrate the superiority in specificity, reading comprehension, and\noverall helpfulness of SWIF$^{2}$T's feedback compared to other approaches. In\nour analysis, we also identified cases where automatically generated reviews\nwere judged better than human ones, suggesting opportunities for integration of\nAI-generated feedback in scientific writing.",
    "keywords": "Automated Focused Feedback Generation, Scientific Writing, Large Language Models, Peer Reviews, Weaknesses in Scientific Papers"
  },
  {
    "Date": "2024.06",
    "Title": "BugBlitz-AI: An Intelligent QA Assistant",
    "link": "https://arxiv.org/abs/2406.04356",
    "abstract": "The evolution of software testing from manual to automated methods has\nsignificantly influenced quality assurance (QA) practices. However, challenges\npersist in post-execution phases, particularly in result analysis and\nreporting. Traditional post-execution validation phases require manual\nintervention for result analysis and report generation, leading to\ninefficiencies and potential development cycle delays. This paper introduces\nBugBlitz-AI, an AI-powered validation toolkit designed to enhance end-to-end\ntest automation by automating result analysis and bug reporting processes.\nBugBlitz-AI leverages recent advancements in artificial intelligence to reduce\nthe time-intensive tasks of manual result analysis and report generation,\nallowing QA teams to focus more on crucial aspects of product quality. By\nadopting BugBlitz-AI, organizations can advance automated testing practices and\nintegrate AI into QA processes, ensuring higher product quality and faster\ntime-to-market. The paper outlines BugBlitz-AI's architecture, discusses\nrelated work, details its quality enhancement strategies, and presents results\ndemonstrating its effectiveness in real-world scenarios.",
    "keywords": "BugBlitz-AI, AI-powered validation toolkit, automated testing, result analysis, bug reporting."
  },
  {
    "Date": "2024.04",
    "Title": "SARA: Smart AI Reading Assistant for Reading Comprehension",
    "link": "https://arxiv.org/abs/2404.06906",
    "abstract": "SARA integrates Eye Tracking and state-of-the-art large language models in a\nmixed reality framework to enhance the reading experience by providing\npersonalized assistance in real-time. By tracking eye movements, SARA\nidentifies the text segments that attract the user's attention the most and\npotentially indicate uncertain areas and comprehension issues. The process\ninvolves these key steps: text detection and extraction, gaze tracking and\nalignment, and assessment of detected reading difficulty. The results are\ncustomized solutions presented directly within the user's field of view as\nvirtual overlays on identified difficult text areas. This support enables users\nto overcome challenges like unfamiliar vocabulary and complex sentences by\noffering additional context, rephrased solutions, and multilingual help. SARA's\ninnovative approach demonstrates it has the potential to transform the reading\nexperience and improve reading proficiency.",
    "keywords": "SARA, Eye Tracking, Reading Comprehension, Large Language Models, Mixed Reality Framework."
  },
  {
    "Date": "2024.06",
    "Title": "The Use of AI-Robotic Systems for Scientific Discovery",
    "link": "https://arxiv.org/abs/2406.17835",
    "abstract": "The process of developing theories and models and testing them with\nexperiments is fundamental to the scientific method. Automating the entire\nscientific method then requires not only automation of the induction of\ntheories from data, but also experimentation from design to implementation.\nThis is the idea behind a robot scientist -- a coupled system of AI and\nlaboratory robotics that has agency to test hypotheses with real-world\nexperiments. In this chapter we explore some of the fundamentals of robot\nscientists in the philosophy of science. We also map the activities of a robot\nscientist to machine learning paradigms, and argue that the scientific method\nshares an analogy with active learning. We demonstrate these concepts using\nexamples from previous robot scientists, and also from Genesis: a next\ngeneration robot scientist designed for research in systems biology, comprising\na micro-fluidic system with 1000 computer-controlled micro-bioreactors and\ninterpretable models based in controlled vocabularies and logic.",
    "keywords": "AI-Robotic Systems, Scientific Discovery, Robot Scientist, Machine Learning, Systems Biology."
  },
  {
    "Date": "2024.07",
    "Title": "Improving Steering and Verification in AI-Assisted Data Analysis with Interactive Task Decomposition",
    "link": "https://arxiv.org/abs/2407.02651",
    "abstract": "LLM-powered tools like ChatGPT Data Analysis, have the potential to help\nusers tackle the challenging task of data analysis programming, which requires\nexpertise in data processing, programming, and statistics. However, our\nformative study (n=15) uncovered serious challenges in verifying AI-generated\nresults and steering the AI (i.e., guiding the AI system to produce the desired\noutput). We developed two contrasting approaches to address these challenges.\nThe first (Stepwise) decomposes the problem into step-by-step subgoals with\npairs of editable assumptions and code until task completion, while the second\n(Phasewise) decomposes the entire problem into three editable, logical phases:\nstructured input/output assumptions, execution plan, and code. A controlled,\nwithin-subjects experiment (n=18) compared these systems against a\nconversational baseline. Users reported significantly greater control with the\nStepwise and Phasewise systems, and found intervention, correction, and\nverification easier, compared to the baseline. The results suggest design\nguidelines and trade-offs for AI-assisted data analysis tools.",
    "keywords": "AI-Assisted Data Analysis, Interactive Task Decomposition, Steering, Verification, LLM-powered tools"
  },
  {
    "Date": "2024.07",
    "Title": "SeqMate: A Novel Large Language Model Pipeline for Automating RNA Sequencing",
    "link": "https://arxiv.org/abs/2407.03381",
    "abstract": "RNA sequencing techniques, like bulk RNA-seq and Single Cell (sc) RNA-seq,\nare critical tools for the biologist looking to analyze the genetic\nactivity/transcriptome of a tissue or cell during an experimental procedure.\nPlatforms like Illumina's next-generation sequencing (NGS) are used to produce\nthe raw data for this experimental procedure. This raw FASTQ data must then be\nprepared via a complex series of data manipulations by bioinformaticians. This\nprocess currently takes place on an unwieldy textual user interface like a\nterminal/command line that requires the user to install and import multiple\nprogram packages, preventing the untrained biologist from initiating data\nanalysis. Open-source platforms like Galaxy have produced a more user-friendly\npipeline, yet the visual interface remains cluttered and highly technical,\nremaining uninviting for the natural scientist. To address this, SeqMate is a\nuser-friendly tool that allows for one-click analytics by utilizing the power\nof a large language model (LLM) to automate both data preparation and analysis\n(differential expression, trajectory analysis, etc). Furthermore, by utilizing\nthe power of generative AI, SeqMate is also capable of analyzing such findings\nand producing written reports of upregulated/downregulated/user-prompted genes\nwith sources cited from known repositories like PubMed, PDB, and Uniprot.",
    "keywords": "RNA sequencing, Large Language Model, Data preparation, Differential expression, Generative AI"
  },
  {
    "Date": "2024.07",
    "Title": "AI-Assisted SQL Authoring at Industry Scale",
    "link": "https://arxiv.org/abs/2407.13280",
    "abstract": "SqlCompose brings generative AI into the data analytics domain. SQL is\ndeclarative, has formal table schemas, and is often written in a non-linear\nmanner. We address each of these challenges and develop a set of models that\nshows the importance of each problem. We first develop an internal SQL\nbenchmark to perform offline tests at Meta. We evaluate how well the Public\nLlama model performs. We attain a BLEU score of 53% and 24% for single- and\nmulti-line predictions, respectively. This performance is consistent with prior\nworks on imperative languages. We then fine-tune Llama on our internal data and\ndatabase schemas. SqlComposeSA substantially outperforms Llama by 16 percentage\npoints on BLEU score. SQL is often written with multiple sub queries and in a\nnon-sequential manner. We develop SqlComposeFIM which is aware of the context\nbefore and after the line(s) that need to be completed. This fill-in-the-middle\nmodel outperform SqlComposeFIM by 35 percentage points. We also measure how\noften the models get the correct table names, and SqlComposeFIM is able to do\nthis 75% of the time. Aside from our scientific research, we also roll out\nSqlComposeFIM at Meta. SqlCompose is used on a weekly basis by over 10k users\nincluding data scientists and software engineers, less than 1% of users have\ndisabled SqlCompose. We use the feedback from users to improve SqlCompose.\nInteresting positive themes include completing tedious or repetitive SQL\nclauses, suggesting boilerplate coding, and help in eliminate the need to\nremember difficult SQL syntax. The most significant negative themes was table\nand column name hallucinations, which has been reduced with the release of\nSqlComposeFIM. The SqlCompose models consistently outperform public and\ninternal LLMs, despite being smaller (7 bn and 13 bn), which provides early\nindications that smaller specialist models can outperform larger general\npurpose models.",
    "keywords": "AI-Assisted SQL, generative AI, data analytics, SQL authoring, industry scale"
  },
  {
    "Date": "2024.08",
    "Title": "The AI Scientist: Towards Fully Automated Open-Ended Scientific Discovery",
    "link": "https://arxiv.org/abs/2408.06292",
    "abstract": "One of the grand challenges of artificial general intelligence is developing\nagents capable of conducting scientific research and discovering new knowledge.\nWhile frontier models have already been used as aides to human scientists, e.g.\nfor brainstorming ideas, writing code, or prediction tasks, they still conduct\nonly a small part of the scientific process. This paper presents the first\ncomprehensive framework for fully automatic scientific discovery, enabling\nfrontier large language models to perform research independently and\ncommunicate their findings. We introduce The AI Scientist, which generates\nnovel research ideas, writes code, executes experiments, visualizes results,\ndescribes its findings by writing a full scientific paper, and then runs a\nsimulated review process for evaluation. In principle, this process can be\nrepeated to iteratively develop ideas in an open-ended fashion, acting like the\nhuman scientific community. We demonstrate its versatility by applying it to\nthree distinct subfields of machine learning: diffusion modeling,\ntransformer-based language modeling, and learning dynamics. Each idea is\nimplemented and developed into a full paper at a cost of less than $15 per\npaper. To evaluate the generated papers, we design and validate an automated\nreviewer, which we show achieves near-human performance in evaluating paper\nscores. The AI Scientist can produce papers that exceed the acceptance\nthreshold at a top machine learning conference as judged by our automated\nreviewer. This approach signifies the beginning of a new era in scientific\ndiscovery in machine learning: bringing the transformative benefits of AI\nagents to the entire research process of AI itself, and taking us closer to a\nworld where endless affordable creativity and innovation can be unleashed on\nthe world's most challenging problems. Our code is open-sourced at\nhttps://github.com/SakanaAI/AI-Scientist",
    "keywords": "Artificial General Intelligence, Scientific Discovery, Large Language Models, Machine Learning, Automated Review Process"
  },
  {
    "Date": "2024.08",
    "Title": "Genesis: Towards the Automation of Systems Biology Research",
    "link": "https://arxiv.org/abs/2408.10689",
    "abstract": "The cutting edge of applying AI to science is the closed-loop automation of\nscientific research: robot scientists. We have previously developed two robot\nscientists: `Adam' (for yeast functional biology), and `Eve' (for early-stage\ndrug design)). We are now developing a next generation robot scientist Genesis.\nWith Genesis we aim to demonstrate that an area of science can be investigated\nusing robot scientists unambiguously faster, and at lower cost, than with human\nscientists. Here we report progress on the Genesis project. Genesis is designed\nto automatically improve system biology models with thousands of interacting\ncausal components. When complete Genesis will be able to initiate and execute\nin parallel one thousand hypothesis-led closed-loop cycles of experiment\nper-day. Here we describe the core Genesis hardware: the one thousand\ncomputer-controlled $\\mu$-bioreactors. For the integrated Mass Spectrometry\nplatform we have developed AutonoMS, a system to automatically run, process,\nand analyse high-throughput experiments. We have also developed Genesis-DB, a\ndatabase system designed to enable software agents access to large quantities\nof structured domain information. We have developed RIMBO (Revisions for\nImprovements of Models in Biology Ontology) to describe the planned hundreds of\nthousands of changes to the models. We have demonstrated the utility of this\ninfrastructure by developed two relational learning bioinformatic projects.\nFinally, we describe LGEM+ a relational learning system for the automated\nabductive improvement of genome-scale metabolic models.",
    "keywords": "AI, Systems Biology, Robot Scientists, Automation, Metabolic Models"
  },
  {
    "Date": "2024.09",
    "Title": "Collective Predictive Coding as Model of Science: Formalizing Scientific Activities Towards Generative Science",
    "link": "https://arxiv.org/abs/2409.00102",
    "abstract": "This paper proposes a new conceptual framework called Collective Predictive\nCoding as a Model of Science (CPC-MS) to formalize and understand scientific\nactivities. Building on the idea of collective predictive coding originally\ndeveloped to explain symbol emergence, CPC-MS models science as a decentralized\nBayesian inference process carried out by a community of agents. The framework\ndescribes how individual scientists' partial observations and internal\nrepresentations are integrated through communication and peer review to produce\nshared external scientific knowledge. Key aspects of scientific practice like\nexperimentation, hypothesis formation, theory development, and paradigm shifts\nare mapped onto components of the probabilistic graphical model. This paper\ndiscusses how CPC-MS provides insights into issues like social objectivity in\nscience, scientific progress, and the potential impacts of AI on research. The\ngenerative view of science offers a unified way to analyze scientific\nactivities and could inform efforts to automate aspects of the scientific\nprocess. Overall, CPC-MS aims to provide an intuitive yet formal model of\nscience as a collective cognitive activity.",
    "keywords": "Collective Predictive Coding, Model of Science, Bayesian Inference, Scientific Activities, Generative Science"
  },
  {
    "Date": "2024.09",
    "Title": "SciAgents: Automating scientific discovery through multi-agent intelligent graph reasoning",
    "link": "https://arxiv.org/abs/2409.05556",
    "abstract": "A key challenge in artificial intelligence is the creation of systems capable\nof autonomously advancing scientific understanding by exploring novel domains,\nidentifying complex patterns, and uncovering previously unseen connections in\nvast scientific data. In this work, we present SciAgents, an approach that\nleverages three core concepts: (1) the use of large-scale ontological knowledge\ngraphs to organize and interconnect diverse scientific concepts, (2) a suite of\nlarge language models (LLMs) and data retrieval tools, and (3) multi-agent\nsystems with in-situ learning capabilities. Applied to biologically inspired\nmaterials, SciAgents reveals hidden interdisciplinary relationships that were\npreviously considered unrelated, achieving a scale, precision, and exploratory\npower that surpasses traditional human-driven research methods. The framework\nautonomously generates and refines research hypotheses, elucidating underlying\nmechanisms, design principles, and unexpected material properties. By\nintegrating these capabilities in a modular fashion, the intelligent system\nyields material discoveries, critique and improve existing hypotheses, retrieve\nup-to-date data about existing research, and highlights their strengths and\nlimitations. Our case studies demonstrate scalable capabilities to combine\ngenerative AI, ontological representations, and multi-agent modeling,\nharnessing a `swarm of intelligence' similar to biological systems. This\nprovides new avenues for materials discovery and accelerates the development of\nadvanced materials by unlocking Nature's design principles.",
    "keywords": "Artificial Intelligence, Multi-Agent Systems, Ontological Knowledge Graphs, Biologically Inspired Materials, Scientific Discovery Automation"
  },
  {
    "Date": "2024.09",
    "Title": "PatentGPT: A Large Language Model for Patent Drafting Using Knowledge-based Fine-tuning Method",
    "link": "https://arxiv.org/abs/2409.00092",
    "abstract": "As humanity stands on the brink of a new era of technological innovation, the\nability to rapidly transform creative ideas into protected intellectual\nproperty (IP) is more crucial than ever. However, the conventional processes\nfor patent drafting are fraught with challenges, demanding a nuanced\nunderstanding of advanced field knowledge and technical concepts. Existing\nlarge language models (LLMs), while powerful, often fall short in this IP\ncreation domain due to their lack of specialized knowledge and\ncontext-awareness necessary for generating technically accurate patent\ndocuments. To bridge this critical gap, we propose a groundbreaking framework\nfor Knowledge Fine-Tuning (KFT) of LLMs, designed to endow AI with the ability\nto autonomously mine, understand, and apply domain-specific knowledge. Our\nmodel, PatentGPT leverages a unique combination of knowledge graph-based\npre-training, domain-specific supervised fine-tuning (SFT), and reinforcement\nlearning from human feedback (RLHF). Through extensive evaluation, PatentGPT\nhas demonstrated outstanding performance, scoring up to approximately 400%\nhigher in patent related benchmark tests compared to state-of-the-art models.\nBy KFT method the model's capability to not only assist but also augment human\ncreativity and innovation, our approach sets a new standard for AI-driven\nintellectual property generation, paving the way for more efficient and\neffective invention processes.",
    "keywords": "PatentGPT, Large Language Model, Knowledge-based Fine-tuning, Patent Drafting, Intellectual Property Generation"
  },
  {
    "Date": "2024.09",
    "Title": "Steward: Natural Language Web Automation",
    "link": "https://arxiv.org/abs/2409.15441",
    "abstract": "Recently, large language models (LLMs) have demonstrated exceptional\ncapabilities in serving as the foundation for AI assistants. One emerging\napplication of LLMs, navigating through websites and interacting with UI\nelements across various web pages, remains somewhat underexplored. We introduce\nSteward, a novel LLM-powered web automation tool designed to serve as a\ncost-effective, scalable, end-to-end solution for automating web interactions.\nTraditional browser automation frameworks like Selenium, Puppeteer, and\nPlaywright are not scalable for extensive web interaction tasks, such as\nstudying recommendation algorithms on platforms like YouTube and Twitter. These\nframeworks require manual coding of interactions, limiting their utility in\nlarge-scale or dynamic contexts. Steward addresses these limitations by\nintegrating LLM capabilities with browser automation, allowing for natural\nlanguage-driven interaction with websites. Steward operates by receiving\nnatural language instructions and reactively planning and executing a sequence\nof actions on websites, looping until completion, making it a practical tool\nfor developers and researchers to use. It achieves high efficiency, completing\nactions in 8.52 to 10.14 seconds at a cost of $0.028 per action or an average\nof $0.18 per task, which is further reduced to 4.8 seconds and $0.022 through a\ncaching mechanism. It runs tasks on real websites with a 40% completion success\nrate. We discuss various design and implementation challenges, including state\nrepresentation, action sequence selection, system responsiveness, detecting\ntask completion, and caching implementation.",
    "keywords": "Large Language Models, Web Automation, Natural Language, Browser Automation, Scalability"
  },
  {
    "Date": "2024.09",
    "Title": "NoTeeline: Supporting Real-Time, Personalized Notetaking with LLM-Enhanced Micronotes",
    "link": "https://arxiv.org/abs/2409.16493",
    "abstract": "Taking notes quickly while effectively capturing key information can be\nchallenging, especially when watching videos that present simultaneous visual\nand auditory streams. Manually taken notes often miss crucial details due to\nthe fast-paced nature of the content, while automatically generated notes fail\nto incorporate user preferences and discourage active engagement with the\ncontent. To address this, we propose an interactive system, NoTeeline, for\nsupporting real-time, personalized notetaking. Given micronotes, NoTeeline\nautomatically expands them into full-fledged notes using a Large Language Model\n(LLM). The generated notes build on the content of micronotes by adding\nrelevant details while maintaining consistency with the user's writing style.\nIn a within-subjects study (n=12), we found that NoTeeline creates high-quality\nnotes that capture the essence of participant micronotes with 93.2% factual\ncorrectness and accurately align with participant writing style (8.33%\nimprovement). Using NoTeeline, participants could capture their desired notes\nwith significantly reduced mental effort, writing 47.0% less text and\ncompleting their notes in 43.9% less time compared to a manual notetaking\nbaseline. Our results suggest that NoTeeline enables users to integrate LLM\nassistance in a familiar notetaking workflow while ensuring consistency with\ntheir preferences - providing an example of how to address broader challenges\nin designing AI-assisted tools to augment human capabilities without\ncompromising user autonomy and personalization.",
    "keywords": "NoTeeline, real-time notetaking, personalized notes, LLM-enhanced micronotes, AI-assisted tools."
  },
  {
    "Date": "2024.10",
    "Title": "Many Heads Are Better Than One: Improved Scientific Idea Generation by A LLM-Based Multi-Agent System",
    "link": "https://arxiv.org/abs/2410.09403",
    "abstract": "The rapid advancement of scientific progress requires innovative tools that\ncan accelerate knowledge discovery. Although recent AI methods, particularly\nlarge language models (LLMs), have shown promise in tasks such as hypothesis\ngeneration and experimental design, they fall short of replicating the\ncollaborative nature of real-world scientific practices, where diverse experts\nwork together in teams to tackle complex problems. To address the limitations,\nwe propose an LLM-based multi-agent system, i.e., Virtual Scientists (VirSci),\ndesigned to mimic the teamwork inherent in scientific research. VirSci\norganizes a team of agents to collaboratively generate, evaluate, and refine\nresearch ideas. Through comprehensive experiments, we demonstrate that this\nmulti-agent approach outperforms the state-of-the-art method in producing novel\nscientific ideas. We further investigate the collaboration mechanisms that\ncontribute to its tendency to produce ideas with higher novelty, offering\nvaluable insights to guide future research and illuminating pathways toward\nbuilding a robust system for autonomous scientific discovery. The code is\navailable at https://github.com/open-sciencelab/Virtual-Scientists.",
    "keywords": "\"LLM-based multi-agent system\", \"scientific idea generation\", \"novelty\", \"teamwork\", \"autonomous scientific discovery\""
  },
  {
    "Date": "2024.10",
    "Title": "A Multi-LLM Orchestration Engine for Personalized, Context-Rich Assistance",
    "link": "https://arxiv.org/abs/2410.10039",
    "abstract": "In recent years, large language models have demonstrated remarkable\ncapabilities in natural language understanding and generation. However, these\nmodels often struggle with hallucinations and maintaining long term contextual\nrelevance, particularly when dealing with private or local data. This paper\npresents a novel architecture that addresses these challenges by integrating an\norchestration engine that utilizes multiple LLMs in conjunction with a temporal\ngraph database and a vector database. The proposed system captures user\ninteractions, builds a graph representation of conversations, and stores nodes\nand edges that map associations between key concepts, entities, and behaviors\nover time. This graph based structure allows the system to develop an evolving\nunderstanding of the user preferences, providing personalized and contextually\nrelevant answers. In addition to this, a vector database encodes private data\nto supply detailed information when needed, allowing the LLM to access and\nsynthesize complex responses. To further enhance reliability, the orchestration\nengine coordinates multiple LLMs to generate comprehensive answers and\niteratively reflect on their accuracy. The result is an adaptive, privacy\ncentric AI assistant capable of offering deeper, more relevant interactions\nwhile minimizing the risk of hallucinations. This paper outlines the\narchitecture, methodology, and potential applications of this system,\ncontributing a new direction in personalized, context aware AI assistance.",
    "keywords": "Multi-LLM Orchestration Engine, Personalized Assistance, Context-Rich, Temporal Graph Database, Vector Database."
  },
  {
    "Date": "2024.10",
    "Title": "Beyond-RAG: Question Identification and Answer Generation in Real-Time Conversations",
    "link": "https://arxiv.org/abs/2410.10136",
    "abstract": "In customer contact centers, human agents often struggle with long average\nhandling times (AHT) due to the need to manually interpret queries and retrieve\nrelevant knowledge base (KB) articles. While retrieval augmented generation\n(RAG) systems using large language models (LLMs) have been widely adopted in\nindustry to assist with such tasks, RAG faces challenges in real-time\nconversations, such as inaccurate query formulation and redundant retrieval of\nfrequently asked questions (FAQs). To address these limitations, we propose a\ndecision support system that can look beyond RAG by first identifying customer\nquestions in real time. If the query matches an FAQ, the system retrieves the\nanswer directly from the FAQ database; otherwise, it generates answers via RAG.\nOur approach reduces reliance on manual queries, providing responses to agents\nwithin 2 seconds. Deployed in AI-powered human-agent assist solution at Minerva\nCQ, this system improves efficiency, reduces AHT, and lowers operational costs.\nWe also introduce an automated LLM-agentic workflow to identify FAQs from\nhistorical transcripts when no predefined FAQs exist.",
    "keywords": "Real-Time Conversations, Retrieval Augmented Generation (RAG), Large Language Models (LLMs), Customer Contact Centers, Automated LLM-Agentic Workflow"
  },
  {
    "Date": "2024.10",
    "Title": "Enhancing AI Assisted Writing with One-Shot Implicit Negative Feedback",
    "link": "https://arxiv.org/abs/2410.11009",
    "abstract": "AI-mediated communication enables users to communicate more quickly and\nefficiently. Various systems have been proposed such as smart reply and\nAI-assisted writing. Yet, the heterogeneity of the forms of inputs and\narchitectures often renders it challenging to combine insights from user\nbehaviour in one system to improve performance in another. In this work, we\nconsider the case where the user does not select any of the suggested replies\nfrom a smart reply system, and how this can be used as one-shot implicit\nnegative feedback to enhance the accuracy of an AI writing model. We introduce\nNifty, an approach that uses classifier guidance to controllably integrate\nimplicit user feedback into the text generation process. Empirically, we find\nup to 34% improvement in Rouge-L, 89% improvement in generating the correct\nintent, and an 86% win-rate according to human evaluators compared to a vanilla\nAI writing system on the MultiWOZ and Schema-Guided Dialog datasets.",
    "keywords": "AI-Assisted Writing, One-Shot Implicit Negative Feedback, Classifier Guidance, Text Generation, User Feedback Integration"
  },
  {
    "Date": "2024.10",
    "Title": "AdaptoML-UX: An Adaptive User-centered GUI-based AutoML Toolkit for Non-AI Experts and HCI Researchers",
    "link": "https://arxiv.org/abs/2410.17469",
    "abstract": "The increasing integration of machine learning across various domains has\nunderscored the necessity for accessible systems that non-experts can utilize\neffectively. To address this need, the field of automated machine learning\n(AutoML) has developed tools to simplify the construction and optimization of\nML pipelines. However, existing AutoML solutions often lack efficiency in\ncreating online pipelines and ease of use for Human-Computer Interaction (HCI)\napplications. Therefore, in this paper, we introduce AdaptoML-UX, an adaptive\nframework that incorporates automated feature engineering, machine learning,\nand incremental learning to assist non-AI experts in developing robust,\nuser-centered ML models. Our toolkit demonstrates the capability to adapt\nefficiently to diverse problem domains and datasets, particularly in HCI,\nthereby reducing the necessity for manual experimentation and conserving time\nand resources. Furthermore, it supports model personalization through\nincremental learning, customizing models to individual user behaviors. HCI\nresearchers can employ AdaptoML-UX\n(\\url{https://github.com/MichaelSargious/AdaptoML_UX}) without requiring\nspecialized expertise, as it automates the selection of algorithms, feature\nengineering, and hyperparameter tuning based on the unique characteristics of\nthe data.",
    "keywords": "AutoML, User-centered, GUI-based, Non-AI Experts, HCI Researchers"
  },
  {
    "Date": "2024.10",
    "Title": "Leveraging Large Language Models for Code Translation and Software Development in Scientific Computing",
    "link": "https://arxiv.org/abs/2410.24119",
    "abstract": "The emergence of foundational models and generative artificial intelligence\n(GenAI) is poised to transform productivity in scientific computing, especially\nin code development, refactoring, and translating from one programming language\nto another. However, because the output of GenAI cannot be guaranteed to be\ncorrect, manual intervention remains necessary. Some of this intervention can\nbe automated through task-specific tools, alongside additional methodologies\nfor correctness verification and effective prompt development. We explored the\napplication of GenAI in assisting with code translation, language\ninteroperability, and codebase inspection within a legacy Fortran codebase used\nto simulate particle interactions at the Large Hadron Collider (LHC). In the\nprocess, we developed a tool, CodeScribe, which combines prompt engineering\nwith user supervision to establish an efficient process for code conversion. In\nthis paper, we demonstrate how CodeScribe assists in converting Fortran code to\nC++, generating Fortran-C APIs for integrating legacy systems with modern C++\nlibraries, and providing developer support for code organization and algorithm\nimplementation. We also address the challenges of AI-driven code translation\nand highlight its benefits for enhancing productivity in scientific computing\nworkflows.",
    "keywords": "Large Language Models, Code Translation, Software Development, Scientific Computing, GenAI"
  },
  {
    "Date": "2024.10",
    "Title": "AiSciVision: A Framework for Specializing Large Multimodal Models in Scientific Image Classification",
    "link": "https://arxiv.org/abs/2410.21480",
    "abstract": "Trust and interpretability are crucial for the use of Artificial Intelligence\n(AI) in scientific research, but current models often operate as black boxes\noffering limited transparency and justifications for their outputs. We\nintroduce AiSciVision, a framework that specializes Large Multimodal Models\n(LMMs) into interactive research partners and classification models for image\nclassification tasks in niche scientific domains. Our framework uses two key\ncomponents: (1) Visual Retrieval-Augmented Generation (VisRAG) and (2)\ndomain-specific tools utilized in an agentic workflow. To classify a target\nimage, AiSciVision first retrieves the most similar positive and negative\nlabeled images as context for the LMM. Then the LMM agent actively selects and\napplies tools to manipulate and inspect the target image over multiple rounds,\nrefining its analysis before making a final prediction. These VisRAG and\ntooling components are designed to mirror the processes of domain experts, as\nhumans often compare new data to similar examples and use specialized tools to\nmanipulate and inspect images before arriving at a conclusion. Each inference\nproduces both a prediction and a natural language transcript detailing the\nreasoning and tool usage that led to the prediction. We evaluate AiSciVision on\nthree real-world scientific image classification datasets: detecting the\npresence of aquaculture ponds, diseased eelgrass, and solar panels. Across\nthese datasets, our method outperforms fully supervised models in low and\nfull-labeled data settings. AiSciVision is actively deployed in real-world use,\nspecifically for aquaculture research, through a dedicated web application that\ndisplays and allows the expert users to converse with the transcripts. This\nwork represents a crucial step toward AI systems that are both interpretable\nand effective, advancing their use in scientific research and scientific\ndiscovery.",
    "keywords": "Artificial Intelligence, Scientific Image Classification, Large Multimodal Models, Interpretability, Trust in AI"
  },
  {
    "Date": "2024.11",
    "Title": "Semantic Navigation for AI-assisted Ideation",
    "link": "https://arxiv.org/abs/2411.03575",
    "abstract": "We present a novel AI-based ideation assistant and evaluate it in a user\nstudy with a group of innovators. The key contribution of our work is twofold:\nwe propose a method of idea exploration in a constrained domain by means of\nLLM-supported semantic navigation of problem and solution spaces, and employ\nnovel automated data input filtering to improve generations. We found that\nsemantic exploration is preferred to the traditional prompt-output\ninteractions, measured both in explicit survey rankings, and in terms of\ninnovation assistant engagement, where 2.1x more generations were performed\nusing semantic exploration. We also show that filtering input data with metrics\nsuch as relevancy, coherence and human alignment leads to improved generations\nin the same metrics as well as enhanced quality of experience among innovators.",
    "keywords": "AI-assisted Ideation, Semantic Navigation, LLM-supported, Data Input Filtering, Innovation Assistant Engagement."
  },
  {
    "Date": "2024.11",
    "Title": "MatPilot: an LLM-enabled AI Materials Scientist under the Framework of Human-Machine Collaboration",
    "link": "https://arxiv.org/abs/2411.08063",
    "abstract": "The rapid evolution of artificial intelligence, particularly large language\nmodels, presents unprecedented opportunities for materials science research. We\nproposed and developed an AI materials scientist named MatPilot, which has\nshown encouraging abilities in the discovery of new materials. The core\nstrength of MatPilot is its natural language interactive human-machine\ncollaboration, which augments the research capabilities of human scientist\nteams through a multi-agent system. MatPilot integrates unique cognitive\nabilities, extensive accumulated experience, and ongoing curiosity of\nhuman-beings with the AI agents' capabilities of advanced abstraction, complex\nknowledge storage and high-dimensional information processing. It could\ngenerate scientific hypotheses and experimental schemes, and employ predictive\nmodels and optimization algorithms to drive an automated experimental platform\nfor experiments. It turns out that our system demonstrates capabilities for\nefficient validation, continuous learning, and iterative optimization.",
    "keywords": "Artificial Intelligence, Materials Science, Large Language Models, Human-Machine Collaboration, MatPilot"
  },
  {
    "Date": "2024.11",
    "Title": "AIGS: Generating Science from AI-Powered Automated Falsification",
    "link": "https://arxiv.org/abs/2411.11910",
    "abstract": "Rapid development of artificial intelligence has drastically accelerated the\ndevelopment of scientific discovery. Trained with large-scale observation data,\ndeep neural networks extract the underlying patterns in an end-to-end manner\nand assist human researchers with highly-precised predictions in unseen\nscenarios. The recent rise of Large Language Models (LLMs) and the empowered\nautonomous agents enable scientists to gain help through interaction in\ndifferent stages of their research, including but not limited to literature\nreview, research ideation, idea implementation, and academic writing. However,\nAI researchers instantiated by foundation model empowered agents with\nfull-process autonomy are still in their infancy. In this paper, we study\n$\\textbf{AI-Generated Science}$ (AIGS), where agents independently and\nautonomously complete the entire research process and discover scientific laws.\nBy revisiting the definition of scientific research, we argue that\n$\\textit{falsification}$ is the essence of both human research process and the\ndesign of an AIGS system. Through the lens of falsification, prior systems\nattempting towards AI-Generated Science either lack the part in their design,\nor rely heavily on existing verification engines that narrow the use in\nspecialized domains. In this work, we propose Baby-AIGS as a baby-step\ndemonstration of a full-process AIGS system, which is a multi-agent system with\nagents in roles representing key research process. By introducing\nFalsificationAgent, which identify and then verify possible scientific\ndiscoveries, we empower the system with explicit falsification. Experiments on\nthree tasks preliminarily show that Baby-AIGS could produce meaningful\nscientific discoveries, though not on par with experienced human researchers.\nFinally, we discuss on the limitations of current Baby-AIGS, actionable\ninsights, and related ethical issues in detail.",
    "keywords": "Artificial Intelligence, Automated Falsification, AI-Generated Science, Large Language Models, Scientific Discovery"
  },
  {
    "Date": "2024.11",
    "Title": "CycleResearcher: Improving Automated Research via Automated Review",
    "link": "https://arxiv.org/abs/2411.00816",
    "abstract": "The automation of scientific discovery has been a long-standing goal within\nthe research community, driven by the potential to accelerate knowledge\ncreation. While significant progress has been made using commercial large\nlanguage models (LLMs) as research assistants or idea generators, the\npossibility of automating the entire research process with open-source LLMs\nremains largely unexplored. This paper explores the feasibility of using\nopen-source post-trained LLMs as autonomous agents capable of performing the\nfull cycle of automated research and review, from literature review and\nmanuscript preparation to peer review and paper refinement. Our iterative\npreference training framework consists of CycleResearcher, which conducts\nresearch tasks, and CycleReviewer, which simulates the peer review process,\nproviding iterative feedback via reinforcement learning. To train these models,\nwe develop two new datasets, Review-5k and Research-14k, reflecting real-world\nmachine learning research and peer review dynamics. Our results demonstrate\nthat CycleReviewer achieves promising performance with a 26.89\\% reduction in\nmean absolute error (MAE) compared to individual human reviewers in predicting\npaper scores, indicating the potential of LLMs to effectively assist\nexpert-level research evaluation. In research, the papers generated by the\nCycleResearcher model achieved a score of 5.36 in simulated peer reviews,\nshowing some competitiveness in terms of simulated review scores compared to\nthe preprint level of 5.24 from human experts, while still having room for\nimprovement compared to the accepted paper level of 5.69. This work represents\na significant step toward fully automated scientific inquiry, providing ethical\nsafeguards and exploring AI-driven research capabilities. The code, dataset and\nmodel weight are released at https://wengsyx.github.io/Researcher/.",
    "keywords": "Automated Research, Large Language Models (LLMs), Peer Review, Reinforcement Learning, Scientific Discovery"
  },
  {
    "Date": "2024.12",
    "Title": "A Retrieval-Augmented Generation Framework for Academic Literature Navigation in Data Science",
    "link": "https://arxiv.org/abs/2412.15404",
    "abstract": "In the rapidly evolving field of data science, efficiently navigating the\nexpansive body of academic literature is crucial for informed decision-making\nand innovation. This paper presents an enhanced Retrieval-Augmented Generation\n(RAG) application, an artificial intelligence (AI)-based system designed to\nassist data scientists in accessing precise and contextually relevant academic\nresources. The AI-powered application integrates advanced techniques, including\nthe GeneRation Of BIbliographic Data (GROBID) technique for extracting\nbibliographic information, fine-tuned embedding models, semantic chunking, and\nan abstract-first retrieval method, to significantly improve the relevance and\naccuracy of the retrieved information. This implementation of AI specifically\naddresses the challenge of academic literature navigation. A comprehensive\nevaluation using the Retrieval-Augmented Generation Assessment System (RAGAS)\nframework demonstrates substantial improvements in key metrics, particularly\nContext Relevance, underscoring the system's effectiveness in reducing\ninformation overload and enhancing decision-making processes. Our findings\nhighlight the potential of this enhanced Retrieval-Augmented Generation system\nto transform academic exploration within data science, ultimately advancing the\nworkflow of research and innovation in the field.",
    "keywords": "Retrieval-Augmented Generation, Data Science, Academic Literature Navigation, AI-Powered System, Context Relevance"
  },
  {
    "Date": "2024.12",
    "Title": "MetaScientist: A Human-AI Synergistic Framework for Automated Mechanical Metamaterial Design",
    "link": "https://arxiv.org/abs/2412.16270",
    "abstract": "The discovery of novel mechanical metamaterials, whose properties are\ndominated by their engineered structures rather than chemical composition, is a\nknowledge-intensive and resource-demanding process. To accelerate the design of\nnovel metamaterials, we present MetaScientist, a human-in-the-loop system that\nintegrates advanced AI capabilities with expert oversight with two primary\nphases: (1) hypothesis generation, where the system performs complex reasoning\nto generate novel and scientifically sound hypotheses, supported with\ndomain-specific foundation models and inductive biases retrieved from existing\nliterature; (2) 3D structure synthesis, where a 3D structure is synthesized\nwith a novel 3D diffusion model based on the textual hypothesis and refined it\nwith a LLM-based refinement model to achieve better structure properties. At\neach phase, domain experts iteratively validate the system outputs, and provide\nfeedback and supplementary materials to ensure the alignment of the outputs\nwith scientific principles and human preferences. Through extensive evaluation\nfrom human scientists, MetaScientist is able to deliver novel and valid\nmechanical metamaterial designs that have the potential to be highly impactful\nin the metamaterial field.",
    "keywords": "MetaScientist, Human-AI Synergistic Framework, Automated Mechanical Metamaterial Design, Hypothesis Generation, 3D Structure Synthesis"
  },
  {
    "Date": "2024.12",
    "Title": "Multi-Agent System for Cosmological Parameter Analysis",
    "link": "https://arxiv.org/abs/2412.00431",
    "abstract": "Multi-agent systems (MAS) utilizing multiple Large Language Model agents with\nRetrieval Augmented Generation and that can execute code locally may become\nbeneficial in cosmological data analysis. Here, we illustrate a first small\nstep towards AI-assisted analyses and a glimpse of the potential of MAS to\nautomate and optimize scientific workflows in Cosmology. The system\narchitecture of our example package, that builds upon the autogen/ag2\nframework, can be applied to MAS in any area of quantitative scientific\nresearch. The particular task we apply our methods to is the cosmological\nparameter analysis of the Atacama Cosmology Telescope lensing power spectrum\nlikelihood using Monte Carlo Markov Chains. Our work-in-progress code is open\nsource and available at https://github.com/CMBAgents/cmbagent.",
    "keywords": "Multi-Agent System, Cosmological Parameter Analysis, Large Language Model, Monte Carlo Markov Chains, Scientific Workflows"
  },
  {
    "Date": "2024.12",
    "Title": "TapeAgents: a Holistic Framework for Agent Development and Optimization",
    "link": "https://arxiv.org/abs/2412.08445",
    "abstract": "We present TapeAgents, an agent framework built around a granular, structured\nlog tape of the agent session that also plays the role of the session's\nresumable state. In TapeAgents we leverage tapes to facilitate all stages of\nthe LLM Agent development lifecycle. The agent reasons by processing the tape\nand the LLM output to produce new thought and action steps and append them to\nthe tape. The environment then reacts to the agent's actions by likewise\nappending observation steps to the tape. By virtue of this tape-centred design,\nTapeAgents can provide AI practitioners with holistic end-to-end support. At\nthe development stage, tapes facilitate session persistence, agent auditing,\nand step-by-step debugging. Post-deployment, one can reuse tapes for\nevaluation, fine-tuning, and prompt-tuning; crucially, one can adapt tapes from\nother agents or use revised historical tapes. In this report, we explain the\nTapeAgents design in detail. We demonstrate possible applications of TapeAgents\nwith several concrete examples of building monolithic agents and multi-agent\nteams, of optimizing agent prompts and finetuning the agent's LLM. We present\ntooling prototypes and report a case study where we use TapeAgents to finetune\na Llama-3.1-8B form-filling assistant to perform as well as GPT-4o while being\norders of magnitude cheaper. Lastly, our comparative analysis shows that\nTapeAgents's advantages over prior frameworks stem from our novel design of the\nLLM agent as a resumable, modular state machine with a structured\nconfiguration, that generates granular, structured logs and that can transform\nthese logs into training text -- a unique combination of features absent in\nprevious work.",
    "keywords": "TapeAgents, LLM Agent, Agent Development, Optimization, Tape-Centred Design"
  },
  {
    "Date": "2024.12",
    "Title": "LLMs can Realize Combinatorial Creativity: Generating Creative Ideas via LLMs for Scientific Research",
    "link": "https://arxiv.org/abs/2412.14141",
    "abstract": "Scientific idea generation has been extensively studied in creativity theory\nand computational creativity research, providing valuable frameworks for\nunderstanding and implementing creative processes. However, recent work using\nLarge Language Models (LLMs) for research idea generation often overlooks these\ntheoretical foundations. We present a framework that explicitly implements\ncombinatorial creativity theory using LLMs, featuring a generalization-level\nretrieval system for cross-domain knowledge discovery and a structured\ncombinatorial process for idea generation. The retrieval system maps concepts\nacross different abstraction levels to enable meaningful connections between\ndisparate domains, while the combinatorial process systematically analyzes and\nrecombines components to generate novel solutions. Experiments on the OAG-Bench\ndataset demonstrate our framework's effectiveness, consistently outperforming\nbaseline approaches in generating ideas that align with real research\ndevelopments (improving similarity scores by 7\\%-10\\% across multiple metrics).\nOur results provide strong evidence that LLMs can effectively realize\ncombinatorial creativity when guided by appropriate theoretical frameworks,\ncontributing both to practical advancement of AI-assisted research and\ntheoretical understanding of machine creativity.",
    "keywords": "Large Language Models, Combinatorial Creativity, Scientific Research, Idea Generation, Cross-Domain Knowledge Discovery."
  },
  {
    "Date": "2024.12",
    "Title": "EvoPat: A Multi-LLM-based Patents Summarization and Analysis Agent",
    "link": "https://arxiv.org/abs/2412.18100",
    "abstract": "The rapid growth of scientific techniques and knowledge is reflected in the\nexponential increase in new patents filed annually. While these patents drive\ninnovation, they also present significant burden for researchers and engineers,\nespecially newcomers. To avoid the tedious work of navigating a vast and\ncomplex landscape to identify trends and breakthroughs, researchers urgently\nneed efficient tools to summarize, evaluate, and contextualize patents,\nrevealing their innovative contributions and underlying scientific\nprinciples.To address this need, we present EvoPat, a multi-LLM-based patent\nagent designed to assist users in analyzing patents through Retrieval-Augmented\nGeneration (RAG) and advanced search strategies. EvoPat leverages multiple\nLarge Language Models (LLMs), each performing specialized roles such as\nplanning, identifying innovations, and conducting comparative evaluations. The\nsystem integrates data from local databases, including patents, literature,\nproduct catalogous, and company repositories, and online searches to provide\nup-to-date insights. The ability to collect information not included in\noriginal database automatically is also implemented. Through extensive testing\nin the natural language processing (NLP) domain, we demonstrate that EvoPat\noutperforms GPT-4 in tasks such as patent summarization, comparative analysis,\nand technical evaluation. EvoPat represents a significant step toward creating\nAI-powered tools that empower researchers and engineers to efficiently navigate\nthe complexities of the patent landscape.",
    "keywords": "EvoPat, Multi-LLM, Patent Summarization, Analysis Agent, Retrieval-Augmented Generation (RAG)"
  },
  {
    "Date": "2024.12",
    "Title": "VISION: A Modular AI Assistant for Natural Human-Instrument Interaction at Scientific User Facilities",
    "link": "https://arxiv.org/abs/2412.18161",
    "abstract": "Scientific user facilities, such as synchrotron beamlines, are equipped with\na wide array of hardware and software tools that require a codebase for\nhuman-computer-interaction. This often necessitates developers to be involved\nto establish connection between users/researchers and the complex\ninstrumentation. The advent of generative AI presents an opportunity to bridge\nthis knowledge gap, enabling seamless communication and efficient experimental\nworkflows. Here we present a modular architecture for the Virtual Scientific\nCompanion (VISION) by assembling multiple AI-enabled cognitive blocks that each\nscaffolds large language models (LLMs) for a specialized task. With VISION, we\nperformed LLM-based operation on the beamline workstation with low latency and\ndemonstrated the first voice-controlled experiment at an X-ray scattering\nbeamline. The modular and scalable architecture allows for easy adaptation to\nnew instrument and capabilities. Development on natural language-based\nscientific experimentation is a building block for an impending future where a\nscience exocortex -- a synthetic extension to the cognition of scientists --\nmay radically transform scientific practice and discovery.",
    "keywords": "AI Assistant, Natural Human-Instrument Interaction, Scientific User Facilities, Modular Architecture, Large Language Models (LLMs)"
  },
  {
    "Date": "2024.12",
    "Title": "Automated Code Review In Practice",
    "link": "https://arxiv.org/abs/2412.18531",
    "abstract": "Code review is a widespread practice to improve software quality and transfer\nknowledge. It is often seen as time-consuming due to the need for manual effort\nand potential delays. Several AI-assisted tools, such as Qodo, GitHub Copilot,\nand Coderabbit, provide automated reviews using large language models (LLMs).\nThe effects of such tools in the industry are yet to be examined.\n  This study examines the impact of LLM-based automated code review tools in an\nindustrial setting. The study was conducted within a software development\nenvironment that adopted an AI-assisted review tool (based on open-source Qodo\nPR Agent). Around 238 practitioners across ten projects had access to the tool.\nWe focused on three projects with 4,335 pull requests, 1,568 of which underwent\nautomated reviews. Data collection comprised three sources: (1) a quantitative\nanalysis of pull request data, including comment labels indicating whether\ndevelopers acted on the automated comments, (2) surveys sent to developers\nregarding their experience with reviews on individual pull requests, and (3) a\nbroader survey of 22 practitioners capturing their general opinions on\nautomated reviews.\n  73.8% of automated comments were resolved. However, the average pull request\nclosure duration increased from five hours 52 minutes to eight hours 20\nminutes, with varying trends across projects. Most practitioners reported a\nminor improvement in code quality due to automated reviews.\n  The LLM-based tool proved useful in software development, enhancing bug\ndetection, increasing awareness of code quality, and promoting best practices.\nHowever, it also led to longer pull request closure times and introduced\ndrawbacks like faulty reviews, unnecessary corrections, and irrelevant\ncomments.",
    "keywords": "Automated Code Review, Large Language Models (LLMs), Software Quality, Pull Request, AI-assisted Tools"
  },
  {
    "Date": "2025.01",
    "Title": "Agent Laboratory: Using LLM Agents as Research Assistants",
    "link": "https://arxiv.org/abs/2501.04227",
    "abstract": "Historically, scientific discovery has been a lengthy and costly process,\ndemanding substantial time and resources from initial conception to final\nresults. To accelerate scientific discovery, reduce research costs, and improve\nresearch quality, we introduce Agent Laboratory, an autonomous LLM-based\nframework capable of completing the entire research process. This framework\naccepts a human-provided research idea and progresses through three\nstages--literature review, experimentation, and report writing to produce\ncomprehensive research outputs, including a code repository and a research\nreport, while enabling users to provide feedback and guidance at each stage. We\ndeploy Agent Laboratory with various state-of-the-art LLMs and invite multiple\nresearchers to assess its quality by participating in a survey, providing human\nfeedback to guide the research process, and then evaluate the final paper. We\nfound that: (1) Agent Laboratory driven by o1-preview generates the best\nresearch outcomes; (2) The generated machine learning code is able to achieve\nstate-of-the-art performance compared to existing methods; (3) Human\ninvolvement, providing feedback at each stage, significantly improves the\noverall quality of research; (4) Agent Laboratory significantly reduces\nresearch expenses, achieving an 84% decrease compared to previous autonomous\nresearch methods. We hope Agent Laboratory enables researchers to allocate more\neffort toward creative ideation rather than low-level coding and writing,\nultimately accelerating scientific discovery.",
    "keywords": "Agent Laboratory, LLM-based framework, scientific discovery, research process, autonomous research methods"
  },
  {
    "Date": "2025.01",
    "Title": "Dolphin: Closed-loop Open-ended Auto-research through Thinking, Practice, and Feedback",
    "link": "https://arxiv.org/abs/2501.03916",
    "abstract": "The scientific research paradigm is undergoing a profound transformation\nowing to the development of Artificial Intelligence (AI). Recent works\ndemonstrate that various AI-assisted research methods can largely improve\nresearch efficiency by improving data analysis, accelerating computation, and\nfostering novel idea generation. To further move towards the ultimate goal\n(i.e., automatic scientific research), in this paper, we propose Dolphin, the\nfirst closed-loop open-ended auto-research framework to further build the\nentire process of human scientific research. Dolphin can generate research\nideas, perform experiments, and get feedback from experimental results to\ngenerate higher-quality ideas. More specifically, Dolphin first generates novel\nideas based on relevant papers which are ranked by the topic and task\nattributes. Then, the codes are automatically generated and debugged with the\nexception-traceback-guided local code structure. Finally, Dolphin automatically\nanalyzes the results of each idea and feeds the results back to the next round\nof idea generation. Experiments are conducted on the benchmark datasets of\ndifferent topics and results show that Dolphin can generate novel ideas\ncontinuously and complete the experiment in a loop. We highlight that Dolphin\ncan automatically propose methods that are comparable to the state-of-the-art\nin some tasks such as 2D image classification and 3D point classification.",
    "keywords": "Artificial Intelligence, Auto-research, Closed-loop, Open-ended, Scientific Research Paradigm"
  },
  {
    "Date": "2025.01",
    "Title": "Knowledge Retrieval Based on Generative AI",
    "link": "https://arxiv.org/abs/2501.04635",
    "abstract": "This study develops a question-answering system based on Retrieval-Augmented\nGeneration (RAG) using Chinese Wikipedia and Lawbank as retrieval sources.\nUsing TTQA and TMMLU+ as evaluation datasets, the system employs BGE-M3 for\ndense vector retrieval to obtain highly relevant search results and\nBGE-reranker to reorder these results based on query relevance. The most\npertinent retrieval outcomes serve as reference knowledge for a Large Language\nModel (LLM), enhancing its ability to answer questions and establishing a\nknowledge retrieval system grounded in generative AI. The system's\neffectiveness is assessed through a two-stage evaluation: automatic and\nassisted performance evaluations. The automatic evaluation calculates accuracy\nby comparing the model's auto-generated labels with ground truth answers,\nmeasuring performance under standardized conditions without human intervention.\nThe assisted performance evaluation involves 20 finance-related multiple-choice\nquestions answered by 20 participants without financial backgrounds. Initially,\nparticipants answer independently. Later, they receive system-generated\nreference information to assist in answering, examining whether the system\nimproves accuracy when assistance is provided. The main contributions of this\nresearch are: (1) Enhanced LLM Capability: By integrating BGE-M3 and\nBGE-reranker, the system retrieves and reorders highly relevant results,\nreduces hallucinations, and dynamically accesses authorized or public knowledge\nsources. (2) Improved Data Privacy: A customized RAG architecture enables local\noperation of the LLM, eliminating the need to send private data to external\nservers. This approach enhances data security, reduces reliance on commercial\nservices, lowers operational costs, and mitigates privacy risks.",
    "keywords": "Knowledge Retrieval, Generative AI, Retrieval-Augmented Generation (RAG), Large Language Model (LLM), Data Privacy"
  },
  {
    "Date": "2025.01",
    "Title": "Automating Care by Self-maintainability for Full Laboratory Automation",
    "link": "https://arxiv.org/abs/2501.05789",
    "abstract": "The automation of experiments in life sciences and chemistry has\nsignificantly advanced with the development of various instruments and AI\ntechnologies. However, achieving full laboratory automation, where experiments\nconceived by scientists are seamlessly executed in automated laboratories,\nremains a challenge. We identify the lack of automation in planning and\noperational tasks--critical human-managed processes collectively termed\n\"care\"--as a major barrier. Automating care is the key enabler for full\nlaboratory automation. To address this, we propose the concept of\nself-maintainability (SeM): the ability of a laboratory system to autonomously\nadapt to internal and external disturbances, maintaining operational readiness\nakin to living cells. A SeM-enabled laboratory features autonomous recognition\nof its state, dynamic resource and information management, and adaptive\nresponses to unexpected conditions. This shifts the planning and execution of\nexperimental workflows, including scheduling and reagent allocation, from\nhumans to the system. We present a conceptual framework for implementing\nSeM-enabled laboratories, comprising three modules--Requirement manager,\nLabware manager, and Device manager--and a Central manager. SeM not only\nenables scientists to execute envisioned experiments seamlessly but also\nprovides developers with a design concept that drives the technological\ninnovations needed for full automation.",
    "keywords": "Full Laboratory Automation, Self-maintainability, Experimental Workflows, Resource Management, Adaptive Responses"
  },
  {
    "Date": "2025.02",
    "Title": "AutoAgent: A Fully-Automated and Zero-Code Framework for LLM Agents",
    "link": "https://arxiv.org/abs/2502.05957",
    "abstract": "Large Language Model (LLM) Agents have demonstrated remarkable capabilities\nin task automation and intelligent decision-making, driving the widespread\nadoption of agent development frameworks such as LangChain and AutoGen.\nHowever, these frameworks predominantly serve developers with extensive\ntechnical expertise - a significant limitation considering that only 0.03 % of\nthe global population possesses the necessary programming skills. This stark\naccessibility gap raises a fundamental question: Can we enable everyone,\nregardless of technical background, to build their own LLM agents using natural\nlanguage alone? To address this challenge, we introduce AutoAgent-a\nFully-Automated and highly Self-Developing framework that enables users to\ncreate and deploy LLM agents through Natural Language Alone. Operating as an\nautonomous Agent Operating System, AutoAgent comprises four key components: i)\nAgentic System Utilities, ii) LLM-powered Actionable Engine, iii) Self-Managing\nFile System, and iv) Self-Play Agent Customization module. This lightweight yet\npowerful system enables efficient and dynamic creation and modification of\ntools, agents, and workflows without coding requirements or manual\nintervention. Beyond its code-free agent development capabilities, AutoAgent\nalso serves as a versatile multi-agent system for General AI Assistants.\nComprehensive evaluations on the GAIA benchmark demonstrate AutoAgent's\neffectiveness in generalist multi-agent tasks, surpassing existing\nstate-of-the-art methods. Furthermore, AutoAgent's Retrieval-Augmented\nGeneration (RAG)-related capabilities have shown consistently superior\nperformance compared to many alternative LLM-based solutions.",
    "keywords": "AutoAgent, Fully-Automated, Zero-Code, LLM Agents, Natural Language Alone"
  },
  {
    "Date": "2025.02",
    "Title": "AIDE: AI-Driven Exploration in the Space of Code",
    "link": "https://arxiv.org/abs/2502.13138",
    "abstract": "Machine learning, the foundation of modern artificial intelligence, has\ndriven innovations that have fundamentally transformed the world. Yet, behind\nadvancements lies a complex and often tedious process requiring labor and\ncompute intensive iteration and experimentation. Engineers and scientists\ndeveloping machine learning models spend much of their time on trial-and-error\ntasks instead of conceptualizing innovative solutions or research hypotheses.\nTo address this challenge, we introduce AI-Driven Exploration (AIDE), a machine\nlearning engineering agent powered by large language models (LLMs). AIDE frames\nmachine learning engineering as a code optimization problem, and formulates\ntrial-and-error as a tree search in the space of potential solutions. By\nstrategically reusing and refining promising solutions, AIDE effectively trades\ncomputational resources for enhanced performance, achieving state-of-the-art\nresults on multiple machine learning engineering benchmarks, including our\nKaggle evaluations, OpenAI MLE-Bench and METRs RE-Bench.",
    "keywords": "Machine Learning, Artificial Intelligence, Code Optimization, Large Language Models, Trial-and-Error Tasks"
  },
  {
    "Date": "2025.02",
    "Title": "Towards an AI co-scientist",
    "link": "https://arxiv.org/abs/2502.18864",
    "abstract": "Scientific discovery relies on scientists generating novel hypotheses that\nundergo rigorous experimental validation. To augment this process, we introduce\nan AI co-scientist, a multi-agent system built on Gemini 2.0. The AI\nco-scientist is intended to help uncover new, original knowledge and to\nformulate demonstrably novel research hypotheses and proposals, building upon\nprior evidence and aligned to scientist-provided research objectives and\nguidance. The system's design incorporates a generate, debate, and evolve\napproach to hypothesis generation, inspired by the scientific method and\naccelerated by scaling test-time compute. Key contributions include: (1) a\nmulti-agent architecture with an asynchronous task execution framework for\nflexible compute scaling; (2) a tournament evolution process for self-improving\nhypotheses generation. Automated evaluations show continued benefits of\ntest-time compute, improving hypothesis quality. While general purpose, we\nfocus development and validation in three biomedical areas: drug repurposing,\nnovel target discovery, and explaining mechanisms of bacterial evolution and\nanti-microbial resistance. For drug repurposing, the system proposes candidates\nwith promising validation findings, including candidates for acute myeloid\nleukemia that show tumor inhibition in vitro at clinically applicable\nconcentrations. For novel target discovery, the AI co-scientist proposed new\nepigenetic targets for liver fibrosis, validated by anti-fibrotic activity and\nliver cell regeneration in human hepatic organoids. Finally, the AI\nco-scientist recapitulated unpublished experimental results via a parallel in\nsilico discovery of a novel gene transfer mechanism in bacterial evolution.\nThese results, detailed in separate, co-timed reports, demonstrate the\npotential to augment biomedical and scientific discovery and usher an era of AI\nempowered scientists.",
    "keywords": "AI co-scientist, hypothesis generation, biomedical discovery, drug repurposing, novel target discovery"
  },
  {
    "Date": "2025.02",
    "Title": "ChartCitor: Multi-Agent Framework for Fine-Grained Chart Visual Attribution",
    "link": "https://arxiv.org/abs/2502.00989",
    "abstract": "Large Language Models (LLMs) can perform chart question-answering tasks but\noften generate unverified hallucinated responses. Existing answer attribution\nmethods struggle to ground responses in source charts due to limited\nvisual-semantic context, complex visual-text alignment requirements, and\ndifficulties in bounding box prediction across complex layouts. We present\nChartCitor, a multi-agent framework that provides fine-grained bounding box\ncitations by identifying supporting evidence within chart images. The system\norchestrates LLM agents to perform chart-to-table extraction, answer\nreformulation, table augmentation, evidence retrieval through pre-filtering and\nre-ranking, and table-to-chart mapping. ChartCitor outperforms existing\nbaselines across different chart types. Qualitative user studies show that\nChartCitor helps increase user trust in Generative AI by providing enhanced\nexplainability for LLM-assisted chart QA and enables professionals to be more\nproductive.",
    "keywords": "ChartCitor, Multi-Agent Framework, Fine-Grained Chart Visual Attribution, Large Language Models, Chart Question-Answering Tasks"
  },
  {
    "Date": "2025.02",
    "Title": "Knowledge Synthesis of Photosynthesis Research Using a Large Language Model",
    "link": "https://arxiv.org/abs/2502.01059",
    "abstract": "The development of biological data analysis tools and large language models\n(LLMs) has opened up new possibilities for utilizing AI in plant science\nresearch, with the potential to contribute significantly to knowledge\nintegration and research gap identification. Nonetheless, current LLMs struggle\nto handle complex biological data and theoretical models in photosynthesis\nresearch and often fail to provide accurate scientific contexts. Therefore,\nthis study proposed a photosynthesis research assistant (PRAG) based on\nOpenAI's GPT-4o with retrieval-augmented generation (RAG) techniques and prompt\noptimization. Vector databases and an automated feedback loop were used in the\nprompt optimization process to enhance the accuracy and relevance of the\nresponses to photosynthesis-related queries. PRAG showed an average improvement\nof 8.7% across five metrics related to scientific writing, with a 25.4%\nincrease in source transparency. Additionally, its scientific depth and domain\ncoverage were comparable to those of photosynthesis research papers. A\nknowledge graph was used to structure PRAG's responses with papers within and\noutside the database, which allowed PRAG to match key entities with 63% and\n39.5% of the database and test papers, respectively. PRAG can be applied for\nphotosynthesis research and broader plant science domains, paving the way for\nmore in-depth data analysis and predictive capabilities.",
    "keywords": "\"Large Language Models\", \"Photosynthesis Research\", \"Knowledge Synthesis\", \"Retrieval-Augmented Generation\", \"Prompt Optimization\""
  },
  {
    "Date": "2025.02",
    "Title": "Accelerating Scientific Research Through a Multi-LLM Framework",
    "link": "https://arxiv.org/abs/2502.07960",
    "abstract": "The exponential growth of academic publications poses challenges for the\nresearch process, such as literature review and procedural planning. Large\nLanguage Models (LLMs) have emerged as powerful AI tools, especially when\ncombined with additional tools and resources. Recent LLM-powered frameworks\noffer promising solutions for handling complex domain-specific tasks, yet their\ndomain-specific implementation limits broader applicability. This highlights\nthe need for LLM-integrated systems that can assist in cross-disciplinary\ntasks, such as streamlining the research process across science and engineering\ndisciplines. To address this need, we introduce Artificial Research Innovator\nAssistant (ARIA), a four-agent, multi-LLM framework. By emulating a team of\nexpert assistants, ARIA systematically replicates the human research workflow\nto autonomously search, retrieve, and filter hundreds of papers, subsequently\nsynthesizing relevant literature into actionable research procedures. In a case\nstudy on dropwise condensation enhancement, ARIA demonstrates its capability to\nstreamline research tasks within an hour, maintaining user oversight during\nexecution and ultimately liberating researchers from time-intensive tasks.",
    "keywords": "Large Language Models (LLMs), Multi-LLM framework, Artificial Research Innovator Assistant (ARIA), Research process, Cross-disciplinary tasks."
  },
  {
    "Date": "2025.02",
    "Title": "Automated Capability Discovery via Model Self-Exploration",
    "link": "https://arxiv.org/abs/2502.07577",
    "abstract": "Foundation models have become general-purpose assistants, exhibiting diverse\ncapabilities across numerous domains through training on web-scale data. It\nremains challenging to precisely characterize even a fraction of the full\nspectrum of capabilities and potential risks in any new model. Existing\nevaluation approaches often require significant human effort, and it is taking\nincreasing effort to design ever harder challenges for more capable models. We\nintroduce Automated Capability Discovery (ACD), a framework that designates one\nfoundation model as a scientist to systematically propose open-ended tasks\nprobing the abilities of a subject model (potentially itself). By combining\nfrontier models with ideas from the field of open-endedness, ACD automatically\nand systematically uncovers both surprising capabilities and failures in the\nsubject model. We demonstrate ACD across a range of foundation models\n(including the GPT, Claude, and Llama series), showing that it automatically\nreveals thousands of capabilities that would be challenging for any single team\nto uncover. We further validate our method's automated scoring with extensive\nhuman surveys, observing high agreement between model-generated and human\nevaluations. By leveraging foundation models' ability to both create tasks and\nself-evaluate, ACD is a significant step toward scalable, automated evaluation\nof novel AI systems. All code and evaluation logs are open-sourced at\nhttps://github.com/conglu1997/ACD.",
    "keywords": "Automated Capability Discovery, Foundation Models, Open-endedness, Evaluation, AI Systems."
  },
  {
    "Date": "2025.02",
    "Title": "CodeA11y: Making AI Coding Assistants Useful for Accessible Web Development",
    "link": "https://arxiv.org/abs/2502.10884",
    "abstract": "A persistent challenge in accessible computing is ensuring developers produce\nweb UI code that supports assistive technologies. Despite numerous specialized\naccessibility tools, novice developers often remain unaware of them, leading to\n~96% of web pages that contain accessibility violations. AI coding assistants,\nsuch as GitHub Copilot, could offer potential by generating\naccessibility-compliant code, but their impact remains uncertain. Our formative\nstudy with 16 developers without accessibility training revealed three key\nissues in AI-assisted coding: failure to prompt AI for accessibility, omitting\ncrucial manual steps like replacing placeholder attributes, and the inability\nto verify compliance. To address these issues, we developed CodeA11y, a GitHub\nCopilot Extension, that suggests accessibility-compliant code and displays\nmanual validation reminders. We evaluated it through a controlled study with\nanother 20 novice developers. Our findings demonstrate its effectiveness in\nguiding novice developers by reinforcing accessibility practices throughout\ninteractions, representing a significant step towards integrating accessibility\ninto AI coding assistants.",
    "keywords": "AI coding assistants, accessible web development, accessibility violations, GitHub Copilot, CodeA11y extension."
  },
  {
    "Date": "2025.02",
    "Title": "From Documents to Dialogue: Building KG-RAG Enhanced AI Assistants",
    "link": "https://arxiv.org/abs/2502.15237",
    "abstract": "The Adobe Experience Platform AI Assistant is a conversational tool that\nenables organizations to interact seamlessly with proprietary enterprise data\nthrough a chatbot. However, due to access restrictions, Large Language Models\n(LLMs) cannot retrieve these internal documents, limiting their ability to\ngenerate accurate zero-shot responses. To overcome this limitation, we use a\nRetrieval-Augmented Generation (RAG) framework powered by a Knowledge Graph\n(KG) to retrieve relevant information from external knowledge sources, enabling\nLLMs to answer questions over private or previously unseen document\ncollections. In this paper, we propose a novel approach for building a\nhigh-quality, low-noise KG. We apply several techniques, including incremental\nentity resolution using seed concepts, similarity-based filtering to\ndeduplicate entries, assigning confidence scores to entity-relation pairs to\nfilter for high-confidence pairs, and linking facts to source documents for\nprovenance. Our KG-RAG system retrieves relevant tuples, which are added to the\nuser prompts context before being sent to the LLM generating the response. Our\nevaluation demonstrates that this approach significantly enhances response\nrelevance, reducing irrelevant answers by over 50% and increasing fully\nrelevant answers by 88% compared to the existing production system.",
    "keywords": "Knowledge Graph, Retrieval-Augmented Generation, Large Language Models, AI Assistant, Response Relevance."
  },
  {
    "Date": "2025.02",
    "Title": "AI-Instruments: Embodying Prompts as Instruments to Abstract & Reflect Graphical Interface Commands as General-Purpose Tools",
    "link": "https://arxiv.org/abs/2502.18736",
    "abstract": "Chat-based prompts respond with verbose linear-sequential texts, making it\ndifficult to explore and refine ambiguous intents, back up and reinterpret, or\nshift directions in creative AI-assisted design work. AI-Instruments instead\nembody \"prompts\" as interface objects via three key principles: (1) Reification\nof user-intent as reusable direct-manipulation instruments; (2) Reflection of\nmultiple interpretations of ambiguous user-intents (Reflection-in-intent) as\nwell as the range of AI-model responses (Reflection-in-response) to inform\ndesign \"moves\" towards a desired result; and (3) Grounding to instantiate an\ninstrument from an example, result, or extrapolation directly from another\ninstrument. Further, AI-Instruments leverage LLM's to suggest, vary, and refine\nnew instruments, enabling a system that goes beyond hard-coded functionality by\ngenerating its own instrumental controls from content. We demonstrate four\ntechnology probes, applied to image generation, and qualitative insights from\ntwelve participants, showing how AI-Instruments address challenges of intent\nformulation, steering via direct manipulation, and non-linear iterative\nworkflows to reflect and resolve ambiguous intents.",
    "keywords": "AI-Instruments, Direct Manipulation, Ambiguous Intents, Creative AI-Assisted Design, Graphical Interface Commands."
  },
  {
    "Date": "2025.03",
    "Title": "Enabling AI Scientists to Recognize Innovation: A Domain-Agnostic Algorithm for Assessing Novelty",
    "link": "https://arxiv.org/abs/2503.01508",
    "abstract": "In the pursuit of Artificial General Intelligence (AGI), automating the\ngeneration and evaluation of novel research ideas is a key challenge in\nAI-driven scientific discovery. This paper presents Relative Neighbor Density\n(RND), a domain-agnostic algorithm for novelty assessment in research ideas\nthat overcomes the limitations of existing approaches by comparing an idea's\nlocal density with its adjacent neighbors' densities. We first developed a\nscalable methodology to create test set without expert labeling, addressing a\nfundamental challenge in novelty assessment. Using these test sets, we\ndemonstrate that our RND algorithm achieves state-of-the-art (SOTA) performance\nin computer science (AUROC=0.820) and biomedical research (AUROC=0.765)\ndomains. Most significantly, while SOTA models like Sonnet-3.7 and existing\nmetrics show domain-specific performance degradation, RND maintains consistent\naccuracies across domains by its domain-invariant property, outperforming all\nbenchmarks by a substantial margin (0.795 v.s. 0.597) on cross-domain\nevaluation. These results validate RND as a generalizable solution for\nautomated novelty assessment in scientific research.",
    "keywords": "Artificial General Intelligence (AGI), novelty assessment, Relative Neighbor Density (RND), domain-agnostic algorithm, scientific discovery."
  }
]