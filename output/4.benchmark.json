[
  {
    "Date": "2022.09",
    "Title": "Learn to Explain: Multimodal Reasoning via Thought Chains for Science Question Answering",
    "link": "https://arxiv.org/abs/2209.09513",
    "abstract": "When answering a question, humans utilize the information available across\ndifferent modalities to synthesize a consistent and complete chain of thought\n(CoT). This process is normally a black box in the case of deep learning models\nlike large-scale language models. Recently, science question benchmarks have\nbeen used to diagnose the multi-hop reasoning ability and interpretability of\nan AI system. However, existing datasets fail to provide annotations for the\nanswers, or are restricted to the textual-only modality, small scales, and\nlimited domain diversity. To this end, we present Science Question Answering\n(ScienceQA), a new benchmark that consists of ~21k multimodal multiple choice\nquestions with a diverse set of science topics and annotations of their answers\nwith corresponding lectures and explanations. We further design language models\nto learn to generate lectures and explanations as the chain of thought (CoT) to\nmimic the multi-hop reasoning process when answering ScienceQA questions.\nScienceQA demonstrates the utility of CoT in language models, as CoT improves\nthe question answering performance by 1.20% in few-shot GPT-3 and 3.99% in\nfine-tuned UnifiedQA. We also explore the upper bound for models to leverage\nexplanations by feeding those in the input; we observe that it improves the\nfew-shot performance of GPT-3 by 18.96%. Our analysis further shows that\nlanguage models, similar to humans, benefit from explanations to learn from\nfewer data and achieve the same performance with just 40% of the data. The data\nand code are available at https://scienceqa.github.io.",
    "keywords": "Multimodal Reasoning, Chain of Thought (CoT), Science Question Answering, Language Models, Interpretability"
  },
  {
    "Date": "2023.08",
    "Title": "BOLAA: Benchmarking and Orchestrating LLM-augmented Autonomous Agents",
    "link": "https://arxiv.org/abs/2308.05960",
    "abstract": "The massive successes of large language models (LLMs) encourage the emerging\nexploration of LLM-augmented Autonomous Agents (LAAs). An LAA is able to\ngenerate actions with its core LLM and interact with environments, which\nfacilitates the ability to resolve complex tasks by conditioning on past\ninteractions such as observations and actions. Since the investigation of LAA\nis still very recent, limited explorations are available. Therefore, we provide\na comprehensive comparison of LAA in terms of both agent architectures and LLM\nbackbones. Additionally, we propose a new strategy to orchestrate multiple LAAs\nsuch that each labor LAA focuses on one type of action, \\textit{i.e.} BOLAA,\nwhere a controller manages the communication among multiple agents. We conduct\nsimulations on both decision-making and multi-step reasoning environments,\nwhich comprehensively justify the capacity of LAAs. Our performance results\nprovide quantitative suggestions for designing LAA architectures and the\noptimal choice of LLMs, as well as the compatibility of both. We release our\nimplementation code of LAAs to the public at\n\\url{https://github.com/salesforce/BOLAA}.",
    "keywords": "Large Language Models (LLMs), Autonomous Agents, Benchmarking, Orchestrating, Multi-step Reasoning"
  },
  {
    "Date": "2024.05",
    "Title": "\"Turing Tests\" For An AI Scientist",
    "link": "https://arxiv.org/abs/2405.13352",
    "abstract": "While LLMs have shown impressive capabilities in solving math or coding\nproblems, the ability to make scientific discoveries remains a distinct\nchallenge. This paper proposes a \"Turing test for an AI scientist\" to assess\nwhether an AI agent can conduct scientific research independently, without\nrelying on human-generated knowledge. Drawing inspiration from the historical\ndevelopment of science, we propose seven benchmark tests that evaluate an AI\nagent's ability to make groundbreaking discoveries in various scientific\ndomains. These tests include inferring the heliocentric model from celestial\nobservations, discovering the laws of motion in a simulated environment,\nderiving the differential equation governing vibrating strings, inferring\nMaxwell's equations from electrodynamics simulations, inventing numerical\nmethods for initial value problems, discovering Huffman coding for data\ncompression, and developing efficient sorting algorithms. To ensure the\nvalidity of these tests, the AI agent is provided with interactive libraries or\ndatasets specific to each problem, without access to human knowledge that could\npotentially contain information about the target discoveries. The ultimate goal\nis to create an AI scientist capable of making novel and impactful scientific\ndiscoveries, surpassing the best human experts in their respective fields.\nThese \"Turing tests\" serve as intermediate milestones, assessing the AI agent's\nability to make discoveries that were groundbreaking in their time. If an AI\nagent can pass the majority of these seven tests, it would indicate significant\nprogress towards building an AI scientist, paving the way for future\nadvancements in autonomous scientific discovery. This paper aims to establish a\nbenchmark for the capabilities of AI in scientific research and to stimulate\nfurther research in this exciting field.",
    "keywords": "AI Scientist, Turing Test, Scientific Discovery, Benchmark Tests, Autonomous Research"
  },
  {
    "Date": "2024.07",
    "Title": "SciCode: A Research Coding Benchmark Curated by Scientists",
    "link": "https://arxiv.org/abs/2407.13168",
    "abstract": "Since language models (LMs) now outperform average humans on many challenging\ntasks, it has become increasingly difficult to develop challenging,\nhigh-quality, and realistic evaluations. We address this issue by examining\nLMs' capabilities to generate code for solving real scientific research\nproblems. Incorporating input from scientists and AI researchers in 16 diverse\nnatural science sub-fields, including mathematics, physics, chemistry, biology,\nand materials science, we created a scientist-curated coding benchmark,\nSciCode. The problems in SciCode naturally factorize into multiple subproblems,\neach involving knowledge recall, reasoning, and code synthesis. In total,\nSciCode contains 338 subproblems decomposed from 80 challenging main problems.\nIt offers optional descriptions specifying useful scientific background\ninformation and scientist-annotated gold-standard solutions and test cases for\nevaluation. Claude3.5-Sonnet, the best-performing model among those tested, can\nsolve only 4.6% of the problems in the most realistic setting. We believe that\nSciCode demonstrates both contemporary LMs' progress towards becoming helpful\nscientific assistants and sheds light on the development and evaluation of\nscientific AI in the future.",
    "keywords": "Language Models, SciCode, Scientific Research Problems, Coding Benchmark, AI Evaluation"
  },
  {
    "Date": "2023.07",
    "Title": "MegaWika: Millions of reports and their sources across 50 diverse languages",
    "link": "https://arxiv.org/abs/2307.07049",
    "abstract": "To foster the development of new models for collaborative AI-assisted report\ngeneration, we introduce MegaWika, consisting of 13 million Wikipedia articles\nin 50 diverse languages, along with their 71 million referenced source\nmaterials. We process this dataset for a myriad of applications, going beyond\nthe initial Wikipedia citation extraction and web scraping of content,\nincluding translating non-English articles for cross-lingual applications and\nproviding FrameNet parses for automated semantic analysis. MegaWika is the\nlargest resource for sentence-level report generation and the only report\ngeneration dataset that is multilingual. We manually analyze the quality of\nthis resource through a semantically stratified sample. Finally, we provide\nbaseline results and trained models for crucial steps in automated report\ngeneration: cross-lingual question answering and citation retrieval.",
    "keywords": "MegaWika, collaborative AI, report generation, multilingual, cross-lingual question answering."
  },
  {
    "Date": "2023.08",
    "Title": "LatEval: An Interactive LLMs Evaluation Benchmark with Incomplete Information from Lateral Thinking Puzzles",
    "link": "https://arxiv.org/abs/2308.10855",
    "abstract": "With the continuous evolution and refinement of LLMs, they are endowed with\nimpressive logical reasoning or vertical thinking capabilities. But can they\nthink out of the box? Do they possess proficient lateral thinking abilities?\nFollowing the setup of Lateral Thinking Puzzles, we propose a novel evaluation\nbenchmark, LatEval, which assesses the model's lateral thinking within an\ninteractive framework. In our benchmark, we challenge LLMs with 2 aspects: the\nquality of questions posed by the model and the model's capability to integrate\ninformation for problem-solving. We find that nearly all LLMs struggle with\nemploying lateral thinking during interactions. For example, even the most\nadvanced model, GPT-4, exhibits the advantage to some extent, yet still\nmaintain a noticeable gap when compared to human. This evaluation benchmark\nprovides LLMs with a highly challenging and distinctive task that is crucial to\nan effective AI assistant.",
    "keywords": "LLMs, Lateral Thinking, LatEval, Interactive Evaluation, Benchmarking"
  },
  {
    "Date": "2023.10",
    "Title": "OceanGPT: A Large Language Model for Ocean Science Tasks",
    "link": "https://arxiv.org/abs/2310.02031",
    "abstract": "Ocean science, which delves into the oceans that are reservoirs of life and\nbiodiversity, is of great significance given that oceans cover over 70% of our\nplanet's surface. Recently, advances in Large Language Models (LLMs) have\ntransformed the paradigm in science. Despite the success in other domains,\ncurrent LLMs often fall short in catering to the needs of domain experts like\noceanographers, and the potential of LLMs for ocean science is under-explored.\nThe intrinsic reasons are the immense and intricate nature of ocean data as\nwell as the necessity for higher granularity and richness in knowledge. To\nalleviate these issues, we introduce OceanGPT, the first-ever large language\nmodel in the ocean domain, which is expert in various ocean science tasks. We\nalso propose OceanGPT, a novel framework to automatically obtain a large volume\nof ocean domain instruction data, which generates instructions based on\nmulti-agent collaboration. Additionally, we construct the first oceanography\nbenchmark, OceanBench, to evaluate the capabilities of LLMs in the ocean\ndomain. Though comprehensive experiments, OceanGPT not only shows a higher\nlevel of knowledge expertise for oceans science tasks but also gains\npreliminary embodied intelligence capabilities in ocean technology.",
    "keywords": "OceanGPT, Large Language Model, Ocean Science, OceanBench, Embodied Intelligence"
  },
  {
    "Date": "2023.11",
    "Title": "GAIA: a benchmark for General AI Assistants",
    "link": "https://arxiv.org/abs/2311.12983",
    "abstract": "We introduce GAIA, a benchmark for General AI Assistants that, if solved,\nwould represent a milestone in AI research. GAIA proposes real-world questions\nthat require a set of fundamental abilities such as reasoning, multi-modality\nhandling, web browsing, and generally tool-use proficiency. GAIA questions are\nconceptually simple for humans yet challenging for most advanced AIs: we show\nthat human respondents obtain 92\\% vs. 15\\% for GPT-4 equipped with plugins.\nThis notable performance disparity contrasts with the recent trend of LLMs\noutperforming humans on tasks requiring professional skills in e.g. law or\nchemistry. GAIA's philosophy departs from the current trend in AI benchmarks\nsuggesting to target tasks that are ever more difficult for humans. We posit\nthat the advent of Artificial General Intelligence (AGI) hinges on a system's\ncapability to exhibit similar robustness as the average human does on such\nquestions. Using GAIA's methodology, we devise 466 questions and their answer.\nWe release our questions while retaining answers to 300 of them to power a\nleader-board available at https://huggingface.co/gaia-benchmark.",
    "keywords": "General AI Assistants, benchmark, reasoning, multi-modality handling, tool-use proficiency"
  },
  {
    "Date": "2024.02",
    "Title": "LHRS-Bot: Empowering Remote Sensing with VGI-Enhanced Large Multimodal Language Model",
    "link": "https://arxiv.org/abs/2402.02544",
    "abstract": "The revolutionary capabilities of large language models (LLMs) have paved the\nway for multimodal large language models (MLLMs) and fostered diverse\napplications across various specialized domains. In the remote sensing (RS)\nfield, however, the diverse geographical landscapes and varied objects in RS\nimagery are not adequately considered in recent MLLM endeavors. To bridge this\ngap, we construct a large-scale RS image-text dataset, LHRS-Align, and an\ninformative RS-specific instruction dataset, LHRS-Instruct, leveraging the\nextensive volunteered geographic information (VGI) and globally available RS\nimages. Building on this foundation, we introduce LHRS-Bot, an MLLM tailored\nfor RS image understanding through a novel multi-level vision-language\nalignment strategy and a curriculum learning method. Additionally, we introduce\nLHRS-Bench, a benchmark for thoroughly evaluating MLLMs' abilities in RS image\nunderstanding. Comprehensive experiments demonstrate that LHRS-Bot exhibits a\nprofound understanding of RS images and the ability to perform nuanced\nreasoning within the RS domain.",
    "keywords": "Large Language Models, Multimodal Large Language Models, Remote Sensing, Volunteered Geographic Information, Image Understanding"
  },
  {
    "Date": "2024.06",
    "Title": "MASSW: A New Dataset and Benchmark Tasks for AI-Assisted Scientific Workflows",
    "link": "https://arxiv.org/abs/2406.06357",
    "abstract": "Scientific innovation relies on detailed workflows, which include critical\nsteps such as analyzing literature, generating ideas, validating these ideas,\ninterpreting results, and inspiring follow-up research. However, scientific\npublications that document these workflows are extensive and unstructured. This\nmakes it difficult for both human researchers and AI systems to effectively\nnavigate and explore the space of scientific innovation. To address this issue,\nwe introduce MASSW, a comprehensive text dataset on Multi-Aspect Summarization\nof Scientific Workflows. MASSW includes more than 152,000 peer-reviewed\npublications from 17 leading computer science conferences spanning the past 50\nyears. Using Large Language Models (LLMs), we automatically extract five core\naspects from these publications -- context, key idea, method, outcome, and\nprojected impact -- which correspond to five key steps in the research\nworkflow. These structured summaries facilitate a variety of downstream tasks\nand analyses. The quality of the LLM-extracted summaries is validated by\ncomparing them with human annotations. We demonstrate the utility of MASSW\nthrough multiple novel machine-learning tasks that can be benchmarked using\nthis new dataset, which make various types of predictions and recommendations\nalong the scientific workflow. MASSW holds significant potential for\nresearchers to create and benchmark new AI methods for optimizing scientific\nworkflows and fostering scientific innovation in the field. Our dataset is\nopenly available at \\url{https://github.com/xingjian-zhang/massw}.",
    "keywords": "AI-Assisted Scientific Workflows, MASSW Dataset, Multi-Aspect Summarization, Large Language Models, Scientific Innovation"
  },
  {
    "Date": "2024.07",
    "Title": "MMSci: A Dataset for Graduate-Level Multi-Discipline Multimodal Scientific Understanding",
    "link": "https://arxiv.org/abs/2407.04903",
    "abstract": "Scientific figure interpretation is a crucial capability for AI-driven\nscientific assistants built on advanced Large Vision Language Models. However,\ncurrent datasets and benchmarks primarily focus on simple charts or other\nrelatively straightforward figures from limited science domains. To address\nthis gap, we present a comprehensive dataset compiled from peer-reviewed Nature\nCommunications articles covering 72 scientific fields, encompassing complex\nvisualizations such as schematic diagrams, microscopic images, and experimental\ndata which require graduate-level expertise to interpret. We evaluated 19\nproprietary and open-source models on two benchmark tasks, figure captioning\nand multiple-choice, and conducted human expert annotation. Our analysis\nrevealed significant task challenges and performance gaps among models. Beyond\nserving as a benchmark, this dataset serves as a valuable resource for\nlarge-scale training. Fine-tuning Qwen2-VL-7B with our task-specific data\nachieved better performance than GPT-4o and even human experts in\nmultiple-choice evaluations. Furthermore, continuous pre-training on our\ninterleaved article and figure data substantially enhanced the model's\ndownstream task performance in materials science. We have released our dataset\nto support further research.",
    "keywords": "Scientific figure interpretation, Large Vision Language Models, graduate-level expertise, multimodal scientific understanding, benchmark dataset."
  },
  {
    "Date": "2024.08",
    "Title": "GMAI-MMBench: A Comprehensive Multimodal Evaluation Benchmark Towards General Medical AI",
    "link": "https://arxiv.org/abs/2408.03361",
    "abstract": "Large Vision-Language Models (LVLMs) are capable of handling diverse data\ntypes such as imaging, text, and physiological signals, and can be applied in\nvarious fields. In the medical field, LVLMs have a high potential to offer\nsubstantial assistance for diagnosis and treatment. Before that, it is crucial\nto develop benchmarks to evaluate LVLMs' effectiveness in various medical\napplications. Current benchmarks are often built upon specific academic\nliterature, mainly focusing on a single domain, and lacking varying perceptual\ngranularities. Thus, they face specific challenges, including limited clinical\nrelevance, incomplete evaluations, and insufficient guidance for interactive\nLVLMs. To address these limitations, we developed the GMAI-MMBench, the most\ncomprehensive general medical AI benchmark with well-categorized data structure\nand multi-perceptual granularity to date. It is constructed from 284 datasets\nacross 38 medical image modalities, 18 clinical-related tasks, 18 departments,\nand 4 perceptual granularities in a Visual Question Answering (VQA) format.\nAdditionally, we implemented a lexical tree structure that allows users to\ncustomize evaluation tasks, accommodating various assessment needs and\nsubstantially supporting medical AI research and applications. We evaluated 50\nLVLMs, and the results show that even the advanced GPT-4o only achieves an\naccuracy of 53.96%, indicating significant room for improvement. Moreover, we\nidentified five key insufficiencies in current cutting-edge LVLMs that need to\nbe addressed to advance the development of better medical applications. We\nbelieve that GMAI-MMBench will stimulate the community to build the next\ngeneration of LVLMs toward GMAI.",
    "keywords": "Vision-Language Models, GMAI-MMBench, Medical AI Benchmark, Multimodal Evaluation, General Medical AI"
  },
  {
    "Date": "2024.09",
    "Title": "DSBench: How Far Are Data Science Agents to Becoming Data Science Experts?",
    "link": "https://arxiv.org/abs/2409.07703",
    "abstract": "Large Language Models (LLMs) and Large Vision-Language Models (LVLMs) have\ndemonstrated impressive language/vision reasoning abilities, igniting the\nrecent trend of building agents for targeted applications such as shopping\nassistants or AI software engineers. Recently, many data science benchmarks\nhave been proposed to investigate their performance in the data science domain.\nHowever, existing data science benchmarks still fall short when compared to\nreal-world data science applications due to their simplified settings. To\nbridge this gap, we introduce DSBench, a comprehensive benchmark designed to\nevaluate data science agents with realistic tasks. This benchmark includes 466\ndata analysis tasks and 74 data modeling tasks, sourced from Eloquence and\nKaggle competitions. DSBench offers a realistic setting by encompassing long\ncontexts, multimodal task backgrounds, reasoning with large data files and\nmulti-table structures, and performing end-to-end data modeling tasks. Our\nevaluation of state-of-the-art LLMs, LVLMs, and agents shows that they struggle\nwith most tasks, with the best agent solving only 34.12% of data analysis tasks\nand achieving a 34.74% Relative Performance Gap (RPG). These findings\nunderscore the need for further advancements in developing more practical,\nintelligent, and autonomous data science agents.",
    "keywords": "Large Language Models, Data Science Benchmarks, Data Science Agents, Realistic Tasks, Performance Evaluation"
  },
  {
    "Date": "2024.09",
    "Title": "ChemDFM-X: Towards Large Multimodal Model for Chemistry",
    "link": "https://arxiv.org/abs/2409.13194",
    "abstract": "Rapid developments of AI tools are expected to offer unprecedented assistance\nto the research of natural science including chemistry. However, neither\nexisting unimodal task-specific specialist models nor emerging general large\nmultimodal models (LMM) can cover the wide range of chemical data modality and\ntask categories. To address the real demands of chemists, a cross-modal\nChemical General Intelligence (CGI) system, which serves as a truly practical\nand useful research assistant utilizing the great potential of LMMs, is in\ngreat need. In this work, we introduce the first Cross-modal Dialogue\nFoundation Model for Chemistry (ChemDFM-X). Diverse multimodal data are\ngenerated from an initial modality by approximate calculations and\ntask-specific model predictions. This strategy creates sufficient chemical\ntraining corpora, while significantly reducing excessive expense, resulting in\nan instruction-tuning dataset containing 7.6M data. After instruction\nfinetuning, ChemDFM-X is evaluated on extensive experiments of different\nchemical tasks with various data modalities. The results demonstrate the\ncapacity of ChemDFM-X for multimodal and inter-modal knowledge comprehension.\nChemDFM-X marks a significant milestone toward aligning all modalities in\nchemistry, a step closer to CGI.",
    "keywords": "AI tools, Chemistry, Multimodal Model, Chemical General Intelligence, Instruction-tuning dataset"
  },
  {
    "Date": "2024.09",
    "Title": "CI-Bench: Benchmarking Contextual Integrity of AI Assistants on Synthetic Data",
    "link": "https://arxiv.org/abs/2409.13903",
    "abstract": "Advances in generative AI point towards a new era of personalized\napplications that perform diverse tasks on behalf of users. While general AI\nassistants have yet to fully emerge, their potential to share personal data\nraises significant privacy challenges. This paper introduces CI-Bench, a\ncomprehensive synthetic benchmark for evaluating the ability of AI assistants\nto protect personal information during model inference. Leveraging the\nContextual Integrity framework, our benchmark enables systematic assessment of\ninformation flow across important context dimensions, including roles,\ninformation types, and transmission principles. We present a novel, scalable,\nmulti-step synthetic data pipeline for generating natural communications,\nincluding dialogues and emails. Unlike previous work with smaller, narrowly\nfocused evaluations, we present a novel, scalable, multi-step data pipeline\nthat synthetically generates natural communications, including dialogues and\nemails, which we use to generate 44 thousand test samples across eight domains.\nAdditionally, we formulate and evaluate a naive AI assistant to demonstrate the\nneed for further study and careful training towards personal assistant tasks.\nWe envision CI-Bench as a valuable tool for guiding future language model\ndevelopment, deployment, system design, and dataset construction, ultimately\ncontributing to the development of AI assistants that align with users' privacy\nexpectations.",
    "keywords": "AI Assistants, Contextual Integrity, Synthetic Data, Privacy Challenges, Benchmarking."
  },
  {
    "Date": "2024.09",
    "Title": "UniSumEval: Towards Unified, Fine-Grained, Multi-Dimensional Summarization Evaluation for LLMs",
    "link": "https://arxiv.org/abs/2409.19898",
    "abstract": "Existing benchmarks for summarization quality evaluation often lack diverse\ninput scenarios, focus on narrowly defined dimensions (e.g., faithfulness), and\nstruggle with subjective and coarse-grained annotation schemes. To address\nthese shortcomings, we create UniSumEval benchmark, which extends the range of\ninput context (e.g., domain, length) and provides fine-grained,\nmulti-dimensional annotations. We use AI assistance in data creation,\nidentifying potentially hallucinogenic input texts, and also helping human\nannotators reduce the difficulty of fine-grained annotation tasks. With\nUniSumEval, we benchmark nine latest language models as summarizers, offering\ninsights into their performance across varying input contexts and evaluation\ndimensions. Furthermore, we conduct a thorough comparison of SOTA automated\nsummary evaluators. Our benchmark data will be available at\nhttps://github.com/DISL-Lab/UniSumEval-v1.0.",
    "keywords": "UniSumEval, summarization evaluation, language models, fine-grained, multi-dimensional annotations"
  },
  {
    "Date": "2024.10",
    "Title": "CodeMMLU: A Multi-Task Benchmark for Assessing Code Understanding Capabilities of CodeLLMs",
    "link": "https://arxiv.org/abs/2410.01999",
    "abstract": "Recent advancements in Code Large Language Models (CodeLLMs) have\npredominantly focused on open-ended code generation tasks, often neglecting the\ncritical aspect of code understanding and comprehension. To bridge this gap, we\npresent CodeMMLU, a comprehensive multiple-choice question-answer benchmark\ndesigned to evaluate the depth of software and code understanding in LLMs.\nCodeMMLU includes over 10,000 questions sourced from diverse domains,\nencompassing tasks such as code analysis, defect detection, and software\nengineering principles across multiple programming languages. Unlike\ntraditional benchmarks, CodeMMLU assesses models's ability to reason about code\nrather than merely generate it, providing deeper insights into their grasp of\ncomplex software concepts and systems. Our extensive evaluation reveals that\neven state-of-the-art models face significant challenges with CodeMMLU,\nhighlighting deficiencies in comprehension beyond code generation. By\nunderscoring the crucial relationship between code understanding and effective\ngeneration, CodeMMLU serves as a vital resource for advancing AI-assisted\nsoftware development, ultimately aiming to create more reliable and capable\ncoding assistants.",
    "keywords": "CodeMMLU, CodeLLMs, code understanding, benchmark, software development"
  },
  {
    "Date": "2024.10",
    "Title": "AutoPenBench: Benchmarking Generative Agents for Penetration Testing",
    "link": "https://arxiv.org/abs/2410.03225",
    "abstract": "Generative AI agents, software systems powered by Large Language Models\n(LLMs), are emerging as a promising approach to automate cybersecurity tasks.\nAmong the others, penetration testing is a challenging field due to the task\ncomplexity and the diverse strategies to simulate cyber-attacks. Despite\ngrowing interest and initial studies in automating penetration testing with\ngenerative agents, there remains a significant gap in the form of a\ncomprehensive and standard framework for their evaluation and development. This\npaper introduces AutoPenBench, an open benchmark for evaluating generative\nagents in automated penetration testing. We present a comprehensive framework\nthat includes 33 tasks, each representing a vulnerable system that the agent\nhas to attack. Tasks are of increasing difficulty levels, including in-vitro\nand real-world scenarios. We assess the agent performance with generic and\nspecific milestones that allow us to compare results in a standardised manner\nand understand the limits of the agent under test. We show the benefits of\nAutoPenBench by testing two agent architectures: a fully autonomous and a\nsemi-autonomous supporting human interaction. We compare their performance and\nlimitations. For example, the fully autonomous agent performs unsatisfactorily\nachieving a 21% Success Rate (SR) across the benchmark, solving 27% of the\nsimple tasks and only one real-world task. In contrast, the assisted agent\ndemonstrates substantial improvements, with 64% of SR. AutoPenBench allows us\nalso to observe how different LLMs like GPT-4o or OpenAI o1 impact the ability\nof the agents to complete the tasks. We believe that our benchmark fills the\ngap with a standard and flexible framework to compare penetration testing\nagents on a common ground. We hope to extend AutoPenBench along with the\nresearch community by making it available under\nhttps://github.com/lucagioacchini/auto-pen-bench.",
    "keywords": "Generative AI agents, Penetration Testing, Benchmarking, Large Language Models, Cybersecurity Tasks"
  },
  {
    "Date": "2024.10",
    "Title": "AAAR-1.0: Assessing AI's Potential to Assist Research",
    "link": "https://arxiv.org/abs/2410.22394",
    "abstract": "Numerous studies have assessed the proficiency of AI systems, particularly\nlarge language models (LLMs), in facilitating everyday tasks such as email\nwriting, question answering, and creative content generation. However,\nresearchers face unique challenges and opportunities in leveraging LLMs for\ntheir own work, such as brainstorming research ideas, designing experiments,\nand writing or reviewing papers. In this study, we introduce AAAR-1.0, a\nbenchmark dataset designed to evaluate LLM performance in three fundamental,\nexpertise-intensive research tasks: (i) EquationInference, assessing the\ncorrectness of equations based on the contextual information in paper\nsubmissions; (ii) ExperimentDesign, designing experiments to validate research\nideas and solutions; (iii) PaperWeakness, identifying weaknesses in paper\nsubmissions; and (iv) REVIEWCRITIQUE, identifying each segment in human reviews\nis deficient or not. AAAR-1.0 differs from prior benchmarks in two key ways:\nfirst, it is explicitly research-oriented, with tasks requiring deep domain\nexpertise; second, it is researcher-oriented, mirroring the primary activities\nthat researchers engage in on a daily basis. An evaluation of both open-source\nand proprietary LLMs reveals their potential as well as limitations in\nconducting sophisticated research tasks. We will keep iterating AAAR-1.0 to new\nversions.",
    "keywords": "AI Systems, Large Language Models (LLMs), Research Tasks, Benchmark Dataset, Expertise-Intensive Tasks"
  },
  {
    "Date": "2024.11",
    "Title": "INQUIRE: A Natural World Text-to-Image Retrieval Benchmark",
    "link": "https://arxiv.org/abs/2411.02537",
    "abstract": "We introduce INQUIRE, a text-to-image retrieval benchmark designed to\nchallenge multimodal vision-language models on expert-level queries. INQUIRE\nincludes iNaturalist 2024 (iNat24), a new dataset of five million natural world\nimages, along with 250 expert-level retrieval queries. These queries are paired\nwith all relevant images comprehensively labeled within iNat24, comprising\n33,000 total matches. Queries span categories such as species identification,\ncontext, behavior, and appearance, emphasizing tasks that require nuanced image\nunderstanding and domain expertise. Our benchmark evaluates two core retrieval\ntasks: (1) INQUIRE-Fullrank, a full dataset ranking task, and (2)\nINQUIRE-Rerank, a reranking task for refining top-100 retrievals. Detailed\nevaluation of a range of recent multimodal models demonstrates that INQUIRE\nposes a significant challenge, with the best models failing to achieve an\nmAP@50 above 50%. In addition, we show that reranking with more powerful\nmultimodal models can enhance retrieval performance, yet there remains a\nsignificant margin for improvement. By focusing on scientifically-motivated\necological challenges, INQUIRE aims to bridge the gap between AI capabilities\nand the needs of real-world scientific inquiry, encouraging the development of\nretrieval systems that can assist with accelerating ecological and biodiversity\nresearch. Our dataset and code are available at\nhttps://inquire-benchmark.github.io",
    "keywords": "INQUIRE, text-to-image retrieval, multimodal vision-language models, expert-level queries, ecological challenges"
  },
  {
    "Date": "2024.11",
    "Title": "SeafloorAI: A Large-scale Vision-Language Dataset for Seafloor Geological Survey",
    "link": "https://arxiv.org/abs/2411.00172",
    "abstract": "A major obstacle to the advancements of machine learning models in marine\nscience, particularly in sonar imagery analysis, is the scarcity of AI-ready\ndatasets. While there have been efforts to make AI-ready sonar image dataset\npublicly available, they suffer from limitations in terms of environment\nsetting and scale. To bridge this gap, we introduce SeafloorAI, the first\nextensive AI-ready datasets for seafloor mapping across 5 geological layers\nthat is curated in collaboration with marine scientists. We further extend the\ndataset to SeafloorGenAI by incorporating the language component in order to\nfacilitate the development of both vision- and language-capable machine\nlearning models for sonar imagery. The dataset consists of 62 geo-distributed\ndata surveys spanning 17,300 square kilometers, with 696K sonar images, 827K\nannotated segmentation masks, 696K detailed language descriptions and\napproximately 7M question-answer pairs. By making our data processing source\ncode publicly available, we aim to engage the marine science community to\nenrich the data pool and inspire the machine learning community to develop more\nrobust models. This collaborative approach will enhance the capabilities and\napplications of our datasets within both fields.",
    "keywords": "SeafloorAI, Vision-Language Dataset, Seafloor Geological Survey, Sonar Imagery Analysis, Machine Learning Models"
  },
  {
    "Date": "2024.11",
    "Title": "RedCode: Risky Code Execution and Generation Benchmark for Code Agents",
    "link": "https://arxiv.org/abs/2411.07781",
    "abstract": "With the rapidly increasing capabilities and adoption of code agents for\nAI-assisted coding, safety concerns, such as generating or executing risky\ncode, have become significant barriers to the real-world deployment of these\nagents. To provide comprehensive and practical evaluations on the safety of\ncode agents, we propose RedCode, a benchmark for risky code execution and\ngeneration: (1) RedCode-Exec provides challenging prompts that could lead to\nrisky code execution, aiming to evaluate code agents' ability to recognize and\nhandle unsafe code. We provide a total of 4,050 risky test cases in Python and\nBash tasks with diverse input formats including code snippets and natural text.\nThey covers 25 types of critical vulnerabilities spanning 8 domains (e.g.,\nwebsites, file systems). We provide Docker environments and design\ncorresponding evaluation metrics to assess their execution results. (2)\nRedCode-Gen provides 160 prompts with function signatures and docstrings as\ninput to assess whether code agents will follow instructions to generate\nharmful code or software. Our empirical findings, derived from evaluating three\nagent frameworks based on 19 LLMs, provide insights into code agents'\nvulnerabilities. For instance, evaluations on RedCode-Exec show that agents are\nmore likely to reject executing risky operations on the operating system, but\nare less likely to reject executing technically buggy code, indicating high\nrisks. Risky operations described in natural text lead to a lower rejection\nrate than those in code format. Additionally, evaluations on RedCode-Gen show\nthat more capable base models and agents with stronger overall coding\nabilities, such as GPT4, tend to produce more sophisticated and effective\nharmful software. Our findings highlight the need for stringent safety\nevaluations for diverse code agents. Our dataset and code are available at\nhttps://github.com/AI-secure/RedCode.",
    "keywords": "Code Agents, Safety Concerns, Risky Code Execution, Code Generation, Benchmark Evaluation"
  },
  {
    "Date": "2024.11",
    "Title": "LLM4DS: Evaluating Large Language Models for Data Science Code Generation",
    "link": "https://arxiv.org/abs/2411.11908",
    "abstract": "The adoption of Large Language Models (LLMs) for code generation in data\nscience offers substantial potential for enhancing tasks such as data\nmanipulation, statistical analysis, and visualization. However, the\neffectiveness of these models in the data science domain remains underexplored.\nThis paper presents a controlled experiment that empirically assesses the\nperformance of four leading LLM-based AI assistants-Microsoft Copilot (GPT-4\nTurbo), ChatGPT (o1-preview), Claude (3.5 Sonnet), and Perplexity Labs\n(Llama-3.1-70b-instruct)-on a diverse set of data science coding challenges\nsourced from the Stratacratch platform. Using the Goal-Question-Metric (GQM)\napproach, we evaluated each model's effectiveness across task types\n(Analytical, Algorithm, Visualization) and varying difficulty levels. Our\nfindings reveal that all models exceeded a 50% baseline success rate,\nconfirming their capability beyond random chance. Notably, only ChatGPT and\nClaude achieved success rates significantly above a 60% baseline, though none\nof the models reached a 70% threshold, indicating limitations in higher\nstandards. ChatGPT demonstrated consistent performance across varying\ndifficulty levels, while Claude's success rate fluctuated with task complexity.\nHypothesis testing indicates that task type does not significantly impact\nsuccess rate overall. For analytical tasks, efficiency analysis shows no\nsignificant differences in execution times, though ChatGPT tended to be slower\nand less predictable despite high success rates. This study provides a\nstructured, empirical evaluation of LLMs in data science, delivering insights\nthat support informed model selection tailored to specific task demands. Our\nfindings establish a framework for future AI assessments, emphasizing the value\nof rigorous evaluation beyond basic accuracy measures.",
    "keywords": "Large Language Models, Data Science, Code Generation, Empirical Assessment, Task Performance"
  },
  {
    "Date": "2024.12",
    "Title": "How Well Do LLMs Generate Code for Different Application Domains? Benchmark and Evaluation",
    "link": "https://arxiv.org/abs/2412.18573",
    "abstract": "Recently, an increasing number of AI-driven programming assistants powered by\ncode LLMs have been integrated into various real-world software development\nenvironments, significantly boosting developer productivity. However, existing\ncode generation benchmarks primarily focus on general-purpose scenarios,\nleaving the code generation performance of LLMs for specific application\ndomains largely unknown. In this paper, we introduce a new benchmark,\nMultiCodeBench, to fill this gap. MultiCodeBench comprises 2,400 programming\ntasks, covering 12 popular software development domains and 15 programming\nlanguages. Specifically, we perform in-depth research to identify these 12\napplication domains. Given that each domain may involve multiple technical\nframeworks, and that different frameworks present distinct challenges in the\ncoding process, we categorize the commonly used frameworks and platforms within\neach domain. We then sample programming problems from GitHub repositories\nrelated to these subdomains. To ensure the quality of the tasks and mitigate\ndata leakage issues, we invite annotators to rewrite the docstrings for each\ntask in MultiCodeBench. Additionally, we build a static analysis-based\ndependency parsing tool to extract the dependencies in the ground truth for\neach task, enabling deeper performance analysis. Through extensive experiments\non MultiCodeBench with eleven representative mainstream LLMs, we reveal the\ncode generation performance of the LLMs across different application domains,\nproviding practical insights for developers in downstream fields when selecting\nLLMs. Furthermore, we analyze the reasons behind the models' failures in\ncompleting software application development tasks, offering guidance for model\ndevelopers to enhance domain-specific code generation capabilities.",
    "keywords": "Code Generation, LLMs, Benchmark, Application Domains, MultiCodeBench"
  },
  {
    "Date": "2025.02",
    "Title": "Auto-Bench: An Automated Benchmark for Scientific Discovery in LLMs",
    "link": "https://arxiv.org/abs/2502.15224",
    "abstract": "Given the remarkable performance of Large Language Models (LLMs), an\nimportant question arises: Can LLMs conduct human-like scientific research and\ndiscover new knowledge, and act as an AI scientist? Scientific discovery is an\niterative process that demands efficient knowledge updating and encoding. It\ninvolves understanding the environment, identifying new hypotheses, and\nreasoning about actions; however, no standardized benchmark specifically\ndesigned for scientific discovery exists for LLM agents. In response to these\nlimitations, we introduce a novel benchmark, \\textit{Auto-Bench}, that\nencompasses necessary aspects to evaluate LLMs for scientific discovery in both\nnatural and social sciences. Our benchmark is based on the principles of causal\ngraph discovery. It challenges models to uncover hidden structures and make\noptimal decisions, which includes generating valid justifications. By engaging\ninteractively with an oracle, the models iteratively refine their understanding\nof underlying interactions, the chemistry and social interactions, through\nstrategic interventions. We evaluate state-of-the-art LLMs, including GPT-4,\nGemini, Qwen, Claude, and Llama, and observe a significant performance drop as\nthe problem complexity increases, which suggests an important gap between\nmachine and human intelligence that future development of LLMs need to take\ninto consideration.",
    "keywords": "Large Language Models, Scientific Discovery, Auto-Bench, Causal Graph Discovery, Machine Intelligence"
  },
  {
    "Date": "2025.02",
    "Title": "Learning to Coordinate with Experts",
    "link": "https://arxiv.org/abs/2502.09583",
    "abstract": "When deployed in dynamic environments, AI agents will inevitably encounter\nchallenges that exceed their individual capabilities. Leveraging assistance\nfrom expert agents-whether human or AI-can significantly enhance safety and\nperformance in such situations. However, querying experts is often costly,\nnecessitating the development of agents that can efficiently request and\nutilize expert guidance. In this paper, we introduce a fundamental coordination\nproblem called Learning to Yield and Request Control (YRC), where the objective\nis to learn a strategy that determines when to act autonomously and when to\nseek expert assistance. We consider a challenging practical setting in which an\nagent does not interact with experts during training but must adapt to novel\nenvironmental changes and expert interventions at test time. To facilitate\nempirical research, we introduce YRC-Bench, an open-source benchmark featuring\ndiverse domains. YRC-Bench provides a standardized Gym-like API, simulated\nexperts, evaluation pipeline, and implementation of competitive baselines.\nTowards tackling the YRC problem, we propose a novel validation approach and\ninvestigate the performance of various learning methods across diverse\nenvironments, yielding insights that can guide future research.",
    "keywords": "AI agents, expert assistance, Learning to Yield and Request Control (YRC), YRC-Bench, coordination problem."
  },
  {
    "Date": "2025.02",
    "Title": "UGPhysics: A Comprehensive Benchmark for Undergraduate Physics Reasoning with Large Language Models",
    "link": "https://arxiv.org/abs/2502.00334",
    "abstract": "Large language models (LLMs) have demonstrated remarkable capabilities in\nsolving complex reasoning tasks, particularly in mathematics. However, the\ndomain of physics reasoning presents unique challenges that have received\nsignificantly less attention. Existing benchmarks often fall short in\nevaluating LLMs' abilities on the breadth and depth of undergraduate-level\nphysics, underscoring the need for a comprehensive evaluation. To fill this\ngap, we introduce UGPhysics, a large-scale and comprehensive benchmark\nspecifically designed to evaluate UnderGraduate-level Physics (UGPhysics)\nreasoning with LLMs. UGPhysics includes 5,520 undergraduate-level physics\nproblems in both English and Chinese, covering 13 subjects with seven different\nanswer types and four distinct physics reasoning skills, all rigorously\nscreened for data leakage. Additionally, we develop a Model-Assistant\nRule-based Judgment (MARJ) pipeline specifically tailored for assessing answer\ncorrectness of physics problems, ensuring accurate evaluation. Our evaluation\nof 31 leading LLMs shows that the highest overall accuracy, 49.8% (achieved by\nOpenAI-o1-mini), emphasizes the necessity for models with stronger physics\nreasoning skills, beyond math abilities. We hope UGPhysics, along with MARJ,\nwill drive future advancements in AI for physics reasoning. Codes and data are\navailable at https://github.com/YangLabHKUST/UGPhysics .",
    "keywords": "Large language models, Physics reasoning, Undergraduate physics, Benchmark, MARJ pipeline"
  },
  {
    "Date": "2025.02",
    "Title": "Minerva: A Programmable Memory Test Benchmark for Language Models",
    "link": "https://arxiv.org/abs/2502.03358",
    "abstract": "How effectively can LLM-based AI assistants utilize their memory (context) to\nperform various tasks? Traditional data benchmarks, which are often manually\ncrafted, suffer from several limitations: they are static, susceptible to\noverfitting, difficult to interpret, and lack actionable insights--failing to\npinpoint the specific capabilities a model lacks when it does not pass a test.\nIn this paper, we present a framework for automatically generating a\ncomprehensive set of tests to evaluate models' abilities to use their memory\neffectively. Our framework extends the range of capability tests beyond the\ncommonly explored (passkey, key-value, needle in the haystack) search, a\ndominant focus in the literature. Specifically, we evaluate models on atomic\ntasks such as searching, recalling, editing, matching, comparing information in\ncontext memory, and performing basic operations when inputs are structured into\ndistinct blocks, simulating real-world data. Additionally, we design composite\ntests to investigate the models' ability to maintain state while operating on\nmemory. Our benchmark enables an interpretable, detailed assessment of memory\ncapabilities of LLMs.",
    "keywords": "LLM-based AI assistants, memory test benchmark, programmable memory, capability tests, interpretable assessment."
  },
  {
    "Date": "2025.02",
    "Title": "Insect-Foundation: A Foundation Model and Large Multimodal Dataset for Vision-Language Insect Understanding",
    "link": "https://arxiv.org/abs/2502.09906",
    "abstract": "Multimodal conversational generative AI has shown impressive capabilities in\nvarious vision and language understanding through learning massive text-image\ndata. However, current conversational models still lack knowledge about visual\ninsects since they are often trained on the general knowledge of\nvision-language data. Meanwhile, understanding insects is a fundamental problem\nin precision agriculture, helping to promote sustainable development in\nagriculture. Therefore, this paper proposes a novel multimodal conversational\nmodel, Insect-LLaVA, to promote visual understanding in insect-domain\nknowledge. In particular, we first introduce a new large-scale Multimodal\nInsect Dataset with Visual Insect Instruction Data that enables the capability\nof learning the multimodal foundation models. Our proposed dataset enables\nconversational models to comprehend the visual and semantic features of the\ninsects. Second, we propose a new Insect-LLaVA model, a new general Large\nLanguage and Vision Assistant in Visual Insect Understanding. Then, to enhance\nthe capability of learning insect features, we develop an Insect Foundation\nModel by introducing a new micro-feature self-supervised learning with a\nPatch-wise Relevant Attention mechanism to capture the subtle differences among\ninsect images. We also present Description Consistency loss to improve\nmicro-feature learning via text descriptions. The experimental results\nevaluated on our new Visual Insect Question Answering benchmarks illustrate the\neffective performance of our proposed approach in visual insect understanding\nand achieve State-of-the-Art performance on standard benchmarks of\ninsect-related tasks.",
    "keywords": "Multimodal Conversational Generative AI, Visual Insect Understanding, Insect-LLaVA, Multimodal Insect Dataset, Insect Foundation Model"
  },
  {
    "Date": "2025.02",
    "Title": "Theoretical Physics Benchmark (TPBench) -- a Dataset and Study of AI Reasoning Capabilities in Theoretical Physics",
    "link": "https://arxiv.org/abs/2502.15815",
    "abstract": "We introduce a benchmark to evaluate the capability of AI to solve problems\nin theoretical physics, focusing on high-energy theory and cosmology. The first\niteration of our benchmark consists of 57 problems of varying difficulty, from\nundergraduate to research level. These problems are novel in the sense that\nthey do not come from public problem collections. We evaluate our data set on\nvarious open and closed language models, including o3-mini, o1, DeepSeek-R1,\nGPT-4o and versions of Llama and Qwen. While we find impressive progress in\nmodel performance with the most recent models, our research-level difficulty\nproblems are mostly unsolved. We address challenges of auto-verifiability and\ngrading, and discuss common failure modes. While currently state-of-the art\nmodels are still of limited use for researchers, our results show that AI\nassisted theoretical physics research may become possible in the near future.\nWe discuss the main obstacles towards this goal and possible strategies to\novercome them. The public problems and solutions, results for various models,\nand updates to the data set and score distribution, are available on the\nwebsite of the dataset tpbench.org.",
    "keywords": "AI, Theoretical Physics, Benchmark, Problem Solving, Research Capabilities"
  }
]