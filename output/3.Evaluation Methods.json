[
  {
    "Date": "2021.10",
    "Title": "Learning to Assist Agents by Observing Them",
    "link": "https://arxiv.org/abs/2110.01311",
    "abstract": "The ability of an AI agent to assist other agents, such as humans, is an\nimportant and challenging goal, which requires the assisting agent to reason\nabout the behavior and infer the goals of the assisted agent. Training such an\nability by using reinforcement learning usually requires large amounts of\nonline training, which is difficult and costly. On the other hand, offline data\nabout the behavior of the assisted agent might be available, but is non-trivial\nto take advantage of by methods such as offline reinforcement learning. We\nintroduce methods where the capability to create a representation of the\nbehavior is first pre-trained with offline data, after which only a small\namount of interaction data is needed to learn an assisting policy. We test the\nsetting in a gridworld where the helper agent has the capability to manipulate\nthe environment of the assisted artificial agents, and introduce three\ndifferent scenarios where the assistance considerably improves the performance\nof the assisted agents.",
    "keywords": "AI agent, reinforcement learning, offline data, assisting policy, gridworld scenarios."
  },
  {
    "Date": "2021.10",
    "Title": "Explaining Reward Functions to Humans for Better Human-Robot Collaboration",
    "link": "https://arxiv.org/abs/2110.04192",
    "abstract": "Explainable AI techniques that describe agent reward functions can enhance\nhuman-robot collaboration in a variety of settings. One context where human\nunderstanding of agent reward functions is particularly beneficial is in the\nvalue alignment setting. In the value alignment context, an agent aims to infer\na human's reward function through interaction so that it can assist the human\nwith their tasks. If the human can understand where gaps exist in the agent's\nreward understanding, they will be able to teach more efficiently and\neffectively, leading to quicker human-agent team performance improvements. In\norder to support human collaborators in the value alignment setting and similar\ncontexts, it is first important to understand the effectiveness of different\nreward explanation techniques in a variety of domains. In this paper, we\nintroduce a categorization of information modalities for reward explanation\ntechniques, suggest a suite of assessment techniques for human reward\nunderstanding, and introduce four axes of domain complexity. We then propose an\nexperiment to study the relative efficacy of a broad set of reward explanation\ntechniques covering multiple modalities of information in a set of domains of\nvarying complexity.",
    "keywords": "Explainable AI, Reward Functions, Human-Robot Collaboration, Value Alignment, Reward Explanation Techniques"
  },
  {
    "Date": "2023.04",
    "Title": "VISAR: A Human-AI Argumentative Writing Assistant with Visual Programming and Rapid Draft Prototyping",
    "link": "https://arxiv.org/abs/2304.07810",
    "abstract": "In argumentative writing, writers must brainstorm hierarchical writing goals,\nensure the persuasiveness of their arguments, and revise and organize their\nplans through drafting. Recent advances in large language models (LLMs) have\nmade interactive text generation through a chat interface (e.g., ChatGPT)\npossible. However, this approach often neglects implicit writing context and\nuser intent, lacks support for user control and autonomy, and provides limited\nassistance for sensemaking and revising writing plans. To address these\nchallenges, we introduce VISAR, an AI-enabled writing assistant system designed\nto help writers brainstorm and revise hierarchical goals within their writing\ncontext, organize argument structures through synchronized text editing and\nvisual programming, and enhance persuasiveness with argumentation spark\nrecommendations. VISAR allows users to explore, experiment with, and validate\ntheir writing plans using automatic draft prototyping. A controlled lab study\nconfirmed the usability and effectiveness of VISAR in facilitating the\nargumentative writing planning process.",
    "keywords": "VISAR, AI-enabled writing assistant, argumentative writing, visual programming, rapid draft prototyping."
  },
  {
    "Date": "2023.04",
    "Title": "Evaluating the Code Quality of AI-Assisted Code Generation Tools: An Empirical Study on GitHub Copilot, Amazon CodeWhisperer, and ChatGPT",
    "link": "https://arxiv.org/abs/2304.10778",
    "abstract": "Context: AI-assisted code generation tools have become increasingly prevalent\nin software engineering, offering the ability to generate code from natural\nlanguage prompts or partial code inputs. Notable examples of these tools\ninclude GitHub Copilot, Amazon CodeWhisperer, and OpenAI's ChatGPT.\n  Objective: This study aims to compare the performance of these prominent code\ngeneration tools in terms of code quality metrics, such as Code Validity, Code\nCorrectness, Code Security, Code Reliability, and Code Maintainability, to\nidentify their strengths and shortcomings.\n  Method: We assess the code generation capabilities of GitHub Copilot, Amazon\nCodeWhisperer, and ChatGPT using the benchmark HumanEval Dataset. The generated\ncode is then evaluated based on the proposed code quality metrics.\n  Results: Our analysis reveals that the latest versions of ChatGPT, GitHub\nCopilot, and Amazon CodeWhisperer generate correct code 65.2%, 46.3%, and 31.1%\nof the time, respectively. In comparison, the newer versions of GitHub CoPilot\nand Amazon CodeWhisperer showed improvement rates of 18% for GitHub Copilot and\n7% for Amazon CodeWhisperer. The average technical debt, considering code\nsmells, was found to be 8.9 minutes for ChatGPT, 9.1 minutes for GitHub\nCopilot, and 5.6 minutes for Amazon CodeWhisperer.\n  Conclusions: This study highlights the strengths and weaknesses of some of\nthe most popular code generation tools, providing valuable insights for\npractitioners. By comparing these generators, our results may assist\npractitioners in selecting the optimal tool for specific tasks, enhancing their\ndecision-making process.",
    "keywords": "AI-assisted code generation, GitHub Copilot, Amazon CodeWhisperer, ChatGPT, code quality metrics"
  },
  {
    "Date": "2024.06",
    "Title": "A Knowledge-Component-Based Methodology for Evaluating AI Assistants",
    "link": "https://arxiv.org/abs/2406.05603",
    "abstract": "We evaluate an automatic hint generator for CS1 programming assignments\npowered by GPT-4, a large language model. This system provides natural language\nguidance about how students can improve their incorrect solutions to short\nprogramming exercises. A hint can be requested each time a student fails a test\ncase. Our evaluation addresses three Research Questions:\n  RQ1: Do the hints help students improve their code? RQ2: How effectively do\nthe hints capture problems in student code? RQ3: Are the issues that students\nresolve the same as the issues addressed in the hints?\n  To address these research questions quantitatively, we identified a set of\nfine-grained knowledge components and determined which ones apply to each\nexercise, incorrect solution, and generated hint. Comparing data from two large\nCS1 offerings, we found that access to the hints helps students to address\nproblems with their code more quickly, that hints are able to consistently\ncapture the most pressing errors in students' code, and that hints that address\na few issues at once rather than a single bug are more likely to lead to direct\nstudent progress.",
    "keywords": "AI Assistants, Knowledge-Component-Based Methodology, Evaluating, GPT-4, CS1 Programming Assignments"
  },
  {
    "Date": "2024.06",
    "Title": "AI Data Readiness Inspector (AIDRIN) for Quantitative Assessment of Data Readiness for AI",
    "link": "https://arxiv.org/abs/2406.19256",
    "abstract": "\"Garbage In Garbage Out\" is a universally agreed quote by computer scientists\nfrom various domains, including Artificial Intelligence (AI). As data is the\nfuel for AI, models trained on low-quality, biased data are often ineffective.\nComputer scientists who use AI invest a considerable amount of time and effort\nin preparing the data for AI. However, there are no standard methods or\nframeworks for assessing the \"readiness\" of data for AI. To provide a\nquantifiable assessment of the readiness of data for AI processes, we define\nparameters of AI data readiness and introduce AIDRIN (AI Data Readiness\nInspector). AIDRIN is a framework covering a broad range of readiness\ndimensions available in the literature that aid in evaluating the readiness of\ndata quantitatively and qualitatively. AIDRIN uses metrics in traditional data\nquality assessment such as completeness, outliers, and duplicates for data\nevaluation. Furthermore, AIDRIN uses metrics specific to assess data for AI,\nsuch as feature importance, feature correlations, class imbalance, fairness,\nprivacy, and FAIR (Findability, Accessibility, Interoperability, and\nReusability) principle compliance. AIDRIN provides visualizations and reports\nto assist data scientists in further investigating the readiness of data. The\nAIDRIN framework enhances the efficiency of the machine learning pipeline to\nmake informed decisions on data readiness for AI applications.",
    "keywords": "AI Data Readiness, Quantitative Assessment, Data Quality, Machine Learning Pipeline, FAIR Principles."
  },
  {
    "Date": "2024.09",
    "Title": "Benchmarking ChatGPT, Codeium, and GitHub Copilot: A Comparative Study of AI-Driven Programming and Debugging Assistants",
    "link": "https://arxiv.org/abs/2409.19922",
    "abstract": "With the increasing adoption of AI-driven tools in software development,\nlarge language models (LLMs) have become essential for tasks like code\ngeneration, bug fixing, and optimization. Tools like ChatGPT, GitHub Copilot,\nand Codeium provide valuable assistance in solving programming challenges, yet\ntheir effectiveness remains underexplored. This paper presents a comparative\nstudy of ChatGPT, Codeium, and GitHub Copilot, evaluating their performance on\nLeetCode problems across varying difficulty levels and categories. Key metrics\nsuch as success rates, runtime efficiency, memory usage, and error-handling\ncapabilities are assessed. GitHub Copilot showed superior performance on easier\nand medium tasks, while ChatGPT excelled in memory efficiency and debugging.\nCodeium, though promising, struggled with more complex problems. Despite their\nstrengths, all tools faced challenges in handling harder problems. These\ninsights provide a deeper understanding of each tool's capabilities and\nlimitations, offering guidance for developers and researchers seeking to\noptimize AI integration in coding workflows.",
    "keywords": "AI-driven tools, software development, large language models (LLMs), code generation, debugging assistants"
  },
  {
    "Date": "2024.10",
    "Title": "Aligning AI-driven discovery with human intuition",
    "link": "https://arxiv.org/abs/2410.07397",
    "abstract": "As data-driven modeling of physical dynamical systems becomes more prevalent,\na new challenge is emerging: making these models more compatible and aligned\nwith existing human knowledge. AI-driven scientific modeling processes\ntypically begin with identifying hidden state variables, then deriving\ngoverning equations, followed by predicting and analyzing future behaviors. The\ncritical initial step of identification of an appropriate set of state\nvariables remains challenging for two reasons. First, finding a compact set of\nmeaningfully predictive variables is mathematically difficult and\nunder-defined. A second reason is that variables found often lack physical\nsignificance, and are therefore difficult for human scientists to interpret. We\npropose a new general principle for distilling representations that are\nnaturally more aligned with human intuition, without relying on prior physical\nknowledge. We demonstrate our approach on a number of experimental and\nsimulated system where the variables generated by the AI closely resemble those\nchosen independently by human scientists. We suggest that this principle can\nhelp make human-AI collaboration more fruitful, as well as shed light on how\nhumans make scientific modeling choices.",
    "keywords": "AI-driven discovery, human intuition, state variables, scientific modeling, human-AI collaboration."
  },
  {
    "Date": "2024.10",
    "Title": "Vital Insight: Assisting Experts' Context-Driven Sensemaking of Multi-modal Personal Tracking Data Using Visualization and Human-In-The-Loop LLM Agents",
    "link": "https://arxiv.org/abs/2410.14879",
    "abstract": "Passive tracking methods, such as phone and wearable sensing, have become\ndominant in monitoring human behaviors in modern ubiquitous computing studies.\nWhile there have been significant advances in machine-learning approaches to\ntranslate periods of raw sensor data to model momentary behaviors, (e.g.,\nphysical activity recognition), there still remains a significant gap in the\ntranslation of these sensing streams into meaningful, high-level, context-aware\ninsights that are required for various applications (e.g., summarizing an\nindividual's daily routine). To bridge this gap, experts often need to employ a\ncontext-driven sensemaking process in real-world studies to derive insights.\nThis process often requires manual effort and can be challenging even for\nexperienced researchers due to the complexity of human behaviors.\n  We conducted three rounds of user studies with 21 experts to explore\nsolutions to address challenges with sensemaking. We follow a human-centered\ndesign process to identify needs and design, iterate, build, and evaluate Vital\nInsight (VI), a novel, LLM-assisted, prototype system to enable\nhuman-in-the-loop inference (sensemaking) and visualizations of multi-modal\npassive sensing data from smartphones and wearables. Using the prototype as a\ntechnology probe, we observe experts' interactions with it and develop an\nexpert sensemaking model that explains how experts move between direct data\nrepresentations and AI-supported inferences to explore, question, and validate\ninsights. Through this iterative process, we also synthesize and discuss a list\nof design implications for the design of future AI-augmented visualization\nsystems to better assist experts' sensemaking processes in multi-modal health\nsensing data.",
    "keywords": "Multi-modal Personal Tracking Data, Visualization, Human-In-The-Loop LLM Agents, Context-Driven Sensemaking, Ubiquitous Computing Studies"
  },
  {
    "Date": "2024.10",
    "Title": "GigaCheck: Detecting LLM-generated Content",
    "link": "https://arxiv.org/abs/2410.23728",
    "abstract": "With the increasing quality and spread of LLM-based assistants, the amount of\nLLM-generated content is growing rapidly. In many cases and tasks, such texts\nare already indistinguishable from those written by humans, and the quality of\ngeneration tends to only increase. At the same time, detection methods are\ndeveloping more slowly, making it challenging to prevent misuse of generative\nAI technologies.\n  In this work, we investigate the task of generated text detection by\nproposing the GigaCheck. Our research explores two approaches: (i)\ndistinguishing human-written texts from LLM-generated ones, and (ii) detecting\nLLM-generated intervals in Human-Machine collaborative texts. For the first\ntask, our approach utilizes a general-purpose LLM, leveraging its extensive\nlanguage abilities to fine-tune efficiently for the downstream task of\nLLM-generated text detection, achieving high performance even with limited\ndata. For the second task, we propose a novel approach that combines computer\nvision and natural language processing techniques. Specifically, we use a\nfine-tuned general-purpose LLM in conjunction with a DETR-like detection model,\nadapted from computer vision, to localize AI-generated intervals within text.\n  We evaluate the GigaCheck on five classification datasets with English texts\nand three datasets designed for Human-Machine collaborative text analysis. Our\nresults demonstrate that GigaCheck outperforms previous methods, even in\nout-of-distribution settings, establishing a strong baseline across all\ndatasets.",
    "keywords": "GigaCheck, LLM-generated content, text detection, Human-Machine collaborative texts, DETR-like detection model"
  },
  {
    "Date": "2024.12",
    "Title": "CATER: Leveraging LLM to Pioneer a Multidimensional, Reference-Independent Paradigm in Translation Quality Evaluation",
    "link": "https://arxiv.org/abs/2412.11261",
    "abstract": "This paper introduces the Comprehensive AI-assisted Translation Edit Ratio\n(CATER), a novel and fully prompt-driven framework for evaluating machine\ntranslation (MT) quality. Leveraging large language models (LLMs) via a\ncarefully designed prompt-based protocol, CATER expands beyond traditional\nreference-bound metrics, offering a multidimensional, reference-independent\nevaluation that addresses linguistic accuracy, semantic fidelity, contextual\ncoherence, stylistic appropriateness, and information completeness. CATER's\nunique advantage lies in its immediate implementability: by providing the\nsource and target texts along with a standardized prompt, an LLM can rapidly\nidentify errors, quantify edit effort, and produce category-level and overall\nscores. This approach eliminates the need for pre-computed references or\ndomain-specific resources, enabling instant adaptation to diverse languages,\ngenres, and user priorities through adjustable weights and prompt\nmodifications. CATER's LLM-enabled strategy supports more nuanced assessments,\ncapturing phenomena such as subtle omissions, hallucinations, and\ndiscourse-level shifts that increasingly challenge contemporary MT systems. By\nuniting the conceptual rigor of frameworks like MQM and DQF with the\nscalability and flexibility of LLM-based evaluation, CATER emerges as a\nvaluable tool for researchers, developers, and professional translators\nworldwide. The framework and example prompts are openly available, encouraging\ncommunity-driven refinement and further empirical validation.",
    "keywords": "CATER, machine translation, quality evaluation, large language models, reference-independent metrics"
  },
  {
    "Date": "2025.01",
    "Title": "Fine-Grained Appropriate Reliance: Human-AI Collaboration with a Multi-Step Transparent Decision Workflow for Complex Task Decomposition",
    "link": "https://arxiv.org/abs/2501.10909",
    "abstract": "In recent years, the rapid development of AI systems has brought about the\nbenefits of intelligent services but also concerns about security and\nreliability. By fostering appropriate user reliance on an AI system, both\ncomplementary team performance and reduced human workload can be achieved.\nPrevious empirical studies have extensively analyzed the impact of factors\nranging from task, system, and human behavior on user trust and appropriate\nreliance in the context of one-step decision making. However, user reliance on\nAI systems in tasks with complex semantics that require multi-step workflows\nremains under-explored. Inspired by recent work on task decomposition with\nlarge language models, we propose to investigate the impact of a novel\nMulti-Step Transparent (MST) decision workflow on user reliance behaviors. We\nconducted an empirical study (N = 233) of AI-assisted decision making in\ncomposite fact-checking tasks (i.e., fact-checking tasks that entail multiple\nsub-fact verification steps). Our findings demonstrate that human-AI\ncollaboration with an MST decision workflow can outperform one-step\ncollaboration in specific contexts (e.g., when advice from an AI system is\nmisleading). Further analysis of the appropriate reliance at fine-grained\nlevels indicates that an MST decision workflow can be effective when users\ndemonstrate a relatively high consideration of the intermediate steps. Our work\nhighlights that there is no one-size-fits-all decision workflow that can help\nobtain optimal human-AI collaboration. Our insights help deepen the\nunderstanding of the role of decision workflows in facilitating appropriate\nreliance. We synthesize important implications for designing effective means to\nfacilitate appropriate reliance on AI systems in composite tasks, positioning\nopportunities for the human-centered AI and broader HCI communities.",
    "keywords": "Human-AI Collaboration, Multi-Step Transparent Decision Workflow, Appropriate Reliance, Complex Task Decomposition, Composite Fact-Checking Tasks."
  },
  {
    "Date": "2025.01",
    "Title": "Self-Explanation in Social AI Agents",
    "link": "https://arxiv.org/abs/2501.13945",
    "abstract": "Social AI agents interact with members of a community, thereby changing the\nbehavior of the community. For example, in online learning, an AI social\nassistant may connect learners and thereby enhance social interaction. These\nsocial AI assistants too need to explain themselves in order to enhance\ntransparency and trust with the learners. We present a method of\nself-explanation that uses introspection over a self-model of an AI social\nassistant. The self-model is captured as a functional model that specifies how\nthe methods of the agent use knowledge to achieve its tasks. The process of\ngenerating self-explanations uses Chain of Thought to reflect on the self-model\nand ChatGPT to provide explanations about its functioning. We evaluate the\nself-explanation of the AI social assistant for completeness and correctness.\nWe also report on its deployment in a live class.",
    "keywords": "Social AI, Self-Explanation, Transparency, Trust, Chain of Thought, ChatGPT, Self-Model, Online Learning, AI Social Assistant, Community Behavior."
  },
  {
    "Date": "2025.02",
    "Title": "Bridging Logic Programming and Deep Learning for Explainability through ILASP",
    "link": "https://arxiv.org/abs/2502.09227",
    "abstract": "My research explores integrating deep learning and logic programming to set\nthe basis for a new generation of AI systems. By combining neural networks with\nInductive Logic Programming (ILP), the goal is to construct systems that make\naccurate predictions and generate comprehensible rules to validate these\npredictions. Deep learning models process and analyze complex data, while ILP\ntechniques derive logical rules to prove the network's conclusions. Explainable\nAI methods, like eXplainable Answer Set Programming (XASP), elucidate the\nreasoning behind these rules and decisions. The focus is on applying ILP\nframeworks, specifically ILASP and FastLAS, to enhance explainability in\nvarious domains. My test cases span weather prediction, the legal field, and\nimage recognition. In weather forecasting, the system will predict events and\nprovides explanations using FastLAS, with plans to integrate recurrent neural\nnetworks in the future. In the legal domain, the research focuses on\ninterpreting vague decisions and assisting legal professionals by encoding\nItalian legal articles and learning reasoning patterns from Court of Cassation\ndecisions using ILASP. For biological laboratories, we will collaborate with a\nresearch group to automate spermatozoa morphology classification for Bull\nBreeding Soundness Evaluation using YOLO networks and ILP to explain\nclassification outcomes. This hybrid approach aims to bridge the gap between\nthe high performance of deep learning models and the transparency of symbolic\nreasoning, advancing AI by providing interpretable and trustworthy\napplications.",
    "keywords": "Deep Learning, Logic Programming, Explainable AI, ILASP, XASP"
  },
  {
    "Date": "2025.02",
    "Title": "EAIRA: Establishing a Methodology for Evaluating AI Models as Scientific Research Assistants",
    "link": "https://arxiv.org/abs/2502.20309",
    "abstract": "Recent advancements have positioned AI, and particularly Large Language\nModels (LLMs), as transformative tools for scientific research, capable of\naddressing complex tasks that require reasoning, problem-solving, and\ndecision-making. Their exceptional capabilities suggest their potential as\nscientific research assistants but also highlight the need for holistic,\nrigorous, and domain-specific evaluation to assess effectiveness in real-world\nscientific applications. This paper describes a multifaceted methodology for\nEvaluating AI models as scientific Research Assistants (EAIRA) developed at\nArgonne National Laboratory. This methodology incorporates four primary classes\nof evaluations. 1) Multiple Choice Questions to assess factual recall; 2) Open\nResponse to evaluate advanced reasoning and problem-solving skills; 3)\nLab-Style Experiments involving detailed analysis of capabilities as research\nassistants in controlled environments; and 4) Field-Style Experiments to\ncapture researcher-LLM interactions at scale in a wide range of scientific\ndomains and applications. These complementary methods enable a comprehensive\nanalysis of LLM strengths and weaknesses with respect to their scientific\nknowledge, reasoning abilities, and adaptability. Recognizing the rapid pace of\nLLM advancements, we designed the methodology to evolve and adapt so as to\nensure its continued relevance and applicability. This paper describes the\nmethodology state at the end of February 2025. Although developed within a\nsubset of scientific domains, the methodology is designed to be generalizable\nto a wide range of scientific domains.",
    "keywords": "AI, Large Language Models, Scientific Research, Evaluation Methodology, EAIRA"
  },
  {
    "Date": "2025.03",
    "Title": "The impact of AI and peer feedback on research writing skills: a study using the CGScholar platform among Kazakhstani scholars",
    "link": "https://arxiv.org/abs/2503.05820",
    "abstract": "This research studies the impact of AI and peer feedback on the academic\nwriting development of Kazakhstani scholars using the CGScholar platform - a\nproduct of research into collaborative learning, big data, and artificial\nintelligence developed by educators and computer scientists at the University\nof Illinois at Urbana-Champaign (UIUC). The study aimed to find out how\nfamiliarity with AI tools and peer feedback processes impacts participants'\nopenness to incorporating feedback into their academic writing. The study\ninvolved 36 scholars enrolled in a scientific internship focused on education\nat UIUC. A survey with 15 multiple-choice questions, a Likert scale, and\nopen-ended questions was used to collect data. The survey was conducted via\nGoogle Forms in both English and Russian to ensure linguistic accessibility.\nDemographic information such as age, gender, and first language was collected\nto provide a detailed understanding of the data. The analysis revealed a\nmoderate positive correlation between familiarity with AI tools and openness to\nmaking changes based on feedback, and a strong positive correlation between\nresearch writing experience and expectations of peer feedback, especially in\nthe area of research methodology. These results show that participants are\nopen-minded to AI-assisted feedback; however, they still highly appreciate peer\ninput, especially regarding methodological guidance. This study demonstrates\nthe potential benefits of integrating AI tools with traditional feedback\nmechanisms to improve research writing quality in academic settings.",
    "keywords": "AI, peer feedback, research writing skills, CGScholar platform, Kazakhstani scholars"
  },
  {
    "Date": "2025.03",
    "Title": "Enabling AI Scientists to Recognize Innovation: A Domain-Agnostic Algorithm for Assessing Novelty",
    "link": "https://arxiv.org/abs/2503.01508",
    "abstract": "In the pursuit of Artificial General Intelligence (AGI), automating the\ngeneration and evaluation of novel research ideas is a key challenge in\nAI-driven scientific discovery. This paper presents Relative Neighbor Density\n(RND), a domain-agnostic algorithm for novelty assessment in research ideas\nthat overcomes the limitations of existing approaches by comparing an idea's\nlocal density with its adjacent neighbors' densities. We first developed a\nscalable methodology to create test set without expert labeling, addressing a\nfundamental challenge in novelty assessment. Using these test sets, we\ndemonstrate that our RND algorithm achieves state-of-the-art (SOTA) performance\nin computer science (AUROC=0.820) and biomedical research (AUROC=0.765)\ndomains. Most significantly, while SOTA models like Sonnet-3.7 and existing\nmetrics show domain-specific performance degradation, RND maintains consistent\naccuracies across domains by its domain-invariant property, outperforming all\nbenchmarks by a substantial margin (0.795 v.s. 0.597) on cross-domain\nevaluation. These results validate RND as a generalizable solution for\nautomated novelty assessment in scientific research.",
    "keywords": "Artificial General Intelligence (AGI), novelty assessment, Relative Neighbor Density (RND), domain-agnostic algorithm, scientific discovery."
  },
  {
    "Date": "2025.02",
    "Title": "Supporting the development of Machine Learning for fundamental science in a federated Cloud with the AI_INFN platform",
    "link": "https://arxiv.org/abs/2502.21266",
    "abstract": "Machine Learning (ML) is driving a revolution in the way scientists design,\ndevelop, and deploy data-intensive software. However, the adoption of ML\npresents new challenges for the computing infrastructure, particularly in terms\nof provisioning and orchestrating access to hardware accelerators for\ndevelopment, testing, and production. The INFN-funded project AI_INFN\n(\"Artificial Intelligence at INFN\") aims at fostering the adoption of ML\ntechniques within INFN use cases by providing support on multiple aspects,\nincluding the provision of AI-tailored computing resources. It leverages\ncloud-native solutions in the context of INFN Cloud, to share hardware\naccelerators as effectively as possible, ensuring the diversity of the\nInstitute's research activities is not compromised. In this contribution, we\nprovide an update on the commissioning of a Kubernetes platform designed to\nease the development of GPU-powered data analysis workflows and their\nscalability on heterogeneous, distributed computing resources, possibly\nfederated as Virtual Kubelets with the interLink provider.",
    "keywords": "Machine Learning, INFN Cloud, Kubernetes, GPU-powered data analysis, Virtual Kubelets."
  }
]