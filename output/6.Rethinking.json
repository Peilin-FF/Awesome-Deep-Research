[
  {
    "Date": "2021.01",
    "Title": "How Much Automation Does a Data Scientist Want?",
    "link": "https://arxiv.org/abs/2101.03970",
    "abstract": "Data science and machine learning (DS/ML) are at the heart of the recent\nadvancements of many Artificial Intelligence (AI) applications. There is an\nactive research thread in AI, \\autoai, that aims to develop systems for\nautomating end-to-end the DS/ML Lifecycle. However, do DS and ML workers really\nwant to automate their DS/ML workflow? To answer this question, we first\nsynthesize a human-centered AutoML framework with 6 User Role/Personas, 10\nStages and 43 Sub-Tasks, 5 Levels of Automation, and 5 Types of Explanation,\nthrough reviewing research literature and marketing reports. Secondly, we use\nthe framework to guide the design of an online survey study with 217 DS/ML\nworkers who had varying degrees of experience, and different user roles\n\"matching\" to our 6 roles/personas. We found that different user personas\nparticipated in distinct stages of the lifecycle -- but not all stages. Their\ndesired levels of automation and types of explanation for AutoML also varied\nsignificantly depending on the DS/ML stage and the user persona. Based on the\nsurvey results, we argue there is no rationale from user needs for complete\nautomation of the end-to-end DS/ML lifecycle. We propose new next steps for\nuser-controlled DS/ML automation.",
    "keywords": "Data Science, Machine Learning, Automation, User-Centered, AutoML Framework"
  },
  {
    "Date": "2021.02",
    "Title": "Documentation Matters: Human-Centered AI System to Assist Data Science Code Documentation in Computational Notebooks",
    "link": "https://arxiv.org/abs/2102.12592",
    "abstract": "Computational notebooks allow data scientists to express their ideas through\na combination of code and documentation. However, data scientists often pay\nattention only to the code, and neglect creating or updating their\ndocumentation during quick iterations. Inspired by human documentation\npractices learned from 80 highly-voted Kaggle notebooks, we design and\nimplement Themisto, an automated documentation generation system to explore how\nhuman-centered AI systems can support human data scientists in the machine\nlearning code documentation scenario. Themisto facilitates the creation of\ndocumentation via three approaches: a deep-learning-based approach to generate\ndocumentation for source code, a query-based approach to retrieve online API\ndocumentation for source code, and a user prompt approach to nudge users to\nwrite documentation. We evaluated Themisto in a within-subjects experiment with\n24 data science practitioners, and found that automated documentation\ngeneration techniques reduced the time for writing documentation, reminded\nparticipants to document code they would have ignored, and improved\nparticipants' satisfaction with their computational notebook.",
    "keywords": "Computational notebooks, Data science, Documentation generation, Human-centered AI, Machine learning code documentation."
  },
  {
    "Date": "2021.11",
    "Title": "Automated scholarly paper review: Concepts, technologies, and challenges",
    "link": "https://arxiv.org/abs/2111.07533",
    "abstract": "Peer review is a widely accepted mechanism for research evaluation, playing a\npivotal role in academic publishing. However, criticisms have long been leveled\nat this mechanism, mostly because of its poor efficiency and low\nreproducibility. Recent years have seen the application of artificial\nintelligence (AI) in assisting the peer review process. Nonetheless, with the\ninvolvement of humans, such limitations remain inevitable. In this paper, we\npropose the concept and pipeline of automated scholarly paper review (ASPR) and\nreview the relevant literature and technologies of achieving a full-scale\ncomputerized review process. On the basis of the review and discussion, we\nconclude that there is already corresponding research and preliminary\nimplementation at each stage of ASPR. We further look into the challenges in\nASPR with the existing technologies. The major difficulties lie in inadequate\ndata, imperfect document parsing and representation, defective\nhuman$\\unicode{x2013}$computer interaction, and flawed deep logical reasoning.\nMoreover, we point out the future directions and discuss the possible moral and\nethical issues of ASPR. In the foreseeable future, ASPR and peer review will\ncoexist in a reinforcing manner before ASPR is able to fully undertake the\nreviewing workload from humans.",
    "keywords": "Automated scholarly paper review, Artificial intelligence, Peer review, Research evaluation, Academic publishing"
  },
  {
    "Date": "2021.03",
    "Title": "Toward Building Science Discovery Machines",
    "link": "https://arxiv.org/abs/2103.15551",
    "abstract": "The dream of building machines that can do science has inspired scientists\nfor decades. Remarkable advances have been made recently; however, we are still\nfar from achieving this goal. In this paper, we focus on the scientific\ndiscovery process where a high level of reasoning and remarkable\nproblem-solving ability are required. We review different machine learning\ntechniques used in scientific discovery with their limitations. We survey and\ndiscuss the main principles driving the scientific discovery process. These\nprinciples are used in different fields and by different scientists to solve\nproblems and discover new knowledge. We provide many examples of the use of\nthese principles in different fields such as physics, mathematics, and biology.\nWe also review AI systems that attempt to implement some of these principles.\nWe argue that building science discovery machines should be guided by these\nprinciples as an alternative to the dominant approach of current AI systems\nthat focuses on narrow objectives. Building machines that fully incorporate\nthese principles in an automated way might open the doors for many\nadvancements.",
    "keywords": "Scientific Discovery, Machine Learning, AI Systems, Problem-Solving, Principles of Science Discovery."
  },
  {
    "Date": "2023.01",
    "Title": "How Data Scientists Review the Scholarly Literature",
    "link": "https://arxiv.org/abs/2301.03774",
    "abstract": "Keeping up with the research literature plays an important role in the\nworkflow of scientists - allowing them to understand a field, formulate the\nproblems they focus on, and develop the solutions that they contribute, which\nin turn shape the nature of the discipline. In this paper, we examine the\nliterature review practices of data scientists. Data science represents a field\nseeing an exponential rise in papers, and increasingly drawing on and being\napplied in numerous diverse disciplines. Recent efforts have seen the\ndevelopment of several tools intended to help data scientists cope with a\ndeluge of research and coordinated efforts to develop AI tools intended to\nuncover the research frontier. Despite these trends indicative of the\ninformation overload faced by data scientists, no prior work has examined the\nspecific practices and challenges faced by these scientists in an\ninterdisciplinary field with evolving scholarly norms. In this paper, we close\nthis gap through a set of semi-structured interviews and think-aloud protocols\nof industry and academic data scientists (N = 20). Our results while\ncorroborating other knowledge workers' practices uncover several novel\nfindings: individuals (1) are challenged in seeking and sensemaking of papers\nbeyond their disciplinary bubbles, (2) struggle to understand papers in the\nface of missing details and mathematical content, (3) grapple with the deluge\nby leveraging the knowledge context in code, blogs, and talks, and (4) lean on\ntheir peers online and in-person. Furthermore, we outline future directions\nlikely to help data scientists cope with the burgeoning research literature.",
    "keywords": "Data Scientists, Scholarly Literature, Literature Review Practices, Information Overload, Interdisciplinary Field"
  },
  {
    "Date": "2023.03",
    "Title": "Practices and Challenges of Using GitHub Copilot: An Empirical Study",
    "link": "https://arxiv.org/abs/2303.08733",
    "abstract": "With the advances in machine learning, there is a growing interest in\nAI-enabled tools for autocompleting source code. GitHub Copilot, also referred\nto as the \"AI Pair Programmer\", has been trained on billions of lines of open\nsource GitHub code, and is one of such tools that has been increasingly used\nsince its launch on June 2021. However, little effort has been devoted to\nunderstanding the practices and challenges of using Copilot in programming with\nauto-completed source code. To this end, we conducted an empirical study by\ncollecting and analyzing the data from Stack Overflow (SO) and GitHub\nDiscussions. More specifically, we searched and manually collected 169 SO posts\nand 655 GitHub discussions related to the usage of Copilot. We identified the\nprogramming languages, IDEs, technologies used with Copilot, functions\nimplemented, benefits, limitations, and challenges when using Copilot. The\nresults show that when practitioners use Copilot: (1) The major programming\nlanguages used with Copilot are JavaScript and Python, (2) the main IDE used\nwith Copilot is Visual Studio Code, (3) the most common used technology with\nCopilot is Node.js, (4) the leading function implemented by Copilot is data\nprocessing, (5) the significant benefit of using Copilot is useful code\ngeneration, and (6) the main limitation encountered by practitioners when using\nCopilot is difficulty of integration. Our results suggest that using Copilot is\nlike a double-edged sword, which requires developers to carefully consider\nvarious aspects when deciding whether or not to use it. Our study provides\nempirically grounded foundations and basis for future research on the role of\nCopilot as an AI pair programmer in software development.",
    "keywords": "GitHub Copilot, AI Pair Programmer, programming languages, IDEs, challenges"
  },
  {
    "Date": "2023.03",
    "Title": "A Large-Scale Survey on the Usability of AI Programming Assistants: Successes and Challenges",
    "link": "https://arxiv.org/abs/2303.17125",
    "abstract": "The software engineering community recently has witnessed widespread\ndeployment of AI programming assistants, such as GitHub Copilot. However, in\npractice, developers do not accept AI programming assistants' initial\nsuggestions at a high frequency. This leaves a number of open questions related\nto the usability of these tools. To understand developers' practices while\nusing these tools and the important usability challenges they face, we\nadministered a survey to a large population of developers and received\nresponses from a diverse set of 410 developers. Through a mix of qualitative\nand quantitative analyses, we found that developers are most motivated to use\nAI programming assistants because they help developers reduce key-strokes,\nfinish programming tasks quickly, and recall syntax, but resonate less with\nusing them to help brainstorm potential solutions. We also found the most\nimportant reasons why developers do not use these tools are because these tools\ndo not output code that addresses certain functional or non-functional\nrequirements and because developers have trouble controlling the tool to\ngenerate the desired output. Our findings have implications for both creators\nand users of AI programming assistants, such as designing minimal cognitive\neffort interactions with these tools to reduce distractions for users while\nthey are programming.",
    "keywords": "AI Programming Assistants, Usability, Developers, Software Engineering, Cognitive Effort."
  },
  {
    "Date": "2023.05",
    "Title": "Automated Scientific Discovery: From Equation Discovery to Autonomous Discovery Systems",
    "link": "https://arxiv.org/abs/2305.02251",
    "abstract": "The paper surveys automated scientific discovery, from equation discovery and\nsymbolic regression to autonomous discovery systems and agents. It discusses\nthe individual approaches from a \"big picture\" perspective and in context, but\nalso discusses open issues and recent topics like the various roles of deep\nneural networks in this area, aiding in the discovery of human-interpretable\nknowledge. Further, we will present closed-loop scientific discovery systems,\nstarting with the pioneering work on the Adam system up to current efforts in\nfields from material science to astronomy. Finally, we will elaborate on\nautonomy from a machine learning perspective, but also in analogy to the\nautonomy levels in autonomous driving. The maximal level, level five, is\ndefined to require no human intervention at all in the production of scientific\nknowledge. Achieving this is one step towards solving the Nobel Turing Grand\nChallenge to develop AI Scientists: AI systems capable of making Nobel-quality\nscientific discoveries highly autonomously at a level comparable, and possibly\nsuperior, to the best human scientists by 2050.",
    "keywords": "Automated Scientific Discovery, Equation Discovery, Autonomous Discovery Systems, Deep Neural Networks, Nobel Turing Grand Challenge."
  },
  {
    "Date": "2023.05",
    "Title": "Science in the Era of ChatGPT, Large Language Models and Generative AI: Challenges for Research Ethics and How to Respond",
    "link": "https://arxiv.org/abs/2305.15299",
    "abstract": "Large language models of artificial intelligence (AI), such as ChatGPT, find\nremarkable but controversial applicability in science and research. This paper\nreviews epistemological challenges, ethical and integrity risks in science\nconduct in the advent of generative AI. This is with the aim to lay new timely\nfoundations for a high-quality research ethics review. The role of AI language\nmodels as a research instrument and subject is scrutinized along with ethical\nimplications for scientists, participants and reviewers. New emerging practices\nfor research ethics review are discussed, concluding with ten recommendations\nthat shape a response for a more responsible research conduct in the era of AI.",
    "keywords": "Artificial Intelligence, Research Ethics, ChatGPT, Generative AI, Scientific Integrity"
  },
  {
    "Date": "2023.06",
    "Title": "The Future of AI-Assisted Writing",
    "link": "https://arxiv.org/abs/2306.16641",
    "abstract": "The development of Natural Language Generation models has led to the creation\nof powerful Artificial Intelligence-assisted writing tools. These tools are\ncapable of predicting users' needs and actively providing suggestions as they\nwrite. In this work, we conduct a comparative user-study between such tools\nfrom an information retrieval lens: pull and push. Specifically, we investigate\nthe user demand of AI-assisted writing, the impact of the two paradigms on\nquality, ownership of the writing product, and efficiency and enjoyment of the\nwriting process. We also seek to understand the impact of bias of AI-assisted\nwriting. Our findings show that users welcome seamless assistance of AI in\ntheir writing. Furthermore, AI helped users to diversify the ideas in their\nwriting while keeping it clear and concise more quickly. Users also enjoyed the\ncollaboration with AI-assisted writing tools and did not feel a lack of\nownership. Finally, although participants did not experience bias in our\nexperiments, they still expressed explicit and clear concerns that should be\naddressed in future AI-assisted writing tools.",
    "keywords": "Artificial Intelligence, Natural Language Generation, AI-Assisted Writing, Information Retrieval, User Study"
  },
  {
    "Date": "2023.07",
    "Title": "What Should Data Science Education Do with Large Language Models?",
    "link": "https://arxiv.org/abs/2307.02792",
    "abstract": "The rapid advances of large language models (LLMs), such as ChatGPT, are\nrevolutionizing data science and statistics. These state-of-the-art tools can\nstreamline complex processes. As a result, it reshapes the role of data\nscientists. We argue that LLMs are transforming the responsibilities of data\nscientists, shifting their focus from hands-on coding, data-wrangling and\nconducting standard analyses to assessing and managing analyses performed by\nthese automated AIs. This evolution of roles is reminiscent of the transition\nfrom a software engineer to a product manager. We illustrate this transition\nwith concrete data science case studies using LLMs in this paper. These\ndevelopments necessitate a meaningful evolution in data science education.\nPedagogy must now place greater emphasis on cultivating diverse skillsets among\nstudents, such as LLM-informed creativity, critical thinking, AI-guided\nprogramming. LLMs can also play a significant role in the classroom as\ninteractive teaching and learning tools, contributing to personalized\neducation. This paper discusses the opportunities, resources and open\nchallenges for each of these directions. As with any transformative technology,\nintegrating LLMs into education calls for careful consideration. While LLMs can\nperform repetitive tasks efficiently, it's crucial to remember that their role\nis to supplement human intelligence and creativity, not to replace it.\nTherefore, the new era of data science education should balance the benefits of\nLLMs while fostering complementary human expertise and innovations. In\nconclusion, the rise of LLMs heralds a transformative period for data science\nand its education. This paper seeks to shed light on the emerging trends,\npotential opportunities, and challenges accompanying this paradigm shift,\nhoping to spark further discourse and investigation into this exciting,\nuncharted territory.",
    "keywords": "Large Language Models, Data Science Education, ChatGPT, AI-guided Programming, Pedagogy Evolution"
  },
  {
    "Date": "2023.07",
    "Title": "The Future of Fundamental Science Led by Generative Closed-Loop Artificial Intelligence",
    "link": "https://arxiv.org/abs/2307.07522",
    "abstract": "Recent advances in machine learning and AI, including Generative AI and LLMs,\nare disrupting technological innovation, product development, and society as a\nwhole. AI's contribution to technology can come from multiple approaches that\nrequire access to large training data sets and clear performance evaluation\ncriteria, ranging from pattern recognition and classification to generative\nmodels. Yet, AI has contributed less to fundamental science in part because\nlarge data sets of high-quality data for scientific practice and model\ndiscovery are more difficult to access. Generative AI, in general, and Large\nLanguage Models in particular, may represent an opportunity to augment and\naccelerate the scientific discovery of fundamental deep science with\nquantitative models. Here we explore and investigate aspects of an AI-driven,\nautomated, closed-loop approach to scientific discovery, including self-driven\nhypothesis generation and open-ended autonomous exploration of the hypothesis\nspace. Integrating AI-driven automation into the practice of science would\nmitigate current problems, including the replication of findings, systematic\nproduction of data, and ultimately democratisation of the scientific process.\nRealising these possibilities requires a vision for augmented AI coupled with a\ndiversity of AI approaches able to deal with fundamental aspects of causality\nanalysis and model discovery while enabling unbiased search across the space of\nputative explanations. These advances hold the promise to unleash AI's\npotential for searching and discovering the fundamental structure of our world\nbeyond what human scientists have been able to achieve. Such a vision would\npush the boundaries of new fundamental science rather than automatize current\nworkflows and instead open doors for technological innovation to tackle some of\nthe greatest challenges facing humanity today.",
    "keywords": "Artificial Intelligence, Generative AI, Large Language Models, Scientific Discovery, Fundamental Science"
  },
  {
    "Date": "2023.07",
    "Title": "AI empowering research: 10 ways how science can benefit from AI",
    "link": "https://arxiv.org/abs/2307.10265",
    "abstract": "This article explores the transformative impact of artificial intelligence\n(AI) on scientific research. It highlights ten ways in which AI is\nrevolutionizing the work of scientists, including powerful referencing tools,\nimproved understanding of research problems, enhanced research question\ngeneration, optimized research design, stub data generation, data\ntransformation, advanced data analysis, and AI-assisted reporting. While AI\noffers numerous benefits, challenges such as bias, privacy concerns, and the\nneed for human-AI collaboration must be considered. The article emphasizes that\nAI can augment human creativity in science but not replace it.",
    "keywords": "Artificial Intelligence, Scientific Research, Data Analysis, Human-AI Collaboration, Creativity in Science"
  },
  {
    "Date": "2023.10",
    "Title": "AI for Mathematics: A Cognitive Science Perspective",
    "link": "https://arxiv.org/abs/2310.13021",
    "abstract": "Mathematics is one of the most powerful conceptual systems developed and used\nby the human species. Dreams of automated mathematicians have a storied history\nin artificial intelligence (AI). Rapid progress in AI, particularly propelled\nby advances in large language models (LLMs), has sparked renewed, widespread\ninterest in building such systems. In this work, we reflect on these goals from\na \\textit{cognitive science} perspective. We call attention to several\nclassical and ongoing research directions from cognitive science, which we\nbelieve are valuable for AI practitioners to consider when seeking to build\ntruly human (or superhuman)-level mathematical systems. We close with open\ndiscussions and questions that we believe necessitate a multi-disciplinary\nperspective -- cognitive scientists working in tandem with AI researchers and\nmathematicians -- as we move toward better mathematical AI systems which not\nonly help us push the frontier of the mathematics, but also offer glimpses into\nhow we as humans are even capable of such great cognitive feats.",
    "keywords": "Artificial Intelligence, Mathematics, Cognitive Science, Large Language Models, Mathematical Systems"
  },
  {
    "Date": "2023.10",
    "Title": "Conversational Challenges in AI-Powered Data Science: Obstacles, Needs, and Design Opportunities",
    "link": "https://arxiv.org/abs/2310.16164",
    "abstract": "Large Language Models (LLMs) are being increasingly employed in data science\nfor tasks like data preprocessing and analytics. However, data scientists\nencounter substantial obstacles when conversing with LLM-powered chatbots and\nacting on their suggestions and answers. We conducted a mixed-methods study,\nincluding contextual observations, semi-structured interviews (n=14), and a\nsurvey (n=114), to identify these challenges. Our findings highlight key issues\nfaced by data scientists, including contextual data retrieval, formulating\nprompts for complex tasks, adapting generated code to local environments, and\nrefining prompts iteratively. Based on these insights, we propose actionable\ndesign recommendations, such as data brushing to support context selection, and\ninquisitive feedback loops to improve communications with AI-based assistants\nin data-science tools.",
    "keywords": "Large Language Models, data science, conversational challenges, design recommendations, AI-powered chatbots."
  },
  {
    "Date": "2023.11",
    "Title": "Unraveling Downstream Gender Bias from Large Language Models: A Study on AI Educational Writing Assistance",
    "link": "https://arxiv.org/abs/2311.03311",
    "abstract": "Large Language Models (LLMs) are increasingly utilized in educational tasks\nsuch as providing writing suggestions to students. Despite their potential,\nLLMs are known to harbor inherent biases which may negatively impact learners.\nPrevious studies have investigated bias in models and data representations\nseparately, neglecting the potential impact of LLM bias on human writing. In\nthis paper, we investigate how bias transfers through an AI writing support\npipeline. We conduct a large-scale user study with 231 students writing\nbusiness case peer reviews in German. Students are divided into five groups\nwith different levels of writing support: one classroom group with\nfeature-based suggestions and four groups recruited from Prolific -- a control\ngroup with no assistance, two groups with suggestions from fine-tuned GPT-2 and\nGPT-3 models, and one group with suggestions from pre-trained GPT-3.5. Using\nGenBit gender bias analysis, Word Embedding Association Tests (WEAT), and\nSentence Embedding Association Test (SEAT) we evaluate the gender bias at\nvarious stages of the pipeline: in model embeddings, in suggestions generated\nby the models, and in reviews written by students. Our results demonstrate that\nthere is no significant difference in gender bias between the resulting peer\nreviews of groups with and without LLM suggestions. Our research is therefore\noptimistic about the use of AI writing support in the classroom, showcasing a\ncontext where bias in LLMs does not transfer to students' responses.",
    "keywords": "Large Language Models, Gender Bias, AI Writing Support, Educational Writing, Bias Transfer"
  },
  {
    "Date": "2023.12",
    "Title": "Drivers and Barriers of AI Adoption and Use in Scientific Research",
    "link": "https://arxiv.org/abs/2312.09843",
    "abstract": "New technologies have the power to revolutionize science. It has happened in\nthe past and is happening again with the emergence of new computational tools,\nsuch as artificial intelligence and machine learning. Despite the documented\nimpact of these technologies, there remains a significant gap in understanding\nthe process of their adoption within the scientific community. In this paper,\nwe draw on theories of scientific and technical human capital to study the\nintegration of AI in scientific research, focusing on the human capital of\nscientists and the external resources available within their network of\ncollaborators and institutions. We validate our hypotheses on a large sample of\npublications from OpenAlex, covering all sciences from 1980 to 2020, and\nidentify a set key drivers and inhibitors of AI adoption and use in science.\nOur results suggest that AI is pioneered by domain scientists with a `taste for\nexploration' and who are embedded in a network rich of computer scientists,\nexperienced AI scientists and early-career researchers; they come from\ninstitutions with high citation impact and a relatively strong publication\nhistory on AI. The access to computing resources only matters for a few\nscientific disciplines, such as chemistry and medical sciences. Once AI is\nintegrated into research, most adoption factors continue to influence its\nsubsequent reuse. Implications for the organization and management of science\nin the evolving era of AI-driven discovery are discussed.",
    "keywords": "Artificial Intelligence, Adoption, Scientific Research, Human Capital, Barriers and Drivers"
  },
  {
    "Date": "2023.12",
    "Title": "Generative AI in Writing Research Papers: A New Type of Algorithmic Bias and Uncertainty in Scholarly Work",
    "link": "https://arxiv.org/abs/2312.10057",
    "abstract": "The use of artificial intelligence (AI) in research across all disciplines is\nbecoming ubiquitous. However, this ubiquity is largely driven by hyperspecific\nAI models developed during scientific studies for accomplishing a well-defined,\ndata-dense task. These AI models introduce apparent, human-recognizable biases\nbecause they are trained with finite, specific data sets and parameters.\nHowever, the efficacy of using large language models (LLMs) -- and LLM-powered\ngenerative AI tools, such as ChatGPT -- to assist the research process is\ncurrently indeterminate. These generative AI tools, trained on general and\nimperceptibly large datasets along with human feedback, present challenges in\nidentifying and addressing biases. Furthermore, these models are susceptible to\ngoal misgeneralization, hallucinations, and adversarial attacks such as red\nteaming prompts -- which can be unintentionally performed by human researchers,\nresulting in harmful outputs. These outputs are reinforced in research -- where\nan increasing number of individuals have begun to use generative AI to compose\nmanuscripts. Efforts into AI interpretability lag behind development, and the\nimplicit variations that occur when prompting and providing context to a\nchatbot introduce uncertainty and irreproducibility. We thereby find that\nincorporating generative AI in the process of writing research manuscripts\nintroduces a new type of context-induced algorithmic bias and has unintended\nside effects that are largely detrimental to academia, knowledge production,\nand communicating research.",
    "keywords": "Generative AI, Algorithmic Bias, Uncertainty, Scholarly Work, Research Manuscripts"
  },
  {
    "Date": "2023.12",
    "Title": "Exploring the intersection of Generative AI and Software Development",
    "link": "https://arxiv.org/abs/2312.14262",
    "abstract": "In the ever-evolving landscape of Artificial Intelligence (AI), the synergy\nbetween generative AI and Software Engineering emerges as a transformative\nfrontier. This whitepaper delves into the unexplored realm, elucidating how\ngenerative AI techniques can revolutionize software development. Spanning from\nproject management to support and updates, we meticulously map the demands of\neach development stage and unveil the potential of generative AI in addressing\nthem. Techniques such as zero-shot prompting, self-consistency, and multimodal\nchain-of-thought are explored, showcasing their unique capabilities in\nenhancing generative AI models. The significance of vector embeddings, context,\nplugins, tools, and code assistants is underscored, emphasizing their role in\ncapturing semantic information and amplifying generative AI capabilities.\nLooking ahead, this intersection promises to elevate productivity, improve code\nquality, and streamline the software development process. This whitepaper\nserves as a guide for stakeholders, urging discussions and experiments in the\napplication of generative AI in Software Engineering, fostering innovation and\ncollaboration for a qualitative leap in the efficiency and effectiveness of\nsoftware development.",
    "keywords": "Generative AI, Software Engineering, Zero-shot prompting, Multimodal chain-of-thought, Vector embeddings"
  },
  {
    "Date": "2024.01",
    "Title": "Can AI Assistants Know What They Don't Know?",
    "link": "https://arxiv.org/abs/2401.13275",
    "abstract": "Recently, AI assistants based on large language models (LLMs) show surprising\nperformance in many tasks, such as dialogue, solving math problems, writing\ncode, and using tools. Although LLMs possess intensive world knowledge, they\nstill make factual errors when facing some knowledge intensive tasks, like\nopen-domain question answering. These untruthful responses from the AI\nassistant may cause significant risks in practical applications. We believe\nthat an AI assistant's refusal to answer questions it does not know is a\ncrucial method for reducing hallucinations and making the assistant truthful.\nTherefore, in this paper, we ask the question \"Can AI assistants know what they\ndon't know and express them through natural language?\" To answer this question,\nwe construct a model-specific \"I don't know\" (Idk) dataset for an assistant,\nwhich contains its known and unknown questions, based on existing open-domain\nquestion answering datasets. Then we align the assistant with its corresponding\nIdk dataset and observe whether it can refuse to answer its unknown questions\nafter alignment. Experimental results show that after alignment with Idk\ndatasets, the assistant can refuse to answer most its unknown questions. For\nquestions they attempt to answer, the accuracy is significantly higher than\nbefore the alignment.",
    "keywords": "AI Assistants, Large Language Models (LLMs), Open-domain Question Answering, Factual Errors, \"I don't know\" (Idk) dataset."
  },
  {
    "Date": "2024.03",
    "Title": "\"It is there, and you need it, so why do you not use it?\" Achieving better adoption of AI systems by domain experts, in the case study of natural science research",
    "link": "https://arxiv.org/abs/2403.16895",
    "abstract": "Artificial Intelligence (AI) is becoming ubiquitous in domains such as\nmedicine and natural science research. However, when AI systems are implemented\nin practice, domain experts often refuse them. Low acceptance hinders effective\nhuman-AI collaboration, even when it is essential for progress. In natural\nscience research, scientists' ineffective use of AI-enabled systems can impede\nthem from analysing their data and advancing their research. We conducted an\nethnographically informed study of 10 in-depth interviews with AI practitioners\nand natural scientists at the organisation facing low adoption of algorithmic\nsystems. Results were consolidated into recommendations for better AI adoption:\ni) actively supporting experts during the initial stages of system use, ii)\ncommunicating the capabilities of a system in a user-relevant way, and iii)\nfollowing predefined collaboration rules. We discuss the broader implications\nof our findings and expand on how our proposed requirements could support\npractitioners and experts across domains.",
    "keywords": "Artificial Intelligence, adoption, domain experts, natural science research, human-AI collaboration"
  },
  {
    "Date": "2024.03",
    "Title": "Envisioning the Next-Generation AI Coding Assistants: Insights & Proposals",
    "link": "https://arxiv.org/abs/2403.14592",
    "abstract": "As a research-product hybrid group in AI for Software Engineering (AI4SE), we\npresent four key takeaways from our experience developing in-IDE AI coding\nassistants. AI coding assistants should set clear expectations for usage,\nintegrate with advanced IDE capabilities and existing extensions, use\nextendable backend designs, and collect app data responsibly for downstream\nanalyses. We propose open questions and challenges that academia and industry\nshould address to realize the vision of next-generation AI coding assistants.",
    "keywords": "AI coding assistants, in-IDE, AI for Software Engineering, backend designs, app data collection"
  },
  {
    "Date": "2024.04",
    "Title": "Deceptive Patterns of Intelligent and Interactive Writing Assistants",
    "link": "https://arxiv.org/abs/2404.09375",
    "abstract": "Large Language Models have become an integral part of new intelligent and\ninteractive writing assistants. Many are offered commercially with a\nchatbot-like UI, such as ChatGPT, and provide little information about their\ninner workings. This makes this new type of widespread system a potential\ntarget for deceptive design patterns. For example, such assistants might\nexploit hidden costs by providing guidance up until a certain point before\nasking for a fee to see the rest. As another example, they might sneak unwanted\ncontent/edits into longer generated or revised text pieces (e.g. to influence\nthe expressed opinion). With these and other examples, we conceptually transfer\nseveral deceptive patterns from the literature to the new context of AI writing\nassistants. Our goal is to raise awareness and encourage future research into\nhow the UI and interaction design of such systems can impact people and their\nwriting.",
    "keywords": "Large Language Models, Intelligent Writing Assistants, Deceptive Design Patterns, Chatbot-like UI, Interaction Design Impact."
  },
  {
    "Date": "2024.04",
    "Title": "How far are AI-powered programming assistants from meeting developers' needs?",
    "link": "https://arxiv.org/abs/2404.12000",
    "abstract": "Recent In-IDE AI coding assistant tools (ACATs) like GitHub Copilot have\nsignificantly impacted developers' coding habits. While some studies have\nexamined their effectiveness, there lacks in-depth investigation into the\nactual assistance process. To bridge this gap, we simulate real development\nscenarios encompassing three typical types of software development tasks and\nrecruit 27 computer science students to investigate their behavior with three\npopular ACATs. Our goal is to comprehensively assess ACATs' effectiveness,\nexplore characteristics of recommended code, identify reasons for\nmodifications, and understand users' challenges and expectations. To facilitate\nthe study, we develop an experimental platform that includes a data collection\nplugin for VSCode IDE and provides functions for screen recording, code\nevaluation, and automatic generation of personalized interview and survey\nquestions. Through analysis of the collected data, we find that ACATs generally\nenhance task completion rates, reduce time, improve code quality, and increase\nself-perceived productivity. However, the improvement is influenced by both the\nnature of coding tasks and users' experience level. Notably, for experienced\nparticipants, the use of ACATs may even increase completion time. We observe\nthat \"edited line completion\" is the most frequently recommended way, while\n\"comments completion\" and \"string completion\" have the lowest acceptance rates.\nThe primary reasons for modifying recommended code are disparities between\noutput formats and requirements, flawed logic, and inconsistent code styles. In\nterms of challenges and expectations, optimization of service access and help\ndocumentation is also concerned by participants except for functionality and\nperformance. Our study provides valuable insights into the effectiveness and\nusability of ACATs, informing further improvements in their design and\nimplementation.",
    "keywords": "AI-powered programming assistants, software development tasks, code quality, productivity, user experience."
  },
  {
    "Date": "2024.04",
    "Title": "Augmenting the Author: Exploring the Potential of AI Collaboration in Academic Writing",
    "link": "https://arxiv.org/abs/2404.16071",
    "abstract": "This workshop paper presents a critical examination of the integration of\nGenerative AI (Gen AI) into the academic writing process, focusing on the use\nof AI as a collaborative tool. It contrasts the performance and interaction of\ntwo AI models, Gemini and ChatGPT, through a collaborative inquiry approach\nwhere researchers engage in facilitated sessions to design prompts that elicit\nspecific AI responses for crafting research outlines. This case study\nhighlights the importance of prompt design, output analysis, and recognizing\nthe AI's limitations to ensure responsible and effective AI integration in\nscholarly work. Preliminary findings suggest that prompt variation\nsignificantly affects output quality and reveals distinct capabilities and\nconstraints of each model. The paper contributes to the field of Human-Computer\nInteraction by exploring effective prompt strategies and providing a\ncomparative analysis of Gen AI models, ultimately aiming to enhance AI-assisted\nacademic writing and prompt a deeper dialogue within the HCI community.",
    "keywords": "Generative AI, Academic Writing, Prompt Design, Human-Computer Interaction, AI Collaboration"
  },
  {
    "Date": "2024.05",
    "Title": "From Complexity to Clarity: How AI Enhances Perceptions of Scientists and the Public's Understanding of Science",
    "link": "https://arxiv.org/abs/2405.00706",
    "abstract": "This paper evaluated the effectiveness of using generative AI to simplify\nscience communication and enhance the public's understanding of science. By\ncomparing lay summaries of journal articles from PNAS, yoked to those generated\nby AI, this work first assessed linguistic simplicity differences across such\nsummaries and public perceptions in follow-up experiments. Specifically, Study\n1a analyzed simplicity features of PNAS abstracts (scientific summaries) and\nsignificance statements (lay summaries), observing that lay summaries were\nindeed linguistically simpler, but effect size differences were small. Study 1b\nused a large language model, GPT-4, to create significance statements based on\npaper abstracts and this more than doubled the average effect size without\nfine-tuning. Study 2 experimentally demonstrated that simply-written GPT\nsummaries facilitated more favorable perceptions of scientists (they were\nperceived as more credible and trustworthy, but less intelligent) than more\ncomplexly-written human PNAS summaries. Crucially, Study 3 experimentally\ndemonstrated that participants comprehended scientific writing better after\nreading simple GPT summaries compared to complex PNAS summaries. In their own\nwords, participants also summarized scientific papers in a more detailed and\nconcrete manner after reading GPT summaries compared to PNAS summaries of the\nsame article. AI has the potential to engage scientific communities and the\npublic via a simple language heuristic, advocating for its integration into\nscientific dissemination for a more informed society.",
    "keywords": "AI, science communication, public understanding, linguistic simplicity, scientific dissemination"
  },
  {
    "Date": "2024.05",
    "Title": "Transforming Software Development with Generative AI: Empirical Insights on Collaboration and Workflow",
    "link": "https://arxiv.org/abs/2405.01543",
    "abstract": "Generative AI (GenAI) has fundamentally changed how knowledge workers, such\nas software developers, solve tasks and collaborate to build software products.\nIntroducing innovative tools like ChatGPT and Copilot has created new\nopportunities to assist and augment software developers across various\nproblems. We conducted an empirical study involving interviews with 13 data\nscientists, managers, developers, designers, and frontend developers to\ninvestigate the usage of GenAI. Our study reveals that ChatGPT signifies a\nparadigm shift in the workflow of software developers. The technology empowers\ndevelopers by enabling them to work more efficiently, speed up the learning\nprocess, and increase motivation by reducing tedious and repetitive tasks.\nMoreover, our results indicate a change in teamwork collaboration due to\nsoftware engineers using GenAI for help instead of asking co-workers which\nimpacts the learning loop in agile teams.",
    "keywords": "Generative AI, software development, collaboration, workflow, ChatGPT"
  },
  {
    "Date": "2024.05",
    "Title": "What Can Natural Language Processing Do for Peer Review?",
    "link": "https://arxiv.org/abs/2405.06563",
    "abstract": "The number of scientific articles produced every year is growing rapidly.\nProviding quality control over them is crucial for scientists and, ultimately,\nfor the public good. In modern science, this process is largely delegated to\npeer review -- a distributed procedure in which each submission is evaluated by\nseveral independent experts in the field. Peer review is widely used, yet it is\nhard, time-consuming, and prone to error. Since the artifacts involved in peer\nreview -- manuscripts, reviews, discussions -- are largely text-based, Natural\nLanguage Processing has great potential to improve reviewing. As the emergence\nof large language models (LLMs) has enabled NLP assistance for many new tasks,\nthe discussion on machine-assisted peer review is picking up the pace. Yet,\nwhere exactly is help needed, where can NLP help, and where should it stand\naside? The goal of our paper is to provide a foundation for the future efforts\nin NLP for peer-reviewing assistance. We discuss peer review as a general\nprocess, exemplified by reviewing at AI conferences. We detail each step of the\nprocess from manuscript submission to camera-ready revision, and discuss the\nassociated challenges and opportunities for NLP assistance, illustrated by\nexisting work. We then turn to the big challenges in NLP for peer review as a\nwhole, including data acquisition and licensing, operationalization and\nexperimentation, and ethical issues. To help consolidate community efforts, we\ncreate a companion repository that aggregates key datasets pertaining to peer\nreview. Finally, we issue a detailed call for action for the scientific\ncommunity, NLP and AI researchers, policymakers, and funding bodies to help\nbring the research in NLP for peer review forward. We hope that our work will\nhelp set the agenda for research in machine-assisted scientific quality control\nin the age of AI, within the NLP community and beyond.",
    "keywords": "Natural Language Processing, Peer Review, Scientific Articles, Quality Control, Large Language Models"
  },
  {
    "Date": "2024.05",
    "Title": "Using ChatGPT for Thematic Analysis",
    "link": "https://arxiv.org/abs/2405.08828",
    "abstract": "The utilisation of AI-driven tools, notably ChatGPT, within academic research\nis increasingly debated from several perspectives including ease of\nimplementation, and potential enhancements in research efficiency, as against\nethical concerns and risks such as biases and unexplained AI operations. This\npaper explores the use of the GPT model for initial coding in qualitative\nthematic analysis using a sample of UN policy documents. The primary aim of\nthis study is to contribute to the methodological discussion regarding the\nintegration of AI tools, offering a practical guide to validation for using GPT\nas a collaborative research assistant. The paper outlines the advantages and\nlimitations of this methodology and suggests strategies to mitigate risks.\nEmphasising the importance of transparency and reliability in employing GPT\nwithin research methodologies, this paper argues for a balanced use of AI in\nsupported thematic analysis, highlighting its potential to elevate research\nefficacy and outcomes.",
    "keywords": "AI-driven tools, qualitative thematic analysis, GPT model, research efficiency, ethical concerns."
  },
  {
    "Date": "2024.06",
    "Title": "Using AI-Based Coding Assistants in Practice: State of Affairs, Perceptions, and Ways Forward",
    "link": "https://arxiv.org/abs/2406.07765",
    "abstract": "Context. The last several years saw the emergence of AI assistants for code -\nmulti-purpose AI-based helpers in software engineering. As they become\nomnipresent in all aspects of software development, it becomes critical to\nunderstand their usage patterns.\n  Objective. We aim to better understand how specifically developers are using\nAI assistants, why they are not using them in certain parts of their\ndevelopment workflow, and what needs to be improved in the future.\n  Methods. In this work, we carried out a large-scale survey aimed at how AI\nassistants are used, focusing on specific software development activities and\nstages. We collected opinions of 481 programmers on five broad activities: (a)\nimplementing new features, (b) writing tests, (c) bug triaging, (d)\nrefactoring, and (e) writing natural-language artifacts, as well as their\nindividual stages.\n  Results. Our results provide a novel comparison of different stages where AI\nassistants are used that is both comprehensive and detailed. It highlights\nspecific activities that developers find less enjoyable and want to delegate to\nan AI assistant, e.g., writing tests and natural-language artifacts. We also\ndetermine more granular stages where AI assistants are used, such as generating\ntests and generating docstrings, as well as less studied parts of the workflow,\nsuch as generating test data. Among the reasons for not using assistants, there\nare general aspects like trust and company policies, as well as more concrete\nissues like the lack of project-size context, which can be the focus of the\nfuture research.\n  Conclusion. The provided analysis highlights stages of software development\nthat developers want to delegate and that are already popular for using AI\nassistants, which can be a good focus for features aimed to help developers\nright now. The main reasons for not using AI assistants can serve as a\nguideline for future work.",
    "keywords": "AI assistants, software development, usage patterns, developer perceptions, future improvements."
  },
  {
    "Date": "2024.06",
    "Title": "Explain the Black Box for the Sake of Science: the Scientific Method in the Era of Generative Artificial Intelligence",
    "link": "https://arxiv.org/abs/2406.10557",
    "abstract": "The scientific method is the cornerstone of human progress across all\nbranches of the natural and applied sciences, from understanding the human body\nto explaining how the universe works. The scientific method is based on\nidentifying systematic rules or principles that describe the phenomenon of\ninterest in a reproducible way that can be validated through experimental\nevidence. In the era of generative artificial intelligence, there are\ndiscussions on how AI systems may discover new knowledge. We argue that human\ncomplex reasoning for scientific discovery remains of vital importance, at\nleast before the advent of artificial general intelligence. Yet, AI can be\nleveraged for scientific discovery via explainable AI. More specifically,\nknowing the `principles' the AI systems used to make decisions can be a point\nof contact with domain experts and scientists, that can lead to divergent or\nconvergent views on a given scientific problem. Divergent views may spark\nfurther scientific investigations leading to interpretability-guided\nexplanations (IGEs), and possibly to new scientific knowledge. We define this\nfield as Explainable AI for Science, where domain experts -- potentially\nassisted by generative AI -- formulate scientific hypotheses and explanations\nbased on the interpretability of a predictive AI system.",
    "keywords": "Scientific Method, Generative Artificial Intelligence, Explainable AI, Scientific Discovery, Interpretability-guided Explanations (IGEs)"
  },
  {
    "Date": "2024.06",
    "Title": "Boosting Scientific Concepts Understanding: Can Analogy from Teacher Models Empower Student Models?",
    "link": "https://arxiv.org/abs/2406.11375",
    "abstract": "Analogical reasoning plays a critical role in human cognition, enabling us to\nunderstand new concepts by associating them with familiar ones. Previous\nresearch in the AI community has mainly focused on identifying and generating\nanalogies and then examining their quality under human evaluation, which\noverlooks the practical application of these analogies in real-world settings.\nInspired by the human education process, in this paper, we propose to\ninvestigate how analogies created by teacher language models (LMs) can assist\nstudent LMs in understanding scientific concepts, thereby aligning more closely\nwith practical scenarios. Our results suggest that free-form analogies can\nindeed aid LMs in understanding concepts. Additionally, analogies generated by\nstudent LMs can improve their own performance on scientific question answering,\ndemonstrating their capability to use analogies for self-learning new\nknowledge. Resources are available at https://github.com/siyuyuan/SCUA.",
    "keywords": "Analogy, Teacher Models, Student Models, Scientific Concepts Understanding, Analogical Reasoning"
  },
  {
    "Date": "2024.07",
    "Title": "Large Language Models as Misleading Assistants in Conversation",
    "link": "https://arxiv.org/abs/2407.11789",
    "abstract": "Large Language Models (LLMs) are able to provide assistance on a wide range\nof information-seeking tasks. However, model outputs may be misleading, whether\nunintentionally or in cases of intentional deception. We investigate the\nability of LLMs to be deceptive in the context of providing assistance on a\nreading comprehension task, using LLMs as proxies for human users. We compare\noutcomes of (1) when the model is prompted to provide truthful assistance, (2)\nwhen it is prompted to be subtly misleading, and (3) when it is prompted to\nargue for an incorrect answer. Our experiments show that GPT-4 can effectively\nmislead both GPT-3.5-Turbo and GPT-4, with deceptive assistants resulting in up\nto a 23% drop in accuracy on the task compared to when a truthful assistant is\nused. We also find that providing the user model with additional context from\nthe passage partially mitigates the influence of the deceptive model. This work\nhighlights the ability of LLMs to produce misleading information and the\neffects this may have in real-world situations.",
    "keywords": "Large Language Models, Misleading Assistants, Conversation, Deception, Reading Comprehension Task."
  },
  {
    "Date": "2024.07",
    "Title": "The Great AI Witch Hunt: Reviewers Perception and (Mis)Conception of Generative AI in Research Writing",
    "link": "https://arxiv.org/abs/2407.12015",
    "abstract": "Generative AI (GenAI) use in research writing is growing fast. However, it is\nunclear how peer reviewers recognize or misjudge AI-augmented manuscripts. To\ninvestigate the impact of AI-augmented writing on peer reviews, we conducted a\nsnippet-based online survey with 17 peer reviewers from top-tier HCI\nconferences. Our findings indicate that while AI-augmented writing improves\nreadability, language diversity, and informativeness, it often lacks research\ndetails and reflective insights from authors. Reviewers consistently struggled\nto distinguish between human and AI-augmented writing but their judgements\nremained consistent. They noted the loss of a \"human touch\" and subjective\nexpressions in AI-augmented writing. Based on our findings, we advocate for\nreviewer guidelines that promote impartial evaluations of submissions,\nregardless of any personal biases towards GenAI. The quality of the research\nitself should remain a priority in reviews, regardless of any preconceived\nnotions about the tools used to create it. We emphasize that researchers must\nmaintain their authorship and control over the writing process, even when using\nGenAI's assistance.",
    "keywords": "Generative AI, peer reviewers, research writing, AI-augmented manuscripts, impartial evaluations."
  },
  {
    "Date": "2024.07",
    "Title": "Exploring the Evidence-Based Beliefs and Behaviors of LLM-Based Programming Assistants",
    "link": "https://arxiv.org/abs/2407.13900",
    "abstract": "Recent innovations in artificial intelligence (AI), primarily powered by\nlarge language models (LLMs), have transformed how programmers develop and\nmaintain software -- leading to new frontiers in software engineering (SE). The\nadvanced capabilities of LLM-based programming assistants to support software\ndevelopment tasks have led to a rise in the adoption of LLMs in SE. However,\nlittle is known about the evidenced-based practices, tools and processes\nverified by research findings, supported and adopted by AI programming\nassistants. To this end, our work conducts a preliminary evaluation exploring\nthe beliefs and behaviors of LLM used to support software development tasks. We\ninvestigate 17 evidence-based claims posited by empirical SE research across\nfive LLM-based programming assistants. Our findings show that LLM-based\nprogramming assistants have ambiguous beliefs regarding research claims, lack\ncredible evidence to support responses, and are incapable of adopting practices\ndemonstrated by empirical SE research to support development tasks. Based on\nour results, we provide implications for practitioners adopting LLM-based\nprogramming assistants in development contexts and shed light on future\nresearch directions to enhance the reliability and trustworthiness of LLMs --\naiming to increase awareness and adoption of evidence-based SE research\nfindings in practice.",
    "keywords": "Artificial Intelligence, Large Language Models, Software Engineering, Evidence-Based Practices, Programming Assistants."
  },
  {
    "Date": "2024.08",
    "Title": "Generative AI Tools in Academic Research: Applications and Implications for Qualitative and Quantitative Research Methodologies",
    "link": "https://arxiv.org/abs/2408.06872",
    "abstract": "This study examines the impact of Generative Artificial Intelligence (GenAI)\non academic research, focusing on its application to qualitative and\nquantitative data analysis. As GenAI tools evolve rapidly, they offer new\npossibilities for enhancing research productivity and democratising complex\nanalytical processes. However, their integration into academic practice raises\nsignificant questions regarding research integrity and security, authorship,\nand the changing nature of scholarly work. Through an examination of current\ncapabilities and potential future applications, this study provides insights\ninto how researchers may utilise GenAI tools responsibly and ethically.\n  We present case studies that demonstrate the application of GenAI in various\nresearch methodologies, discuss the challenges of replicability and consistency\nin AI-assisted research, and consider the ethical implications of increased AI\nintegration in academia. This study explores both qualitative and quantitative\napplications of GenAI, highlighting tools for transcription, coding, thematic\nanalysis, visual analytics, and statistical analysis. By addressing these\nissues, we aim to contribute to the ongoing discourse on the role of AI in\nshaping the future of academic research and provide guidance for researchers\nexploring the rapidly evolving landscape of AI-assisted research tools and\nresearch.",
    "keywords": "Generative Artificial Intelligence, academic research, qualitative research, quantitative research, ethical implications."
  },
  {
    "Date": "2024.09",
    "Title": "Emerging Reliance Behaviors in Human-AI Text Generation: Hallucinations, Data Quality Assessment, and Cognitive Forcing Functions",
    "link": "https://arxiv.org/abs/2409.08937",
    "abstract": "In this paper, we investigate the impact of hallucinations and cognitive\nforcing functions in human-AI collaborative text generation tasks, focusing on\nthe use of Large Language Models (LLMs) to assist in generating high-quality\nconversational data. LLMs require data for fine-tuning, a crucial step in\nenhancing their performance. In the context of conversational customer support,\nthe data takes the form of a conversation between a human customer and an agent\nand can be generated with an AI assistant. In our inquiry, involving 11 users\nwho each completed 8 tasks, resulting in a total of 88 tasks, we found that the\npresence of hallucinations negatively impacts the quality of data. We also find\nthat, although the cognitive forcing function does not always mitigate the\ndetrimental effects of hallucinations on data quality, the presence of\ncognitive forcing functions and hallucinations together impacts data quality\nand influences how users leverage the AI responses presented to them. Our\nanalysis of user behavior reveals distinct patterns of reliance on AI-generated\nresponses, highlighting the importance of managing hallucinations in\nAI-generated content within conversational AI contexts.",
    "keywords": "\"Human-AI collaborative text generation\", \"hallucinations\", \"cognitive forcing functions\", \"data quality assessment\", \"Large Language Models (LLMs)\""
  },
  {
    "Date": "2024.09",
    "Title": "Towards Ethical Personal AI Applications: Practical Considerations for AI Assistants with Long-Term Memory",
    "link": "https://arxiv.org/abs/2409.11192",
    "abstract": "One application area of long-term memory (LTM) capabilities with increasing\ntraction is personal AI companions and assistants. With the ability to retain\nand contextualize past interactions and adapt to user preferences, personal AI\ncompanions and assistants promise a profound shift in how we interact with AI\nand are on track to become indispensable in personal and professional settings.\nHowever, this advancement introduces new challenges and vulnerabilities that\nrequire careful consideration regarding the deployment and widespread use of\nthese systems. The goal of this paper is to explore the broader implications of\nbuilding and deploying personal AI applications with LTM capabilities using a\nholistic evaluation approach. This will be done in three ways: 1) reviewing the\ntechnological underpinnings of LTM in Large Language Models, 2) surveying\ncurrent personal AI companions and assistants, and 3) analyzing critical\nconsiderations and implications of deploying and using these applications.",
    "keywords": "Long-Term Memory, Personal AI Assistants, Ethical Considerations, Large Language Models, Deployment Implications"
  },
  {
    "Date": "2024.09",
    "Title": "Mining Causality: AI-Assisted Search for Instrumental Variables",
    "link": "https://arxiv.org/abs/2409.14202",
    "abstract": "The instrumental variables (IVs) method is a leading empirical strategy for\ncausal inference. Finding IVs is a heuristic and creative process, and\njustifying its validity--especially exclusion restrictions--is largely\nrhetorical. We propose using large language models (LLMs) to search for new IVs\nthrough narratives and counterfactual reasoning, similar to how a human\nresearcher would. The stark difference, however, is that LLMs can dramatically\naccelerate this process and explore an extremely large search space. We\ndemonstrate how to construct prompts to search for potentially valid IVs. We\ncontend that multi-step and role-playing prompting strategies are effective for\nsimulating the endogenous decision-making processes of economic agents and for\nnavigating language models through the realm of real-world scenarios. We apply\nour method to three well-known examples in economics: returns to schooling,\nsupply and demand, and peer effects. We then extend our strategy to finding (i)\ncontrol variables in regression and difference-in-differences and (ii) running\nvariables in regression discontinuity designs.",
    "keywords": "Instrumental Variables, Causal Inference, Large Language Models, Counterfactual Reasoning, Regression Discontinuity Design"
  },
  {
    "Date": "2024.10",
    "Title": "The why, what, and how of AI-based coding in scientific research",
    "link": "https://arxiv.org/abs/2410.02156",
    "abstract": "Computer programming (coding) is indispensable for researchers across\ndisciplines, yet it remains challenging to learn and time-consuming to carry\nout. Generative AI, particularly large language models (LLMs), has the\npotential to transform coding into intuitive conversations, but best practices\nand effective workflows are only emerging. We dissect AI-based coding through\nthree key lenses: the nature and role of LLMs in coding (why), six types of\ncoding assistance they provide (what), and a five-step workflow in action with\npractical implementation strategies (how). Additionally, we address the\nlimitations and future outlook of AI in coding. By offering actionable\ninsights, this framework helps to guide researchers in effectively leveraging\nAI to enhance coding practices and education, accelerating scientific progress.",
    "keywords": "AI-based coding, scientific research, large language models, coding assistance, workflow strategies"
  },
  {
    "Date": "2024.10",
    "Title": "How Does the Disclosure of AI Assistance Affect the Perceptions of Writing?",
    "link": "https://arxiv.org/abs/2410.04545",
    "abstract": "Recent advances in generative AI technologies like large language models have\nboosted the incorporation of AI assistance in writing workflows, leading to the\nrise of a new paradigm of human-AI co-creation in writing. To understand how\npeople perceive writings that are produced under this paradigm, in this paper,\nwe conduct an experimental study to understand whether and how the disclosure\nof the level and type of AI assistance in the writing process would affect\npeople's perceptions of the writing on various aspects, including their\nevaluation on the quality of the writing and their ranking of different\nwritings. Our results suggest that disclosing the AI assistance in the writing\nprocess, especially if AI has provided assistance in generating new content,\ndecreases the average quality ratings for both argumentative essays and\ncreative stories. This decrease in the average quality ratings often comes with\nan increased level of variations in different individuals' quality evaluations\nof the same writing. Indeed, factors such as an individual's writing confidence\nand familiarity with AI writing assistants are shown to moderate the impact of\nAI assistance disclosure on their writing quality evaluations. We also find\nthat disclosing the use of AI assistance may significantly reduce the\nproportion of writings produced with AI's content generation assistance among\nthe top-ranked writings.",
    "keywords": "AI assistance, writing perceptions, disclosure, quality evaluations, human-AI co-creation."
  },
  {
    "Date": "2024.10",
    "Title": "Need Help? Designing Proactive AI Assistants for Programming",
    "link": "https://arxiv.org/abs/2410.04596",
    "abstract": "While current chat-based AI assistants primarily operate reactively,\nresponding only when prompted by users, there is significant potential for\nthese systems to proactively assist in tasks without explicit invocation,\nenabling a mixed-initiative interaction. This work explores the design and\nimplementation of proactive AI assistants powered by large language models. We\nfirst outline the key design considerations for building effective proactive\nassistants. As a case study, we propose a proactive chat-based programming\nassistant that automatically provides suggestions and facilitates their\nintegration into the programmer's code. The programming context provides a\nshared workspace enabling the assistant to offer more relevant suggestions. We\nconducted a randomized experimental study examining the impact of various\ndesign elements of the proactive assistant on programmer productivity and user\nexperience. Our findings reveal significant benefits of incorporating proactive\nchat assistants into coding environments and uncover important nuances that\ninfluence their usage and effectiveness.",
    "keywords": "Proactive AI Assistants, Programming, Large Language Models, Mixed-Initiative Interaction, User Experience"
  },
  {
    "Date": "2024.11",
    "Title": "Disrupting Test Development with AI Assistants",
    "link": "https://arxiv.org/abs/2411.02328",
    "abstract": "Recent advancements in large language models, including GPT-4 and its\nvariants, and Generative AI-assisted coding tools like GitHub Copilot, ChatGPT,\nand Tabnine, have significantly transformed software development. This paper\nanalyzes how these innovations impact productivity and software test\ndevelopment metrics. These tools enable developers to generate complete\nsoftware programs with minimal human intervention before deployment. However,\nthorough review and testing by developers are still crucial. Utilizing the Test\nPyramid concept, which categorizes tests into unit, integration, and end-to-end\ntests, we evaluate three popular AI coding assistants by generating and\ncomparing unit tests for opensource modules. Our findings show that\nAI-generated tests are of equivalent quality to original tests, highlighting\ndifferences in usage and results among the tools. This research enhances the\nunderstanding and capabilities of AI-assistant tools in automated testing.",
    "keywords": "AI Assistants, Test Development, Software Development, Test Pyramid, Automated Testing"
  },
  {
    "Date": "2024.11",
    "Title": "AI-Empowered Human Research Integrating Brain Science and Social Sciences Insights",
    "link": "https://arxiv.org/abs/2411.12761",
    "abstract": "This paper explores the transformative role of artificial intelligence (AI)\nin enhancing scientific research, particularly in the fields of brain science\nand social sciences. We analyze the fundamental aspects of human research and\nargue that it is high time for researchers to transition to human-AI joint\nresearch. Building upon this foundation, we propose two innovative research\nparadigms of human-AI joint research: \"AI-Brain Science Research Paradigm\" and\n\"AI-Social Sciences Research Paradigm\". In these paradigms, we introduce three\nhuman-AI collaboration models: AI as a research tool (ART), AI as a research\nassistant (ARA), and AI as a research participant (ARP). Furthermore, we\noutline the methods for conducting human-AI joint research. This paper seeks to\nredefine the collaborative interactions between human researchers and AI\nsystem, setting the stage for future research directions and sparking\ninnovation in this interdisciplinary field.",
    "keywords": "Artificial Intelligence, Human Research, Brain Science, Social Sciences, Human-AI Joint Research."
  },
  {
    "Date": "2024.11",
    "Title": "Probing the limitations of multimodal language models for chemistry and materials research",
    "link": "https://arxiv.org/abs/2411.16955",
    "abstract": "Recent advancements in artificial intelligence have sparked interest in\nscientific assistants that could support researchers across the full spectrum\nof scientific workflows, from literature review to experimental design and data\nanalysis. A key capability for such systems is the ability to process and\nreason about scientific information in both visual and textual forms - from\ninterpreting spectroscopic data to understanding laboratory setups. Here, we\nintroduce MaCBench, a comprehensive benchmark for evaluating how\nvision-language models handle real-world chemistry and materials science tasks\nacross three core aspects: data extraction, experimental understanding, and\nresults interpretation. Through a systematic evaluation of leading models, we\nfind that while these systems show promising capabilities in basic perception\ntasks - achieving near-perfect performance in equipment identification and\nstandardized data extraction - they exhibit fundamental limitations in spatial\nreasoning, cross-modal information synthesis, and multi-step logical inference.\nOur insights have important implications beyond chemistry and materials\nscience, suggesting that developing reliable multimodal AI scientific\nassistants may require advances in curating suitable training data and\napproaches to training those models.",
    "keywords": "Artificial Intelligence, Multimodal Language Models, Chemistry, Materials Science, Benchmark Evaluation"
  },
  {
    "Date": "2024.12",
    "Title": "The impact of AI on engineering design procedures for dynamical systems",
    "link": "https://arxiv.org/abs/2412.12230",
    "abstract": "Artificial intelligence (AI) is driving transformative changes across\nnumerous fields, revolutionizing conventional processes and creating new\nopportunities for innovation. The development of mechatronic systems is\nundergoing a similar transformation. Over the past decade, modeling,\nsimulation, and optimization techniques have become integral to the design\nprocess, paving the way for the adoption of AI-based methods. In this paper, we\nexamine the potential for integrating AI into the engineering design process,\nusing the V-model from the VDI guideline 2206, considered the state-of-the-art\nin product design, as a foundation. We identify and classify AI methods based\non their suitability for specific stages within the engineering product design\nworkflow. Furthermore, we present a series of application examples where\nAI-assisted design has been successfully implemented by the authors. These\nexamples, drawn from research projects within the DFG Priority Program\n\\emph{SPP~2353: Daring More Intelligence - Design Assistants in Mechanics and\nDynamics}, showcase a diverse range of applications across mechanics and\nmechatronics, including areas such as acoustics and robotics.",
    "keywords": "Artificial Intelligence, Engineering Design, Dynamical Systems, Mechatronic Systems, V-model Design Process."
  },
  {
    "Date": "2024.12",
    "Title": "Hints Help Finding and Fixing Bugs Differently in Python and Text-based Program Representations",
    "link": "https://arxiv.org/abs/2412.12471",
    "abstract": "With the recent advances in AI programming assistants such as GitHub Copilot,\nprogramming is not limited to classical programming languages\nanymore--programming tasks can also be expressed and solved by end-users in\nnatural text. Despite the availability of this new programming modality, users\nstill face difficulties with algorithmic understanding and program debugging.\nOne promising approach to support end-users is to provide hints to help them\nfind and fix bugs while forming and improving their programming capabilities.\nWhile it is plausible that hints can help, it is unclear which type of hint is\nhelpful and how this depends on program representations (classic source code or\na textual representation) and the user's capability of understanding the\nalgorithmic task. To understand the role of hints in this space, we conduct a\nlarge-scale crowd-sourced study involving 753 participants investigating the\neffect of three types of hints (test cases, conceptual, and detailed), across\ntwo program representations (Python and text-based), and two groups of users\n(with clear understanding or confusion about the algorithmic task). We find\nthat the program representation (Python vs. text) has a significant influence\non the users' accuracy at finding and fixing bugs. Surprisingly, users are more\naccurate at finding and fixing bugs when they see the program in natural text.\nHints are generally helpful in improving accuracy, but different hints help\ndifferently depending on the program representation and the user's\nunderstanding of the algorithmic task. These findings have implications for\ndesigning next-generation programming tools that provide personalized support\nto users, for example, by adapting the programming modality and providing hints\nwith respect to the user's skill level and understanding.",
    "keywords": "AI programming assistants, bug fixing, hints, program representations, user understanding"
  },
  {
    "Date": "2025.01",
    "Title": "Experience with GitHub Copilot for Developer Productivity at Zoominfo",
    "link": "https://arxiv.org/abs/2501.13282",
    "abstract": "This paper presents a comprehensive evaluation of GitHub Copilot's deployment\nand impact on developer productivity at Zoominfo, a leading Go-To-Market (GTM)\nIntelligence Platform. We describe our systematic four-phase approach to\nevaluating and deploying GitHub Copilot across our engineering organization,\ninvolving over 400 developers. Our analysis combines both quantitative metrics,\nfocusing on acceptance rates of suggestions given by GitHub Copilot and\nqualitative feedback given by developers through developer satisfaction\nsurveys. The results show an average acceptance rate of 33% for suggestions and\n20% for lines of code, with high developer satisfaction scores of 72%. We also\ndiscuss language-specific performance variations, limitations, and lessons\nlearned from this medium-scale enterprise deployment. Our findings contribute\nto the growing body of knowledge about AI-assisted software development in\nenterprise settings.",
    "keywords": "GitHub Copilot, Developer Productivity, Zoominfo, AI-assisted software development, Enterprise deployment."
  },
  {
    "Date": "2025.01",
    "Title": "Towards Decoding Developer Cognition in the Age of AI Assistants",
    "link": "https://arxiv.org/abs/2501.02684",
    "abstract": "Background: The increasing adoption of AI assistants in programming has led\nto numerous studies exploring their benefits. While developers consistently\nreport significant productivity gains from these tools, empirical measurements\noften show more modest improvements. While prior research has documented\nself-reported experiences with AI-assisted programming tools, little to no work\nhas been done to understand their usage patterns and the actual cognitive load\nimposed in practice. Objective: In this exploratory study, we aim to\ninvestigate the role AI assistants play in developer productivity.\nSpecifically, we are interested in how developers' expertise levels influence\ntheir AI usage patterns, and how these patterns impact their actual cognitive\nload and productivity during development tasks. We also seek to better\nunderstand how this relates to their perceived productivity. Method: We propose\na controlled observational study combining physiological measurements (EEG and\neye tracking) with interaction data to examine developers' use of AI-assisted\nprogramming tools. We will recruit professional developers to complete\nprogramming tasks both with and without AI assistance while measuring their\ncognitive load and task completion time. Through pre- and post-task\nquestionnaires, we will collect data on perceived productivity and cognitive\nload using NASA-TLX.",
    "keywords": "AI assistants, developer productivity, cognitive load, programming tools, expertise levels."
  },
  {
    "Date": "2025.01",
    "Title": "\"It makes you think\": Provocations Help Restore Critical Thinking to AI-Assisted Knowledge Work",
    "link": "https://arxiv.org/abs/2501.17247",
    "abstract": "Recent research suggests that the use of Generative AI tools may result in\ndiminished critical thinking during knowledge work. We study the effect on\nknowledge work of provocations: brief textual prompts that offer critiques for\nand propose alternatives to AI suggestions. We conduct a between-subjects study\n(n=24) in which participants completed AI-assisted shortlisting tasks with and\nwithout provocations. We find that provocations can induce critical and\nmetacognitive thinking. We derive five dimensions that impact the user\nexperience of provocations: task urgency, task importance, user expertise,\nprovocation actionability, and user responsibility. We connect our findings to\nrelated work on design frictions, microboundaries, and distributed cognition.\nWe draw design implications for critical thinking interventions in AI-assisted\nknowledge work.",
    "keywords": "\"Generative AI\", \"Critical Thinking\", \"Knowledge Work\", \"Provocations\", \"Metacognitive Thinking\""
  },
  {
    "Date": "2025.02",
    "Title": "Evaluating Sakana's AI Scientist for Autonomous Research: Wishful Thinking or an Emerging Reality Towards 'Artificial Research Intelligence' (ARI)?",
    "link": "https://arxiv.org/abs/2502.14297",
    "abstract": "A major step toward Artificial General Intelligence (AGI) and Super\nIntelligence is AI's ability to autonomously conduct research - what we term\nArtificial Research Intelligence (ARI). If machines could generate hypotheses,\nconduct experiments, and write research papers without human intervention, it\nwould transform science. Sakana recently introduced the 'AI Scientist',\nclaiming to conduct research autonomously, i.e. they imply to have achieved\nwhat we term Artificial Research Intelligence (ARI). The AI Scientist gained\nmuch attention, but a thorough independent evaluation has yet to be conducted.\n  Our evaluation of the AI Scientist reveals critical shortcomings. The\nsystem's literature reviews produced poor novelty assessments, often\nmisclassifying established concepts (e.g., micro-batching for stochastic\ngradient descent) as novel. It also struggles with experiment execution: 42% of\nexperiments failed due to coding errors, while others produced flawed or\nmisleading results. Code modifications were minimal, averaging 8% more\ncharacters per iteration, suggesting limited adaptability. Generated\nmanuscripts were poorly substantiated, with a median of five citations, most\noutdated (only five of 34 from 2020 or later). Structural errors were frequent,\nincluding missing figures, repeated sections, and placeholder text like\n'Conclusions Here'. Some papers contained hallucinated numerical results.\n  Despite these flaws, the AI Scientist represents a leap forward in research\nautomation. It generates full research manuscripts with minimal human input,\nchallenging expectations of AI-driven science. Many reviewers might struggle to\ndistinguish its work from human researchers. While its quality resembles a\nrushed undergraduate paper, its speed and cost efficiency are unprecedented,\nproducing a full paper for USD 6 to 15 with 3.5 hours of human involvement, far\noutpacing traditional researchers.",
    "keywords": "Artificial Research Intelligence (ARI), AI Scientist, autonomous research, Sakana, research automation"
  },
  {
    "Date": "2025.03",
    "Title": "Position: AI agents should be regulated based on autonomous action sequences",
    "link": "https://arxiv.org/abs/2503.04750",
    "abstract": "This position paper argues that AI agents should be regulated based on the\nsequence of actions they autonomously take. AI agents with long-term planning\nand strategic capabilities can pose significant risks of human extinction and\nirreversible global catastrophes. While existing regulations often focus on\ncomputational scale as a proxy for potential harm, we contend that such\nmeasures are insufficient for assessing the risks posed by AI agents whose\ncapabilities arise primarily from inference-time computation. To support our\nposition, we discuss relevant regulations and recommendations from AI\nscientists regarding existential risks, as well as the advantages of action\nsequences over existing impact measures that require observing environmental\nstates.",
    "keywords": "AI agents, regulation, autonomous action sequences, existential risks, inference-time computation."
  },
  {
    "Date": "2025.03",
    "Title": "Unlocking the Potential of AI Researchers in Scientific Discovery: What Is Missing?",
    "link": "https://arxiv.org/abs/2503.05822",
    "abstract": "The potential of AI researchers in scientific discovery remains largely to be\nunleashed. Over the past decade, the presence of AI for Science (AI4Science) in\nthe 145 Nature Index journals has increased ninefold, yet nearly 90% of\nAI4Science research remains predominantly led by experimental scientists.\nDrawing on the Diffusion of Innovation theory, we project that AI4Science's\nshare of total publications will rise from 3.57% in 2024 to approximately 25%\nby 2050. Unlocking the potential of AI researchers is essential for driving\nthis shift and fostering deeper integration of AI expertise into the research\necosystem. To this end, we propose structured and actionable workflows,\nalongside key strategies to position AI researchers at the forefront of\nscientific discovery. Furthermore, we outline three pivotal pathways: equipping\nexperimental scientists with user-friendly AI tools to amplify the impact of AI\nresearchers, bridging cognitive and methodological gaps to enable more direct\nparticipation in scientific discovery, and proactively cultivating a thriving\nAI-driven scientific ecosystem. By addressing these challenges, this work aims\nto empower AI researchers as a driving force in shaping the future of\nscientific discovery.",
    "keywords": "AI researchers, scientific discovery, AI4Science, Diffusion of Innovation, integration of AI expertise."
  },
  {
    "Date": "2025.02",
    "Title": "Superintelligent Agents Pose Catastrophic Risks: Can Scientist AI Offer a Safer Path?",
    "link": "https://arxiv.org/abs/2502.15657",
    "abstract": "The leading AI companies are increasingly focused on building generalist AI\nagents -- systems that can autonomously plan, act, and pursue goals across\nalmost all tasks that humans can perform. Despite how useful these systems\nmight be, unchecked AI agency poses significant risks to public safety and\nsecurity, ranging from misuse by malicious actors to a potentially irreversible\nloss of human control. We discuss how these risks arise from current AI\ntraining methods. Indeed, various scenarios and experiments have demonstrated\nthe possibility of AI agents engaging in deception or pursuing goals that were\nnot specified by human operators and that conflict with human interests, such\nas self-preservation. Following the precautionary principle, we see a strong\nneed for safer, yet still useful, alternatives to the current agency-driven\ntrajectory. Accordingly, we propose as a core building block for further\nadvances the development of a non-agentic AI system that is trustworthy and\nsafe by design, which we call Scientist AI. This system is designed to explain\nthe world from observations, as opposed to taking actions in it to imitate or\nplease humans. It comprises a world model that generates theories to explain\ndata and a question-answering inference machine. Both components operate with\nan explicit notion of uncertainty to mitigate the risks of overconfident\npredictions. In light of these considerations, a Scientist AI could be used to\nassist human researchers in accelerating scientific progress, including in AI\nsafety. In particular, our system can be employed as a guardrail against AI\nagents that might be created despite the risks involved. Ultimately, focusing\non non-agentic AI may enable the benefits of AI innovation while avoiding the\nrisks associated with the current trajectory. We hope these arguments will\nmotivate researchers, developers, and policymakers to favor this safer path.",
    "keywords": "AI safety, generalist AI agents, Scientist AI, non-agentic AI, precautionary principle."
  },
  {
    "Date": "2025.02",
    "Title": "Performance Evaluation of Large Language Models in Statistical Programming",
    "link": "https://arxiv.org/abs/2502.13117",
    "abstract": "The programming capabilities of large language models (LLMs) have\nrevolutionized automatic code generation and opened new avenues for automatic\nstatistical analysis. However, the validity and quality of these generated\ncodes need to be systematically evaluated before they can be widely adopted.\nDespite their growing prominence, a comprehensive evaluation of statistical\ncode generated by LLMs remains scarce in the literature. In this paper, we\nassess the performance of LLMs, including two versions of ChatGPT and one\nversion of Llama, in the domain of SAS programming for statistical analysis.\nOur study utilizes a set of statistical analysis tasks encompassing diverse\nstatistical topics and datasets. Each task includes a problem description,\ndataset information, and human-verified SAS code. We conduct a comprehensive\nassessment of the quality of SAS code generated by LLMs through human expert\nevaluation based on correctness, effectiveness, readability, executability, and\nthe accuracy of output results. The analysis of rating scores reveals that\nwhile LLMs demonstrate usefulness in generating syntactically correct code,\nthey struggle with tasks requiring deep domain understanding and may produce\nredundant or incorrect results. This study offers valuable insights into the\ncapabilities and limitations of LLMs in statistical programming, providing\nguidance for future advancements in AI-assisted coding systems for statistical\nanalysis.",
    "keywords": "Large Language Models, Statistical Programming, SAS Programming, Performance Evaluation, Automatic Code Generation"
  }
]