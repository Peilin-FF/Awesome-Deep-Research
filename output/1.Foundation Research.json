[
  {
    "Date": "2017",
    "Title": "Natural Language is a Programming Language: Applying Natural Language Processing to Software Development",
    "link": "https://drops.dagstuhl.de/storage/00lipics/lipics-vol071-snapl2017/LIPIcs.SNAPL.2017.4/LIPIcs.SNAPL.2017.4.pdf",
    "abstract": "A powerful, but limited, way to view software is as source code alone. Treating a program as a sequence of instructions enables it to be formalized and makes it amenable to mathematical techniques such as abstract interpretation and model checking. A program consists of much more than a sequence of instructions. Developers make use of test cases, documentation, variable names, program structure, the version control repository, and more. I argue that it is time to take the blinders off of software analysis tools: tools should use all these artifacts to deduce more powerful and useful information about the program.Researchers are beginning to make progress towards this vision. This paper gives, as examples,four results that find bugs and generate code by applying natural language processing techniques to software artifacts. The four techniques use as input error messages, variable names, procedure documentation, and user questions. They use four different NLP techniques: document similarity,word semantics, parse trees, and neural networks.The initial results suggest that this is a promising avenue for future work.",
    "keywords": "Natural Language Processing, Software Development, Bug Detection, Code Generation, Mathematical Techniques."
  },
  {
    "Date": "2022.03",
    "Title": "Training language models to follow instructions with human feedback",
    "link": "https://arxiv.org/abs/2203.02155",
    "abstract": "Making language models bigger does not inherently make them better at\nfollowing a user's intent. For example, large language models can generate\noutputs that are untruthful, toxic, or simply not helpful to the user. In other\nwords, these models are not aligned with their users. In this paper, we show an\navenue for aligning language models with user intent on a wide range of tasks\nby fine-tuning with human feedback. Starting with a set of labeler-written\nprompts and prompts submitted through the OpenAI API, we collect a dataset of\nlabeler demonstrations of the desired model behavior, which we use to fine-tune\nGPT-3 using supervised learning. We then collect a dataset of rankings of model\noutputs, which we use to further fine-tune this supervised model using\nreinforcement learning from human feedback. We call the resulting models\nInstructGPT. In human evaluations on our prompt distribution, outputs from the\n1.3B parameter InstructGPT model are preferred to outputs from the 175B GPT-3,\ndespite having 100x fewer parameters. Moreover, InstructGPT models show\nimprovements in truthfulness and reductions in toxic output generation while\nhaving minimal performance regressions on public NLP datasets. Even though\nInstructGPT still makes simple mistakes, our results show that fine-tuning with\nhuman feedback is a promising direction for aligning language models with human\nintent.",
    "keywords": "language models, human feedback, fine-tuning, alignment, user intent"
  },
  {
    "Date": "2022.01",
    "Title": "Chain-of-Thought Prompting Elicits Reasoning in Large Language Models",
    "link": "https://arxiv.org/abs/2201.11903",
    "abstract": "We explore how generating a chain of thought -- a series of intermediate\nreasoning steps -- significantly improves the ability of large language models\nto perform complex reasoning. In particular, we show how such reasoning\nabilities emerge naturally in sufficiently large language models via a simple\nmethod called chain of thought prompting, where a few chain of thought\ndemonstrations are provided as exemplars in prompting. Experiments on three\nlarge language models show that chain of thought prompting improves performance\non a range of arithmetic, commonsense, and symbolic reasoning tasks. The\nempirical gains can be striking. For instance, prompting a 540B-parameter\nlanguage model with just eight chain of thought exemplars achieves state of the\nart accuracy on the GSM8K benchmark of math word problems, surpassing even\nfinetuned GPT-3 with a verifier.",
    "keywords": "Chain-of-Thought Prompting, Large Language Models, Complex Reasoning, Chain of Thought Demonstrations, Arithmetic, Commonsense, Symbolic Reasoning Tasks, Empirical Gains, GSM8K Benchmark, Math Word Problems, Finetuned GPT-3, Verifier."
  },
  {
    "Date": "2022.03",
    "Title": "Self-Consistency Improves Chain of Thought Reasoning in Language Models",
    "link": "https://arxiv.org/abs/2203.11171",
    "abstract": "Chain-of-thought prompting combined with pre-trained large language models\nhas achieved encouraging results on complex reasoning tasks. In this paper, we\npropose a new decoding strategy, self-consistency, to replace the naive greedy\ndecoding used in chain-of-thought prompting. It first samples a diverse set of\nreasoning paths instead of only taking the greedy one, and then selects the\nmost consistent answer by marginalizing out the sampled reasoning paths.\nSelf-consistency leverages the intuition that a complex reasoning problem\ntypically admits multiple different ways of thinking leading to its unique\ncorrect answer. Our extensive empirical evaluation shows that self-consistency\nboosts the performance of chain-of-thought prompting with a striking margin on\na range of popular arithmetic and commonsense reasoning benchmarks, including\nGSM8K (+17.9%), SVAMP (+11.0%), AQuA (+12.2%), StrategyQA (+6.4%) and\nARC-challenge (+3.9%).",
    "keywords": "Chain-of-thought prompting, self-consistency, large language models, reasoning tasks, benchmarks."
  },
  {
    "Date": "2023.08",
    "Title": "AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation",
    "link": "https://arxiv.org/abs/2308.08155",
    "abstract": "AutoGen is an open-source framework that allows developers to build LLM\napplications via multiple agents that can converse with each other to\naccomplish tasks. AutoGen agents are customizable, conversable, and can operate\nin various modes that employ combinations of LLMs, human inputs, and tools.\nUsing AutoGen, developers can also flexibly define agent interaction behaviors.\nBoth natural language and computer code can be used to program flexible\nconversation patterns for different applications. AutoGen serves as a generic\ninfrastructure to build diverse applications of various complexities and LLM\ncapacities. Empirical studies demonstrate the effectiveness of the framework in\nmany example applications, with domains ranging from mathematics, coding,\nquestion answering, operations research, online decision-making, entertainment,\netc.",
    "keywords": "AutoGen, LLM applications, multi-agent conversation, customizable agents, flexible conversation patterns"
  },
  {
    "Date": "2023.08",
    "Title": "MetaGPT: Meta Programming for A Multi-Agent Collaborative Framework",
    "link": "https://arxiv.org/abs/2308.00352",
    "abstract": "Remarkable progress has been made on automated problem solving through\nsocieties of agents based on large language models (LLMs). Existing LLM-based\nmulti-agent systems can already solve simple dialogue tasks. Solutions to more\ncomplex tasks, however, are complicated through logic inconsistencies due to\ncascading hallucinations caused by naively chaining LLMs. Here we introduce\nMetaGPT, an innovative meta-programming framework incorporating efficient human\nworkflows into LLM-based multi-agent collaborations. MetaGPT encodes\nStandardized Operating Procedures (SOPs) into prompt sequences for more\nstreamlined workflows, thus allowing agents with human-like domain expertise to\nverify intermediate results and reduce errors. MetaGPT utilizes an assembly\nline paradigm to assign diverse roles to various agents, efficiently breaking\ndown complex tasks into subtasks involving many agents working together. On\ncollaborative software engineering benchmarks, MetaGPT generates more coherent\nsolutions than previous chat-based multi-agent systems. Our project can be\nfound at https://github.com/geekan/MetaGPT",
    "keywords": "MetaGPT, Multi-Agent Collaborative Framework, Large Language Models (LLMs), Standardized Operating Procedures (SOPs), Collaborative Software Engineering Benchmarks"
  },
  {
    "Date": "2023.10",
    "Title": "MetaAgents: Simulating Interactions of Human Behaviors for LLM-based Task-oriented Coordination via Collaborative Generative Agents",
    "link": "https://arxiv.org/abs/2310.06500",
    "abstract": "Significant advancements have occurred in the application of Large Language\nModels (LLMs) for various tasks and social simulations. Despite this, their\ncapacities to coordinate within task-oriented social contexts are\nunder-explored. Such capabilities are crucial if LLMs are to effectively mimic\nhuman-like social behavior and produce meaningful results. To bridge this gap,\nwe introduce collaborative generative agents, endowing LLM-based Agents with\nconsistent behavior patterns and task-solving abilities. We situate these\nagents in a simulated job fair environment as a case study to scrutinize their\ncoordination skills. We propose a novel framework that equips collaborative\ngenerative agents with human-like reasoning abilities and specialized skills.\nOur evaluation demonstrates that these agents show promising performance.\nHowever, we also uncover limitations that hinder their effectiveness in more\ncomplex coordination tasks. Our work provides valuable insights into the role\nand evolution of LLMs in task-oriented social simulations.",
    "keywords": "Large Language Models, Collaborative Generative Agents, Task-oriented Coordination, Social Simulations, Human-like Reasoning Abilities"
  },
  {
    "Date": "2023.10",
    "Title": "OpenAgents: An Open Platform for Language Agents in the Wild",
    "link": "https://arxiv.org/abs/2310.10634",
    "abstract": "Language agents show potential in being capable of utilizing natural language\nfor varied and intricate tasks in diverse environments, particularly when built\nupon large language models (LLMs). Current language agent frameworks aim to\nfacilitate the construction of proof-of-concept language agents while\nneglecting the non-expert user access to agents and paying little attention to\napplication-level designs. We present OpenAgents, an open platform for using\nand hosting language agents in the wild of everyday life. OpenAgents includes\nthree agents: (1) Data Agent for data analysis with Python/SQL and data tools;\n(2) Plugins Agent with 200+ daily API tools; (3) Web Agent for autonomous web\nbrowsing. OpenAgents enables general users to interact with agent\nfunctionalities through a web user interface optimized for swift responses and\ncommon failures while offering developers and researchers a seamless deployment\nexperience on local setups, providing a foundation for crafting innovative\nlanguage agents and facilitating real-world evaluations. We elucidate the\nchallenges and opportunities, aspiring to set a foundation for future research\nand development of real-world language agents.",
    "keywords": "Language agents, Large language models (LLMs), OpenAgents, Application-level designs, Real-world evaluations"
  },
  {
    "Date": "2025.03",
    "Title": "Mask-DPO: Generalizable Fine-grained Factuality Alignment of LLMs",
    "link": "https://arxiv.org/abs/2503.02846",
    "abstract": "Large language models (LLMs) exhibit hallucinations (i.e., unfaithful or\nnonsensical information) when serving as AI assistants in various domains.\nSince hallucinations always come with truthful content in the LLM responses,\nprevious factuality alignment methods that conduct response-level preference\nlearning inevitably introduced noises during training. Therefore, this paper\nproposes a fine-grained factuality alignment method based on Direct Preference\nOptimization (DPO), called Mask-DPO. Incorporating sentence-level factuality as\nmask signals, Mask-DPO only learns from factually correct sentences in the\npreferred samples and prevents the penalty on factual contents in the not\npreferred samples, which resolves the ambiguity in the preference learning.\nExtensive experimental results demonstrate that Mask-DPO can significantly\nimprove the factuality of LLMs responses to questions from both in-domain and\nout-of-domain datasets, although these questions and their corresponding topics\nare unseen during training. Only trained on the ANAH train set, the score of\nLlama3.1-8B-Instruct on the ANAH test set is improved from 49.19% to 77.53%,\neven surpassing the score of Llama3.1-70B-Instruct (53.44%), while its\nFactScore on the out-of-domain Biography dataset is also improved from 30.29%\nto 39.39%. We further study the generalization property of Mask-DPO using\ndifferent training sample scaling strategies and find that scaling the number\nof topics in the dataset is more effective than the number of questions. We\nprovide a hypothesis of what factual alignment is doing with LLMs, on the\nimplication of this phenomenon, and conduct proof-of-concept experiments to\nverify it. We hope the method and the findings pave the way for future research\non scaling factuality alignment.",
    "keywords": "Large Language Models, Hallucinations, Direct Preference Optimization, Factuality Alignment, Generalization Property"
  }
]