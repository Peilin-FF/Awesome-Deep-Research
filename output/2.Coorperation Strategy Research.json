[
  {
    "Date": "2023.03",
    "Title": "CAMEL: Communicative Agents for \"Mind\" Exploration of Large Language Model Society",
    "link": "https://arxiv.org/abs/2303.17760",
    "abstract": "The rapid advancement of chat-based language models has led to remarkable\nprogress in complex task-solving. However, their success heavily relies on\nhuman input to guide the conversation, which can be challenging and\ntime-consuming. This paper explores the potential of building scalable\ntechniques to facilitate autonomous cooperation among communicative agents, and\nprovides insight into their \"cognitive\" processes. To address the challenges of\nachieving autonomous cooperation, we propose a novel communicative agent\nframework named role-playing. Our approach involves using inception prompting\nto guide chat agents toward task completion while maintaining consistency with\nhuman intentions. We showcase how role-playing can be used to generate\nconversational data for studying the behaviors and capabilities of a society of\nagents, providing a valuable resource for investigating conversational language\nmodels. In particular, we conduct comprehensive studies on\ninstruction-following cooperation in multi-agent settings. Our contributions\ninclude introducing a novel communicative agent framework, offering a scalable\napproach for studying the cooperative behaviors and capabilities of multi-agent\nsystems, and open-sourcing our library to support research on communicative\nagents and beyond: https://github.com/camel-ai/camel.",
    "keywords": "\"Communicative Agents\", \"Large Language Model\", \"Autonomous Cooperation\", \"Role-playing\", \"Instruction-following Cooperation\""
  },
  {
    "Date": "2023.07",
    "Title": "ChatDev: Communicative Agents for Software Development",
    "link": "https://aclanthology.org/2024.acl-long.810.pdf",
    "abstract": "Text evaluation has historically posed significant challenges, often\ndemanding substantial labor and time cost. With the emergence of large language\nmodels (LLMs), researchers have explored LLMs' potential as alternatives for\nhuman evaluation. While these single-agent-based approaches show promise,\nexperimental results suggest that further advancements are needed to bridge the\ngap between their current effectiveness and human-level evaluation quality.\nRecognizing that best practices of human evaluation processes often involve\nmultiple human annotators collaborating in the evaluation, we resort to a\nmulti-agent debate framework, moving beyond single-agent prompting strategies.\nThe multi-agent-based approach enables a group of LLMs to synergize with an\narray of intelligent counterparts, harnessing their distinct capabilities and\nexpertise to enhance efficiency and effectiveness in handling intricate tasks.\nIn this paper, we construct a multi-agent referee team called ChatEval to\nautonomously discuss and evaluate the quality of generated responses from\ndifferent models on open-ended questions and traditional natural language\ngeneration (NLG) tasks. Our analysis shows that ChatEval transcends mere\ntextual scoring, offering a human-mimicking evaluation process for reliable\nassessments. Our code is available at https://github.com/chanchimin/ChatEval.",
    "keywords": "Large Language Models (LLMs), Multi-Agent Debate Framework, Text Evaluation, Natural Language Generation (NLG), ChatEval"
  },
  {
    "Date": "2023.08",
    "Title": "ChatEval: Towards Better LLM-based Evaluators through Multi-Agent Debate",
    "link": "https://arxiv.org/abs/2308.07201",
    "abstract": "Software development is a complex task that necessitates cooperation among multiple members with diverse skills. Numerous studies used deep learning to improve specific phases in a waterfall model, such as design, coding, and testing. However, the deep learning model in each phase requires unique designs, leading to technical inconsistencies across various phases, which results in a fragmented and ineffective development process. In this paper,we introduce ChatDev, a chat-powered software development framework in which specialized agents driven by large language models. (LLMs) are guided in what to communicate(via chat chain) and how to communicate (via communicative dehallucination). These agents actively contribute to the design, coding, and testing phases through unified language-based communication, with solutions derived from their multi-turn dialogues. We found their utilization of natural language is advantageous for system design, and communicating in programming language proves helpful in debugging. This paradigm demonstrates how linguistic communication facilitates multi-agent collaboration, establishin language as a unifying bridge for autonomous task-solving among LLM agents. The code and data are available at https://github.com/OpenBMB/ChatDev",
    "keywords": "ChatEval, Large Language Models (LLMs), Multi-Agent Debate, Software Development, Linguistic Communication"
  },
  {
    "Date": "2023.08",
    "Title": "AgentVerse: Facilitating Multi-Agent Collaboration and Exploring Emergent Behaviors",
    "link": "https://arxiv.org/abs/2308.10848",
    "abstract": "Autonomous agents empowered by Large Language Models (LLMs) have undergone\nsignificant improvements, enabling them to generalize across a broad spectrum\nof tasks. However, in real-world scenarios, cooperation among individuals is\noften required to enhance the efficiency and effectiveness of task\naccomplishment. Hence, inspired by human group dynamics, we propose a\nmulti-agent framework \\framework that can collaboratively and dynamically\nadjust its composition as a greater-than-the-sum-of-its-parts system. Our\nexperiments demonstrate that \\framework framework can effectively deploy\nmulti-agent groups that outperform a single agent. Furthermore, we delve into\nthe emergence of social behaviors among individual agents within a group during\ncollaborative task accomplishment. In view of these behaviors, we discuss some\npossible strategies to leverage positive ones and mitigate negative ones for\nimproving the collaborative potential of multi-agent groups. Our codes for\n\\framework will soon be released at\n\\url{https://github.com/OpenBMB/AgentVerse}.",
    "keywords": "Multi-Agent, Collaboration, Emergent Behaviors, Large Language Models, Social Behaviors"
  },
  {
    "Date": "2024.06",
    "Title": "Generative AI Voting: Fair Collective Choice is Resilient to LLM Biases and Inconsistencies",
    "link": "https://arxiv.org/abs/2406.11871",
    "abstract": "Scaling up deliberative and voting participation is a longstanding endeavor\n-- a cornerstone for direct democracy and legitimate collective choice. Recent\nbreakthroughs in generative artificial intelligence (AI) and large language\nmodels (LLMs) unravel new capabilities for AI personal assistants to overcome\ncognitive bandwidth limitations of humans, providing decision support or even\ndirect representation of human voters at large scale. However, the quality of\nthis representation and what underlying biases manifest when delegating\ncollective decision-making to LLMs is an alarming and timely challenge to\ntackle. By rigorously emulating with high realism more than >50K LLM voting\npersonas in 81 real-world voting elections, we disentangle the nature of\ndifferent biases in LLMS (GPT 3, GPT 3.5, and Llama2). Complex preferential\nballot formats exhibit significant inconsistencies compared to simpler\nmajoritarian elections that show higher consistency. Strikingly though, by\ndemonstrating for the first time in real-world a proportional representation of\nvoters in direct democracy, we are also able to show that fair ballot\naggregation methods, such as equal shares, prove to be a win-win: fairer voting\noutcomes for humans with fairer AI representation. This novel underlying\nrelationship proves paramount for democratic resilience in progressives\nscenarios with low voters turnout and voter fatigue supported by AI\nrepresentatives: abstained voters are mitigated by recovering highly\nrepresentative voting outcomes that are fairer. These interdisciplinary\ninsights provide remarkable foundations for science, policymakers, and citizens\nto develop safeguards and resilience for AI risks in democratic innovations.",
    "keywords": "Generative AI, Voting, Fair Collective Choice, LLM Biases, Proportional Representation"
  },
  {
    "Date": "2024.12",
    "Title": "LLMs as Debate Partners: Utilizing Genetic Algorithms and Adversarial Search for Adaptive Arguments",
    "link": "https://arxiv.org/abs/2412.06229",
    "abstract": "This paper introduces DebateBrawl, an innovative AI-powered debate platform\nthat integrates Large Language Models (LLMs), Genetic Algorithms (GA), and\nAdversarial Search (AS) to create an adaptive and engaging debating experience.\nDebateBrawl addresses the limitations of traditional LLMs in strategic planning\nby incorporating evolutionary optimization and game-theoretic techniques. The\nsystem demonstrates remarkable performance in generating coherent, contextually\nrelevant arguments while adapting its strategy in real-time. Experimental\nresults involving 23 debates show balanced outcomes between AI and human\nparticipants, with the AI system achieving an average score of 2.72 compared to\nthe human average of 2.67 out of 10. User feedback indicates significant\nimprovements in debating skills and a highly satisfactory learning experience,\nwith 85% of users reporting improved debating abilities and 78% finding the AI\nopponent appropriately challenging. The system's ability to maintain high\nfactual accuracy (92% compared to 78% in human-only debates) while generating\ndiverse arguments addresses critical concerns in AI-assisted discourse.\nDebateBrawl not only serves as an effective educational tool but also\ncontributes to the broader goal of improving public discourse through\nAI-assisted argumentation. The paper discusses the ethical implications of AI\nin persuasive contexts and outlines the measures implemented to ensure\nresponsible development and deployment of the system, including robust\nfact-checking mechanisms and transparency in decision-making processes.",
    "keywords": "Large Language Models (LLMs), Genetic Algorithms (GA), Adversarial Search (AS), AI-powered debate platform, adaptive arguments"
  },
  {
    "Date": "2025.01",
    "Title": "Debate Helps Weak-to-Strong Generalization",
    "link": "https://arxiv.org/abs/2501.13124",
    "abstract": "Common methods for aligning already-capable models with desired behavior rely\non the ability of humans to provide supervision. However, future superhuman\nmodels will surpass the capability of humans. Therefore, humans will only be\nable to weakly supervise superhuman models. This expected deficiency of human\nevaluation would weaken the safety of future AI systems. Scalable oversight and\nweak-to-strong generalization are two complementary approaches to tackle this\nissue. In this paper, we attempt to combine the strengths of these two\napproaches to further improve alignment. Specifically, we investigate ways of\nimproving human supervision with a strong pretrained model and then supervise\nthe strong model with enhanced weak human supervision. To make iterative\nempirical progress, we consider an analogy: can we use a strong model to\nimprove weak model supervision and then use it to supervise the strong model?\nWe empirically test it by finetuning a small weak model on ground truth labels\nwith the additional help from a large strong model, and then finetuning the\nstrong model on labels generated by the weak model. We find that debate can\nassist a weak model in extracting trustworthy information from an untrustworthy\nstrong model, which provides leverage as context on samples when training a\nweak model. We also show that an ensemble of weak models helps exploit long\narguments generated by strong model debaters and obtain a more robust\nsupervision estimate. Extensive experiments on the OpenAI weak-to-strong NLP\nbenchmarks show that the combination approach leads to better alignment, which\nindicates that debate has the potential to help weak-to-strong generalization.",
    "keywords": "\"Debate\", \"Weak-to-Strong Generalization\", \"Alignment\", \"Supervision\", \"Superhuman Models\""
  }
]